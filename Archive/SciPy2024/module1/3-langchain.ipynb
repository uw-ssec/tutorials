{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LangChain: The LLM Application Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "LangChain, an open-source library, empowers developers by providing a standardized and structured interface for building and integrating various components of an LLM Application. Its model-agnostic nature allows for compatibility with models from multiple LLM providers, including OpenAI, HuggingFace, and others. \n",
    "\n",
    "Using Langchain allows us to build (\"like a chain\") reusable components as part of complex multi-step LLM-based applications clearly and succinctly. \n",
    "\n",
    "You can learn about different [LangChain components here.](https://python.langchain.com/v0.2/docs/concepts/#components)\n",
    "\n",
    "This tutorial will focus on a few LangChain components and learn about `chaining`, one of its powerful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "[Prompt Templates](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates) provides templates for designing prompts fed as inputs to the LLM models.\n",
    "It helps us design templates with multiple inputs that are parameterized and reusable.\n",
    "\n",
    "```{note}\n",
    "For this tutorial, we will only cover the use of `String PromptTemplates` as this gives us a finer control over the template string structure, unlike the `ChatPromptTemplate`.\n",
    "```\n",
    "\n",
    "Below is an example of how to use a prompt template. We can import this class from the `langchain-core` package.\n",
    "\n",
    "The `langchain-core` package contains base abstractions of different components and ways to compose them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcf656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### String PromptTemplates\n",
    "\n",
    "The [String PromptTemplates](https://python.langchain.com/v0.2/docs/concepts/#string-prompttemplates) is used to format a string input. By default, the template takes Python `f-string` format. There are currently 2 choices of `template_format` available: `f-string` and `jinja2`. Later we will see the use of `jinja2` format. In the example below, we will use the `f-string` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mars in the solar system is the '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{planet_name} in the solar system is the \"\n",
    ")\n",
    "\n",
    "prompt_template.format(planet_name=\"Mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ac98b",
   "metadata": {},
   "source": [
    "Let's instantiate our OLMo model like in the [previous section](./2-llms-and-prompt-engineering-with-olmo.ipynb#introduction) of the tutorial with `llama-cpp-python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cf3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from ssec_tutorials import download_olmo_model\n",
    "from ssec_tutorials.scipy_conf import parse_text_generation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at /Users/lsetiawan/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "OLMO_MODEL = (\n",
    "    download_olmo_model()\n",
    ")  # It won't actually download again if it's already there\n",
    "olmo = Llama(model_path=str(OLMO_MODEL), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf3d2e",
   "metadata": {},
   "source": [
    "Now that we have our model ready to go, let's try different prompt templating starting from the previous prompt template with an input of `planet_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=prompt_template.format(planet_name=\"Mars\"),\n",
    "    temperature=0.2,\n",
    "    max_tokens=8,\n",
    "    echo=True,\n",
    ")  # Generate a completion, can also call olmo.create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars in the solar system is the \n",
      "4th largest planet from the sun\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Size of the planet Earth is '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{entity_1} of the planet {entity_2} is \"\n",
    ")\n",
    "prompt_template.format(entity_1=\"Size\", entity_2=\"Earth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=prompt_template.format(entity_1=\"Size\", entity_2=\"Earth\"),\n",
    "    temperature=0.2,\n",
    "    echo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the planet Earth is \n",
      "5,147 kilometers or 3,158 miles. The diameter of the planet\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10abf9",
   "metadata": {},
   "source": [
    "#### Your turn üòé\n",
    "\n",
    "Create a `StringPromptTemplate` that outputs some text generation prompt, for example, \"Sun is part of galaxy ...\".\n",
    "\n",
    "Feel free to experiment with the built in [Python `f-string` ](https://docs.python.org/3.11/tutorial/inputoutput.html#formatted-string-literals) for the `prompt` input argument to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cdc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your prompt_template and model_response code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## LLM Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67e77a",
   "metadata": {},
   "source": [
    "LangChain have implemented a [`Runnable`](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) protocol that allows us to create custom \"chains\".\n",
    "This protocol has a standard interface for defining and invoking various LLMs, PromptTemplates, and other components, enabling reusability.\n",
    "For more details, go to LangChain's [Runnable documentation](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface).\n",
    "\n",
    "```{note}\n",
    "In this tutorial, you will see the use of `.invoke` method on various LangChain's object.\n",
    "This is essentially using that standard interface for the `Runnable` protocol.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99921350",
   "metadata": {},
   "source": [
    "Loading the model via [LangChain's LlamaCpp](https://python.langchain.com/v0.2/docs/integrations/llms/llamacpp/) abstraction enables us to use the `chaining` feature. This class is part of the `langchain-community` package, which contains third party integrations that are maintained by the LangChain community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    temperature=0.8,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5710a",
   "metadata": {},
   "source": [
    "As you can see below, we now have a `LlamaCpp` Langchain object rather than the `Llama` llama-cpp-python object from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e617a3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.llms.llamacpp.LlamaCpp"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(olmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a900fc",
   "metadata": {},
   "source": [
    "We learned above about the `Runnable` protocol. Let's see how we can invoke the model using the standard interface compared to how we originally invoked the model with `llama-cpp-python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864d5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = olmo.invoke(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b3ce37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In one of his famous essays, Samuel Johnson replied to this question with a question: ‚ÄúWhat is the purpose of life?‚Äù He answered his own question by saying that the purpose of life was a philosophical problem, which could not be answered by an essay or a book.\n",
      "Johnson‚Äôs response reflects a common attitude towards the meaning of life. Many people assume that the meaning of life is an intellectual or philosophical question that can only be answered through the study of philosophy, religion, or science. However, this view overlooks the fact that the meaning of life is not just a theoretical concept but also a practical one.\n",
      "The meaning of life has real-world implications for our lives and for society as a whole. It affects how we live, what we do, and why we do it. For example, if the meaning of life is simply to be happy or to achieve success, then many people will focus on those goals without considering the broader implications of their actions. This can lead to a shallow and meaningless existence.\n",
      "On the other hand, if the meaning of life is something more than just personal satisfaction, then it becomes important to consider how our actions contribute to the greater good of society. This may involve thinking about issues such as social justice, environmental responsibility,\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b730d9e",
   "metadata": {},
   "source": [
    "If you'd like to access the base object `Llama` object from the `llama-cpp-python` package, you can access it via the `.client` attribute of the `LlamaCpp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d56ddd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_cpp.llama.Llama"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(olmo.client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a1da2",
   "metadata": {},
   "source": [
    "With access to the underlying `Llama` object, you can directly retrieve any metadata information. In this example, we are retrieving OLMo's tokenizer chat template we saw in the [previous notebook](./2-llms-and-prompt-engineering-with-olmo.ipynb#chat-completion) to setup a String PromptTemplate.\n",
    "\n",
    "The built in model's chat template is using [`jinja2`](https://jinja.palletsprojects.com/en/3.1.x/) templating syntax, which is a popular templating engine for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"], template_format=\"jinja2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31784193",
   "metadata": {},
   "source": [
    "The `PromptTemplate` object has 2 main attributes that are very useful to explore the built-in prompt template of the model:\n",
    "- `input_variables`: This is a list of all the input variables that the prompt template expects.\n",
    "- `template`: This is the actual template string that the model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69d801b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_generation_prompt', 'eos_token', 'messages']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3af2bc",
   "metadata": {},
   "source": [
    "For this particular template, we can see that it expects `add_generation_prompt`, `eos_token` and `messages`. But what are the variable types for these inputs? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ebacd",
   "metadata": {},
   "source": [
    "We can answer the questions above by looking at the template string itself. The template string is using the jinja2 templating engine syntax, so it may look confusing at first, but at the end of the day it's essentially just some python code in a template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4822b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ eos_token }}{% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79759be0",
   "metadata": {},
   "source": [
    "As we can see above, the template reads as follows:\n",
    "- `eos_token` is a string that is added at the top of the resulting string after prompt is formatted.\n",
    "You can also see that `eos_token` is used to append `content` string values from an `assistant` `role`.\n",
    "You can find this value by going to the Model's [`tokenizer_config.json`](https://huggingface.co/allenai/OLMo-7B-Instruct-hf/blob/main/tokenizer_config.json#L233) file and looking for the `eos_token` key. *Unfornately, this is currently the only way to get this information, you can go to https://github.com/ggerganov/llama.cpp/issues/5040 for more details.* In our case, the `eos_token` is `<|endoftext|>`.\n",
    "- `messages` is a list of dictionary that is iterated over. As you can see that this dictionary should contain a `role` and `content` key.\n",
    "- `add_generation_prompt` is a boolean that is used to determine whether to add a generation prompt or not. In this case, when it's the last message and `add_generation_prompt` is `True`, it will add `<|assistant|>` string to the end of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad5f5c",
   "metadata": {},
   "source": [
    "Now that we know what the template expects we can create the final prompt string by passing in the expected input variables, this time, instead of using the `.format` method, let's see what happens if we use the `.invoke` method on the `PromptTemplate` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BasePromptTemplate.invoke() got an unexpected keyword argument 'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant. Tell me a joke about cats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: BasePromptTemplate.invoke() got an unexpected keyword argument 'messages'"
     ]
    }
   ],
   "source": [
    "prompt_template.invoke(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "        }\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb45212",
   "metadata": {},
   "source": [
    "As you can see, this results to an error if we pass in the input variables directly. This is because the `.invoke` method expects an input argument called `input` that is a **dictionary** of the input variables, which will be passed into the runnable.\n",
    "Also, there's a `config` input argument that is a `RunnableConfig` object, however, this is optional and can be omitted,\n",
    "and you will see that we will use this later when invoking the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abf42949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Dict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[RunnableConfig]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'PromptValue'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Transform a single input into an output. Override to implement.\n",
      "\n",
      "Args:\n",
      "    input: The input to the runnable.\n",
      "    config: A config to use when invoking the runnable.\n",
      "       The config supports standard keys like 'tags', 'metadata' for tracing\n",
      "       purposes, 'max_concurrency' for controlling how much work to do\n",
      "       in parallel, and other keys. Please refer to the RunnableConfig\n",
      "       for more details.\n",
      "\n",
      "Returns:\n",
      "    The output of the runnable.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/ssec-scipy2024/lib/python3.11/site-packages/langchain_core/prompts/base.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?prompt_template.invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9cf84",
   "metadata": {},
   "source": [
    "Let's try again, this time with the correct input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4caf24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = prompt_template.invoke(\n",
    "    input=dict(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "            }\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        eos_token=\"<|endoftext|>\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33e0d4",
   "metadata": {},
   "source": [
    "You can see below that we get [`StringPromptValue`](https://api.python.langchain.com/en/latest/prompt_values/langchain_core.prompt_values.StringPromptValue.html) object this time as the output rather than pure string. But we can still get the string value by calling the `.to_string` method on the `StringPromptValue` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46faf746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\n\\n<|user|>\\nYou are a helpful assistant. Tell me a joke about cats\\n\\n\\n<|assistant|>\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9525fc6",
   "metadata": {},
   "source": [
    "The output string above contains the necessary signifier tokens for the OLMo Model to understand what the user input is and where the model should put generated responses. This whole string output will then become the full prompt for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b20ffc",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For the rest of the tutorial, we won't be using `.invoke` method on the `PromptTemplate` object, but rather we will use the `.format` method to get the final prompt string. This is more straightforward and easier to understand. The walkthrough above is just to show you how to use the `.invoke` method.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "\n",
    "Chaining allows us to combine multiple components, as described above, in series or parallel to develop a multi-step LLM pipeline.\n",
    "As shown in the image below, any number of components can be linked together to form a chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "![LancChain Chain](../../images/langchain-chain.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "\n",
    "Image Source: [www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2023/10/a-comprehensive-guide-to-using-chains-in-langchain/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Internally, the chain works like below:\n",
    "\n",
    "STEP 1: Dictionary is processed as an input to the prompt template.  \n",
    "STEP 2: Prompt Template reads the variables to form the prompt text as output - \"What are stars and moon?\"  \n",
    "STEP 3: The prompt is given as input to the LLM model.  \n",
    "STEP 4: LLM Model produces output.  \n",
    "STEP 5: The output goes through StrOutputParser that parses it into string and gives the result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We can use the pipe operator (\"|\"), which is part of the [LCEL(Lang Chain Expression Language)](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel). The pipe operator sequentially arranges each component, similar to the above image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2337d",
   "metadata": {},
   "source": [
    "When we check the type of the resulting chain, it's just a `RunnableSequence`! So, essentially, it's a series of runnables that are executed in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3364f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e296cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['add_generation_prompt', 'eos_token', 'messages'], template=\"{{ eos_token }}{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", template_format='jinja2')\n",
       "| LlamaCpp(verbose=False, client=<llama_cpp.llama.Llama object at 0x125cbb6d0>, model_path='/Users/lsetiawan/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b5e1c",
   "metadata": {},
   "source": [
    "Like other `Runnable` type, it has an `invoke` method that expects the same `input` and `config` arguments as we've seen before with the `LLM` and `PromptTemplate` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca95b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Input'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[RunnableConfig]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Output'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Transform a single input into an output. Override to implement.\n",
      "\n",
      "Args:\n",
      "    input: The input to the runnable.\n",
      "    config: A config to use when invoking the runnable.\n",
      "       The config supports standard keys like 'tags', 'metadata' for tracing\n",
      "       purposes, 'max_concurrency' for controlling how much work to do\n",
      "       in parallel, and other keys. Please refer to the RunnableConfig\n",
      "       for more details.\n",
      "\n",
      "Returns:\n",
      "    The output of the runnable.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/mambaforge/envs/ssec-scipy2024/lib/python3.11/site-packages/langchain_core/runnables/base.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?llm_chain.invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d4c13",
   "metadata": {},
   "source": [
    "Just like the example above, we'll need to pass in the input variables as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Why don't cats play poker in the jungle? There are too many predators!\\n\\n ‚Äî Jim Benton ‚òïÔ∏èüê±üåä (May The Beans Be With You)\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the prompt as expected by OLMo\n",
    "llm_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "            }\n",
    "        ],\n",
    "        \"add_generation_prompt\": True,\n",
    "        \"eos_token\": \"<|endoftext|>\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Instead of having to invoke `llm_chain` repeatedly with `add_generation_prompt` and `eos_token`, we can update our `prompt_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using OLMo's tokenizer chat template we saw in module 1, but this time use partial variables.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Let's stream the output instead of waiting for OLMo to generate and display the text.\n",
    "We can use [Callbacks](https://python.langchain.com/v0.2/docs/concepts/#callbacks) to subscribe to various events in your LLM application pipeline. \n",
    "Check [this out](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) for a list of events.\n",
    "\n",
    "Below, we will use the [`StreamingStdOutCallbackHander`](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html#langchain-core-callbacks-streaming-stdout-streamingstdoutcallbackhandler) to stream the output to the console.\n",
    "To do this, we can pass in a dictionary to the `config` argument of the `invoke` method, with a `callbacks` key that contains a list of callback handlers, to see all the options, checkout the [`RunnableConfig`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here's a cute cat joke for you:\n",
      "\n",
      "\n",
      "Why don't cats like Wi-Fi? Because they prefer the old-school method of tracking down live cables to play with! üòäüê±üí¨ Remember, sharing is caring, but not when it comes to your wireless connection. üì∂üåç‚ùì"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure, here's a cute cat joke for you:\\n\\n\\nWhy don't cats like Wi-Fi? Because they prefer the old-school method of tracking down live cables to play with! üòäüê±üí¨ Remember, sharing is caring, but not when it comes to your wireless connection. üì∂üåç‚ùì\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config={\"callbacks\": [StreamingStdOutCallbackHandler()]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "We will cover more LangChain concepts in upcoming notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90d26f",
   "metadata": {},
   "source": [
    "#### Your turn üòé\n",
    "\n",
    "Try different messages value(s) and see how the output changes. But remember to follow the template structure.\n",
    "The dictionary keys must contain `role` and `content` and the allowed `role` values are only `user` and `assistant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your llm_chain.invoke code here, feel free to also, create your own template and try partial_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
