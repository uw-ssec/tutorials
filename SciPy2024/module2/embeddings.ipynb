{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bdeece8-77d6-488c-945e-0b929e318b5f",
   "metadata": {},
   "source": [
    "## **What are Embedding Models in LLMs? How do they work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc1cd1c-93d8-41a9-889a-b8f26dbd6740",
   "metadata": {},
   "source": [
    "The world of generative AI is growing at an exponential pace. Large Language models are the driving engines behind this bandwagon. We are at the tipping point (or maybe beyond) of another IT revolution. \n",
    "\n",
    "That being said, it becomes even more crucial to understand the magician's trick that makes these large language models work. \n",
    "\n",
    "In this tutorial, we will try to understand the core concepts around \"Embeddings\" that power the LLMs.\n",
    "Embeddings, also called \"Vector Embedding\", allow LLMs to develop a semantic understanding of the textual data that they are trained upon. In simpler terms, these embedding models lay the groundwork for LLMs to perform tasks like sentence completion, similarity search, questions and answers etc.\n",
    "\n",
    "But before we dive into the concepts of \"Vector Embeddings\", it is important to establish a baseline understanding of a few terms below:\n",
    "\n",
    "- What is a Vector? \n",
    "- What are Tokens?\n",
    "- What are Tokenizers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d0e58-75a0-4f4f-ab46-b6ced9425a9d",
   "metadata": {},
   "source": [
    "#### **What is a Vector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16d2f1-e140-4f96-a68a-3518b36340b2",
   "metadata": {},
   "source": [
    "At the low algorithmic level, machines only understand numeric values. But we all know that models like ChatGPT are able to perform decently well on natural human language (like English). \n",
    "\n",
    "How are they able to do so? \n",
    "\n",
    "Natural languages (texts) are converted into an array of numeric values before they are fed into the complex algorithms that power LLMs. These arrays of numeric values are called \"Vector\".\n",
    "\n",
    "An example of a vector: [2.5, 1.0, 3.3, 7.8]\n",
    "The above is an example of a vector of size 4. (also called 4-dimensional vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2279371a-77e1-4042-bc7d-75e9198c206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: [2.5 1.7 3.3 7.8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector = np.array([2.5, 1.7, 3.3, 7.8])\n",
    "print(f\"Vector: {vector}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182aea3-8cf3-4122-a719-7cc4ee240dde",
   "metadata": {},
   "source": [
    "#### **What are Tokens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2ae40-e255-422b-a3b9-e88cf5a25996",
   "metadata": {},
   "source": [
    "We stated above that **\"texts are converted into numeric array called vectors\"**.\n",
    "\n",
    "But what does that mean? \n",
    "\n",
    "Does it mean that whole document is converted into a vector?\n",
    "Does it mean that each paragraph is converted into a vector?\n",
    "Does it mean that each sentence is converted into a vector?\n",
    "Does it mean that each word is converted into a vector?\n",
    "\n",
    "Answer: **IT DEPENDS!!**.\n",
    "\n",
    "Tokens are the smallest unit of natural language that are converted into a vector. It could be at the document level, paragraph level, sentence level, word level, sub-word level, or character level etc. Let us look at a few examples below:\n",
    "\n",
    "Example: Consider the text below.\n",
    "\n",
    "\"Earth is a planet of the solar system. There are 9 planets in the solar system. \n",
    "All planets revolve around the sun. Sun is a star.\"\n",
    "\n",
    "\n",
    "- Case 1: **Tokenizing the entire paragraph into vector.**  \n",
    "Tokenization: The whole paragrapg is one single token.   \n",
    "Vectorization: A single vector.  \n",
    "Sample Vector Representation: [3.1, 6.8, 5.4, 8.0, 7.1]\n",
    "\n",
    "- Case 2: **Tokenizing each sentence into vectors.**  \n",
    "Tokenization: One token for each sentence (total 4 token)  \n",
    "Vectorization: One vector for each sentence (total 4 vectors).   \n",
    "Sample Vector Representation: [[1.2, 2.3, 3.8, 7.9, 0.8], [2.5, 3.0, 8.2, 6.6, 4.1], [3.2, 6.5, 8.1, 9.3, 1.4], [1.1, 0.7, 7.2, 3.5, 8.5]]\n",
    "\n",
    "- Case 3: **Tokenizing each word in the paragraph into a vector. There are 26 words in the paragraph, ignoring punctuations. Each word gets converted into a vector.**  \n",
    "Tokenization: One token for each word in the paragraph (26 tokens)  \n",
    "Vectorization: One vector for each token ( total 26 vectors).    \n",
    "Sample Vector Representation: [[2.1, 3.2, 4.1, 9.8, 7.0], [8.2, 4.2, 7.1, 3.8, 2.0].....total 26 such represenatations]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f4ba9-eed7-4c3e-8318-f50a0f165a48",
   "metadata": {},
   "source": [
    "#### **What are Tokenizers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4fc89-7742-46c3-9a1c-8ada3680c297",
   "metadata": {},
   "source": [
    "Tokenizers are components that responsible for converting texts into tokens (tokenization). There are different types of pre-trained tokenizers that are available. You can train your own tokenizers. But for the scope of this tutorial, we will use a pre-trained one. \n",
    "\n",
    "Generally, each tokenizers follows the following steps:\n",
    "\n",
    "1. Break down the original text into tokens. These tokens could be any unit (sentence, word, sub-word-character etc) based on tokenizer.\n",
    "2. Assign a token id to each of the tokens created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff7fe8b9-cbbc-4081-974e-cd273bf8cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earth', 'is', 'a', 'planet', 'in', 'the', 'solar', 'system', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Earth is a planet in the solar system.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71b127d-5a34-4b28-b435-e855a9ea6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3011, 2003, 1037, 4774, 1999, 1996, 5943, 2291, 1012]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25f7a8-847f-4d26-8c47-e7ad07bb4f3c",
   "metadata": {},
   "source": [
    "Great work so far! So now, we have a basic understanding of vectors and tokens. But I am sure you must be having these questions in mind.\n",
    "\n",
    "Q. Why are these tokens converted into vectors?  \n",
    "Q. How are these tokens converted into vectors? Is there a formula for conversion for tokens into vectors?  \n",
    "Q. What determines the size (dimesions) of the vector created from these tokens?  \n",
    "\n",
    "To answer these questions - lets jump on to the next topic - **Embedding Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef2377-7715-42d2-83cc-298ee5e3d3f5",
   "metadata": {},
   "source": [
    "#### **What are Embedding Models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f335a6-5717-47db-9025-607c347e34cd",
   "metadata": {},
   "source": [
    "A language model needs to understand that how tokens are related to each other in the context of human language. To understand this semantic relationship, these tokens are converted into numerical vectors.\n",
    "\n",
    "Embedding Models are trained upon these token to develop an \"embedding space\".\n",
    "\n",
    "- Before the training: the embedding model initializes an N-dimensional 'vector' corresponding to each 'token' with random values. (Value of N depends on the embedding model)\n",
    "  \n",
    "- During the embedding model training, the values for these vectors are updated across iterations. In this process, tokens that are similar or related are updated to have similarly valued vectors.\n",
    "  \n",
    "- After the training is complete, the collection of all the 'vector' corresponding to all the tokens is called the \"embedding space\".\n",
    "\n",
    "- \"Embedding Space\" is an encoded representation of meanings of tokens and inter-token relationship.\n",
    "\n",
    "\n",
    "As mentioned above, to create a final vector embedding from a token we need to train a model. But similar to pre-trained tokenizers, there are several pre-trained embedding models as well. For this example, we will use an existing pre-trained model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0943f508-1119-46bc-9ad6-6a00c87b66fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['earth', 'is', 'a', 'planet', 'in', 'the', 'solar', 'system', '.']\n",
      "Token Ids:  [3011, 2003, 1037, 4774, 1999, 1996, 5943, 2291, 1012]\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "print(\"Tokens: \", tokens) # ['earth', 'is', 'a', 'planet', 'in', 'the', 'solar', 'system', '.']\n",
    "print(\"Token Ids: \", token_ids) # [3011, 2003, 1037, 4774, 1999, 1996, 5943, 2291, 1012]\n",
    "\n",
    "# get the embedding vector for the word \"earth\"\n",
    "example_vector_embedding = model.embeddings.word_embeddings(torch.tensor([token_ids[0]]))\n",
    "\n",
    "print(example_vector_embedding.shape)\n",
    "# print(example_vector_embedding) # example_vector_embedding is 768-Dimensional vector represntation for the word \"earth\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579a6dd-1f0b-4b25-98f2-c3a21862ae73",
   "metadata": {},
   "source": [
    "In an embedding space, two tokens are similar to each other have similar vectors. To know the similarlity score, one of the techniques used is \"cosine similarity\".  \n",
    "\n",
    "To calculate cosine similarly, we compute the the values between two N-dimensional vectors. \n",
    "- If the value is closer to 1 that means the tokens are similar.\n",
    "- If the value is closer to -1 that means the tokens are dissimilar.\n",
    "- If the value is closer to 0 that means the tokens are unrelated.\n",
    "\n",
    "\n",
    "Lets calculate the cosine similarity score for a few pair of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af35efbe-86ea-4ef2-bee6-396ec2367663",
   "metadata": {},
   "outputs": [],
   "source": [
    "earth_token_id = tokenizer.convert_tokens_to_ids([\"earth\"])[0]\n",
    "earth_embedding = model.embeddings.word_embeddings(torch.tensor([earth_token_id]))\n",
    "\n",
    "planet_token_id = tokenizer.convert_tokens_to_ids([\"planet\"])[0]\n",
    "planet_embedding = model.embeddings.word_embeddings(torch.tensor([planet_token_id]))\n",
    "\n",
    "hotdog_token_id = tokenizer.convert_tokens_to_ids([\"hotdog\"])[0]\n",
    "hotdog_embedding = model.embeddings.word_embeddings(torch.tensor([hotdog_token_id]))\n",
    "\n",
    "life_token_id = tokenizer.convert_tokens_to_ids([\"life\"])[0]\n",
    "life_embedding = model.embeddings.word_embeddings(torch.tensor([life_token_id]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57108b82-f287-4a2b-b5af-8d6d7fd2d948",
   "metadata": {},
   "source": [
    "#### **Experiment**: Try different pairs of vector embeddings and report their cosine similarity score. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb3bde8-bda7-4e5d-9ac2-c9907812668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3102], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "similarity = cos(life_embedding, earth_embedding)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd4c78d-cd7e-4a02-9847-f267e27967e4",
   "metadata": {},
   "source": [
    "**Congratulations!!!**\n",
    "\n",
    "With this you have completed the basic understanding of tokens, vectors and embeddings.  \n",
    "\n",
    "You are all set to level up and learn about \"**RAG based LLMs**\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssec-scipy-kernel",
   "language": "python",
   "name": "ssec-scipy2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
