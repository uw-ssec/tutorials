{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astrophysics Prompting with OLMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At University of Washington, eScience Institute, we are very fortunate to have an in-house astronomer,\n",
    "[Professor Andy Connolly](https://escience.washington.edu/member/andy-connolly/).\n",
    "\n",
    "He has been generously sharing his knowledge and came up with a very interesting set of questions for us to\n",
    "try out with OLMo:\n",
    "\n",
    "* What is dark matter?\n",
    "* How many dimensions are there in the elemental abundances of stars?\n",
    "* What is the evidence for cosmological\tparity violation?\n",
    "* What observations show evidence for dark matter?\n",
    "* What are the most popular theories for what dark matter might be made of?\n",
    "* Does dark matter need to be a particle?\n",
    "* What is the expected mass of dark matter?\n",
    "* What experiments are currently ongoing and searching for dark matter?\n",
    "* How do the above experiments work?\n",
    "* What type of dark matter would the LHC be able to detect?\n",
    "* Can you modify gravity to explain dark matter? If so how?\n",
    "* How do astronomers detect the presence of planets around nearby stars?\n",
    "* What is an eclipsing binary star?\n",
    "* Can you describe a taxonomy for different classes of variable stars?\n",
    "* What would the light curve of a tidal disruption event look like? How would it be different from a supernova?\n",
    "* How do supernova explode?\n",
    "* How is a Type II SN different from a Type 1a?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze how OLMo does without any additional context with these questions or your own specific domain questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the OLMo Model and Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin with a recap of the previous module, setting up the OLMo model and prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from ssec_tutorials import download_olmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at /Users/lsetiawan/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time during the model setup, we'll try to increase the `n_ctx`, input context length, to `2048` tokens and the `max_tokens`, maximum tokens generated by the model, to `512` tokens.\n",
    "This is so later we can really expand on the questions that we ask the model and get a more expansive answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.8,\n",
    "    verbose=False,\n",
    "    n_ctx=2048,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model ready, let's setup the prompt template like before,\n",
    "using the internal chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using OLMo's tokenizer chat template we saw in module 1.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again use the partial variables\n",
    "here to fill out the `add_generation_prompt` and `eos_token` fields.\n",
    "So that we're left with just the `messages` input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['messages']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the prompt template ready, let's move on to the next step, and create a prompt for the model.\n",
    "For simplicity of this tutorial, we'll only use one message, `user` input to the model.\n",
    "This means we'll only ask the model a single question at a time,\n",
    "rather than a series of questions that can feed of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap  # a module to wrap text to make it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like before, we'll start by checking out what our full prompt text is going to look like.\n",
    "In this example, we've also used a handy built-in python module called textwrap to wrap the text to a certain width. We are using this to dedent the extra spaces to make it look cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prompt you want to send to OLMo.\n",
    "question = \"What is dark matter?\"\n",
    "input_content = textwrap.dedent(\n",
    "    f\"\"\"\\\n",
    "    You are an astrophysics expert. Please answer the following question on astrophysics.\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "input_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_content,\n",
    "    }\n",
    "]\n",
    "\n",
    "full_prompt_text = prompt_template.format(messages=input_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "\n",
      "<|user|>\n",
      "You are an astrophysics expert. Please answer the following question on astrophysics.\n",
      "Question: What is dark matter?\n",
      "\n",
      "\n",
      "\n",
      "<|assistant|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompt looks good. Let's now make a chain and invoke it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the prompt template and olmo\n",
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dark matter is a theoretical particle or collection of particles that has been observed through its effect on visible matter, but not directly seen due to the absence of electromagnetic radiation emitted by it. In this sense, it is \"dark\" or invisible. However, various methods indicate the presence of this substance in the universe, with roughly 85% of all mass-energy in the observable universe being comprised of dark matter.\n",
      "\n",
      "It was first introduced by scientists to explain the motion and structure of galaxies without considering the influence of normal (baryonic) matter. Dark matter's properties are yet to be precisely defined due to its non-observation, but it is hypothesized to have a large mass per unit volume. This substance interacts with the visible universe through gravitational forces only.\n",
      "\n",
      "To date, scientists still seek evidence or direct detection of dark matter particles; however, different theoretical approaches and indirect methods provide compelling evidence for the existence of this fundamental component of the cosmos. Dark matter is essential to understand various cosmic phenomena like galaxy rotation curves, the structure formation in the universe, and the distribution of matter within our own Milky Way galaxy."
     ]
    }
   ],
   "source": [
    "# Invoke the chain with a question and other parameters.\n",
    "captured_answer = llm_chain.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! At this point we have reviewed essentially module 1.\n",
    "But to ask different questions, we'll need a way to pass in different questions to the chain.\n",
    "We know that we can just create new values for `question`, `input_content`, and `input_messages` variables,\n",
    "but that's a lot of work and formatting to do every time we want to ask a new question.\n",
    "So what can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now introduce a new concept called [partial formatting](https://python.langchain.com/v0.2/docs/how_to/prompts_partial/).\n",
    "By using this feature, we can expand the input variables to be ones that we can easily change and pass in new values to.\n",
    "Essentially, we are creating a new prompt template from the underlying model template.\n",
    "\n",
    "We've seen this feature in module 1 and above with the use of `partial_variables` in the model setup.\n",
    "This time, since we know that we're only using one message,\n",
    "we can simplify the prompt template to take variables `question` and `instruction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a simple prompt template string that takes in the variables we want to pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt_template = textwrap.dedent(\n",
    "    \"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above prompt template string is NOT an f-string, but rather the simple string, like the ones you've created in module 1.\n",
    "\n",
    "Now that we have the prompt template string ready, let's create a partial formatting from it.\n",
    "Remember that `prompt_template` is a String PromptTemplate object that contains the original jinja-2 template string with the variables `add_generation_prompt` and `eos_token` filled in. The only variable left is `messages`, which we will create a partial formatting with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['messages'], partial_variables={'add_generation_prompt': True, 'eos_token': '<|endoftext|>'}, template=\"{{ eos_token }}{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", template_format='jinja2')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prompt_template = prompt_template.partial(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_prompt_template,\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], partial_variables={'add_generation_prompt': True, 'eos_token': '<|endoftext|>', 'messages': [{'role': 'user', 'content': '{instruction}\\n\\nQuestion: {question}\\n'}]}, template=\"{{ eos_token }}{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", template_format='jinja2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the partial formatting is simply filling in the variables `messages` and now we're left with no `input_variables`. So at this point, how can we create a new prompt template from this?\n",
    "\n",
    "The answer is pretty straightforward. Let's just call the `.format` and get the \"final\" prompt template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_string = partial_prompt_template.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\n\\n<|user|>\\n{instruction}\\n\\nQuestion: {question}\\n\\n\\n\\n<|assistant|>\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prompt_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a simple prompt string that we can create a String PromptTemplate from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template = PromptTemplate.from_template(new_prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['instruction', 'question'], template='<|endoftext|>\\n\\n<|user|>\\n{instruction}\\n\\nQuestion: {question}\\n\\n\\n\\n<|assistant|>\\n\\n')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see now that the new prompt template takes in `instruction` and `question`. Let's create a new chain and invoke it with this new prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A Session with OLMo\n",
    "\n",
    "We'll first create a single domain instruction, since we know that we're asking questions about astrophysics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_instruction = \"You are an astrophysics expert. Please answer the following question on astrophysics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many dimensions are there in the elemental abundances of stars?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = new_prompt_template.partial(instruction=domain_instruction) | olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question seems to be asking about the number of dimensions involved in calculating the elemental abundances of stars. In astronomy, \"elemental abundances\" refer to the relative amounts of different elements present in a celestial body like a star, planet or asteroid. These abundance values are obtained by analyzing the composition of a sample and comparing it with known elemental ratios for each element.\n",
      "\n",
      "The number of dimensions involved in this process is three: (1) The sample under study can have any shape (sphere, ellipsoid, etc.). In practice, it's typically a solid object or a piece(s) cut out from a bigger body like a star. (2) A sample has three spatial dimensions, namely x, y, and z, as we're generally talking about the composition of stars in three-dimensional space.\n",
      " (3) The elemental abundances refer to the number of atoms per unit volume of an element at each location within the sample. This means that the abundance of each element is a function of position in the sample (x, y, and z).\n",
      "\n",
      "In summary, there are three dimensions: spatial (x, y, z), which have four components each; one time dimension (t) typically for measurements from various times during the star's life or from different locations on its surface; finally, the elemental abundances themselves, which are functions of these spatial and temporal coordinates.\n",
      "\n",
      "It is important to mention that this example provides a simplified scenario for understanding elemental abundance values in stars. In reality, astronomers often encounter multi-phase systems like white dwarfs with distinct regions exhibiting distinct elemental compositions or other more complex cases. These additional complexities can further increase the number of dimensions involved in their analyses."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The question seems to be asking about the number of dimensions involved in calculating the elemental abundances of stars. In astronomy, \"elemental abundances\" refer to the relative amounts of different elements present in a celestial body like a star, planet or asteroid. These abundance values are obtained by analyzing the composition of a sample and comparing it with known elemental ratios for each element.\\n\\nThe number of dimensions involved in this process is three: (1) The sample under study can have any shape (sphere, ellipsoid, etc.). In practice, it\\'s typically a solid object or a piece(s) cut out from a bigger body like a star. (2) A sample has three spatial dimensions, namely x, y, and z, as we\\'re generally talking about the composition of stars in three-dimensional space.\\n (3) The elemental abundances refer to the number of atoms per unit volume of an element at each location within the sample. This means that the abundance of each element is a function of position in the sample (x, y, and z).\\n\\nIn summary, there are three dimensions: spatial (x, y, z), which have four components each; one time dimension (t) typically for measurements from various times during the star\\'s life or from different locations on its surface; finally, the elemental abundances themselves, which are functions of these spatial and temporal coordinates.\\n\\nIt is important to mention that this example provides a simplified scenario for understanding elemental abundance values in stars. In reality, astronomers often encounter multi-phase systems like white dwarfs with distinct regions exhibiting distinct elemental compositions or other more complex cases. These additional complexities can further increase the number of dimensions involved in their analyses.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn 😎\n",
    "\n",
    "You have two options:\n",
    "\n",
    "1. Use the questions provided at the beginning of this notebook and reuse the llm chain to ask questions about astrophysics.\n",
    "2. With the new prompt template `new_prompt_template` and the `olmo` model. Create a new chain with a different domain instruction, and ask questions about that domain.\n",
    "\n",
    "Feel free to ask any questions you like, and see how OLMo responds to them! If you're open to sharing, we'd love to hear about the questions you asked and the responses you received in the etherpad at https://etherpad.wikimedia.org/p/ipvVZZVxeP2JhpPxE4j6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
