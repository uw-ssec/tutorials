{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec41c35-21d0-486f-b28c-57d13abdd39c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea8cae-b19b-4d1a-9409-004670dbc152",
   "metadata": {},
   "source": [
    "An introduction to large language models and how they're trained is out of scope, but they have been trained over large amount of textual information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022c4d5-0072-4244-9c81-222ad9e5d059",
   "metadata": {},
   "source": [
    "Early language models could predict the probability of a single word token or n-grams; modern large language models can predict the likelihood of sentences, paragraphs, or entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5af3eb-a2ad-4d69-98cf-ef144e86fd65",
   "metadata": {},
   "source": [
    "However, LLMs are notoriously unable to retrieve and manipulate the knowledge they possess, which leads to issues like hallucination (i.e., generating factually incorrect information), knowledge cutoffs, and poor performance in domain-specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa232e-3442-4f55-8c98-beaaa66a6341",
   "metadata": {},
   "source": [
    "# Load an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519ac95-de35-42e4-b9f3-83a3b5ae55fc",
   "metadata": {},
   "source": [
    "We will be using OLMo, `add reasons for using OLMo`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091b7a4-5208-413a-b296-a5cfbbcc539d",
   "metadata": {},
   "source": [
    "[OLMo Suite](https://huggingface.co/collections/allenai/olmo-suite-65aeaae8fe5b6b2122b46778) has a set of OLMo models. \n",
    "\n",
    "`Give an overview of OLMo models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad625007-0979-4f95-a30f-c685d4773f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hf_olmo import OLMoForCausalLM, OLMoTokenizerFast\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d309591-6075-4a98-8148-40b57e525202",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = OLMoForCausalLM.from_pretrained(\"allenai/OLMo-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b758a1-ef72-4a04-b697-56cb6cee35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = OLMoTokenizerFast.from_pretrained(\"allenai/OLMo-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa84ed0f-cdb3-4a32-a135-4a1d59e2d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\"Astrophysics is the branch of space science that\"]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c597dd5c-3f12-4346-ad7d-7fbfc2b3a645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astrophysics is the branch of space science that deals with the fundamental characteristics of celestial objects and phenomena. It encompasses astronomy, which deals with the study of celestial bodies, and astrobiology, which studies the origins, evolution and distribution of life on Earth. In addition, astrophysics involves questions about the origin and fate of the universe, as well as what it would mean to live in a universe that is much older and much simpler than the universe that we live in.\n",
      "An astrophysicist is a person that specializes in studying the\n"
     ]
    }
   ],
   "source": [
    "response = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a176b2-9607-4374-b950-6cc425b80b73",
   "metadata": {},
   "source": [
    "As you noticed, the base `OLMo-1B` model took some time for the text generation task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da71e5-de93-4a1c-a0d7-5a6e995b2af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6bd338a-cd40-4d6b-8eb0-47849cbf9852",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306db2d1-10f6-4504-9c89-b7ac47005dc3",
   "metadata": {},
   "source": [
    "RAG is a technique to inject domain-specific knowledge into an LLM to improve LLM's knowledge with relevant information or even build a contextual model over your data without the need for finetuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb5e9f-f6ea-4e86-b7ea-6e9fbc4b7f2a",
   "metadata": {},
   "source": [
    "# What are -chat and -instruct models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ae3b2-4c48-425e-824f-16bd002642e8",
   "metadata": {},
   "source": [
    "Some of the LLMs are listed with the suffix `instruct` or `chat.` The 'instruct' version of the model has been fine-tuned to follow the prompted instructions. These models 'expect' to be asked to do something. Models with the 'chat' suffix have been fine-tuned to work in chatbots. These models 'expect' to be involved in a conversation with different actors. In contrast, non-instruct tuned models will generate an output following the prompt. If you make a chatbot, implement RAG, or use agents, use instruct or chat models. If in doubt, use an instruct model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab0a81-96db-400e-bfcf-426497a881cc",
   "metadata": {},
   "source": [
    "# Common LLM Parameters\n",
    "\n",
    "Temperature \n",
    "\n",
    "Top-k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23ed50-eecb-4334-8368-fb5f2f4a523f",
   "metadata": {},
   "source": [
    "\n",
    "# Prompt Elements: \n",
    "\n",
    "Instruction - a specific task or instruction you want the model to perform\n",
    "\n",
    "Context - external information or additional context that can steer the model to better responses\n",
    "\n",
    "Input Data - the input or question that we are interested to find a response for\n",
    "\n",
    "Output Indicator - the type or format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2cc353-3df3-4046-ac0f-801a5737c2e6",
   "metadata": {},
   "source": [
    "There is a ton of research coming out around how to design these prompts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05620799-0778-41f8-8735-cb869ece568a",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d4616-a361-40e8-8309-2591a08065bb",
   "metadata": {},
   "source": [
    "Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly asks the model to perform a task without any additional examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c554ee1-bd27-4426-96fc-5bd5efb9b085",
   "metadata": {},
   "source": [
    "# Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819566c4-2352-4ddc-8c51-91dfcbf2f281",
   "metadata": {},
   "source": [
    "# Embeddings and Vector Libraries/Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044f3f9-668c-4b07-8f9d-b801f3040230",
   "metadata": {},
   "source": [
    "# Exposing RAG Application through CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9523b508-f220-4452-8d0d-288894bc921f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
