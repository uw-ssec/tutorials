{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models are a type of machine learning model trained to learn a probability distribution over words. They've been used for various applications, including text generation, question answering, text summarization, language translation, and speech recognition. \n",
    "\n",
    "These models have existed since the 1980s and are mainly categorized into two kinds: (1) **statistical models** that use statistical techniques such as N-grams, hidden markov models, etc. These models are interpretable and more suitable when amount of data used for training is smaall, and (2) **neural models** that use neural networks for training on large amounts of data.\n",
    "\n",
    "Humans are inherently good at learning the probability of the next word. For example, if asked which of the below sentences has a higher probability for you to encounter, we know that the probability of the first sentence is greater than the second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  p(\\text{jupiter is the largest planet}) > p(\\text{jupiter is the largest moon})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling aims to train models that can do well in tasks like the above statement. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An N-gram is a sequence of N words (or tokens.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the sentence : `The Sun is the Solar System's star and by far its most massive component. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unigram, or 1-gram for the above sentence would be: `\"The\", \"Sun\", \"is\", \"the\", \"Solar\", \"Systems\", \"star\", \"and\", \"by\", \"far\", \"its\", \"most\", \"massive\", \"component\"`\n",
    "\n",
    "A bigram, or 2-gram for the above sentence would be: `\"The Sun\", \"Sun is\", \"is the\", \"the Solar\", \"Solar Systems\", \"Systems star\", \"star and\", \"and by\", \"by far\", \"far its\", \"its most\", \"most massive\", \"massive component\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram models consider the probability of the given word, given the (N-1) previous words. For trigrams, they consider the probability of a word given the two previous words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an N-Gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demystify how language models in principle, we will **build an n-gram language model** from scratch in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv?resource=download) for this tutorial, in particular we will use the abstracts for all the papers classified under the astrophysics category, i.e., with category value of `astro-ph`. \n",
    "\n",
    "The raw dataset has already been cleaned and stored in a pickle file for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials import ASTROPH_ARXIV_ABSTRACTS, download_astroph_arxiv_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/a42/.cache/ssec_tutorials/astro-ph-arXiv-abstracts.pkl')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_astroph_arxiv_abstracts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/a42/.cache/ssec_tutorials/astro-ph-arXiv-abstracts.pkl')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASTROPH_ARXIV_ABSTRACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "astro_ph_df = pd.read_pickle(ASTROPH_ARXIV_ABSTRACTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(astro_ph_df.head())\n",
    "f\"There are {astro_ph_df.shape[0]} papers in our dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we won't use the entire dataset, but only sample 10000 papers from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_astro_ph_df = astro_ph_df.sample(10000)\n",
    "assert sampled_astro_ph_df.shape[0] == 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to clean raw text data because computers, unlike humans, do not know that the word `many` and `Many` mean the same thing. We will use the `nltk` library to clean our raw text as part of data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampled_astro_ph_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch all abstracts together\n",
    "abstracts = \" \".join(list(sampled_astro_ph_df.abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing extra spaces\n",
    "abstracts = re.sub(\"\\s+\", \" \", abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "abstracts = re.sub(\"[^-9A-Za-z ]\", \"\", abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lowercase all data\n",
    "abstracts = abstracts.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An important part of any text processing pipeline is `tokenizers,` which break your unstructured data and natural language text into discrete elements. For this tutorial, we will use nltk's word tokenizer, which can help you separate words and punctuations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/a42/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk word tokenizer requires Punkt sentence tokenization models.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens = nltk.tokenize.word_tokenize(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'chameleon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'scalar',\n",
       " 'field',\n",
       " 'whose',\n",
       " 'mass',\n",
       " 'depends',\n",
       " 'on']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also remove stop words that are widely used words (such as \"the,\" \"a,\" \"an,\" or \"in\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/a42/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens = [token for token in abstract_tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` package provides an abstraction to build trigrams out of your given text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sun', 'is', 'the'),\n",
       " ('is', 'the', 'largest'),\n",
       " ('the', 'largest', 'star'),\n",
       " ('largest', 'star', 'our'),\n",
       " ('star', 'our', 'solar'),\n",
       " ('our', 'solar', 'system'),\n",
       " ('solar', 'system', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams([\"Sun\", \"is\", \"the\", \"largest\", \"star\", \"our\", \"solar\", \"system\", \".\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model.\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the frequency of the word given its two previous words.\n",
    "for w1, w2, w3 in nltk.trigrams(abstract_tokens):\n",
    "    model[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 10 random possible words given \"black hole\" as a prompt to our language model. Note that the trigram model can tell you the probability of a word given two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('photon-sphere', 2),\n",
       " ('except', 2),\n",
       " ('system', 8),\n",
       " ('filled', 2),\n",
       " ('e', 2),\n",
       " ('favoured', 2),\n",
       " ('hole', 4),\n",
       " ('puts', 2),\n",
       " ('spins', 4),\n",
       " ('entropy', 4)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(model[(\"black\", \"hole\")].items()), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform frequency to probabilities\n",
    "for key in model:\n",
    "    total_count = float(sum(model[key].values()))\n",
    "    for word in model[key]:\n",
    "        model[key][word] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \"black hole\" as a prompt to the model, these are the most probable words that our model generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mass', 0.11090225563909774),\n",
       " ('binaries', 0.03665413533834586),\n",
       " ('bh', 0.03571428571428571),\n",
       " ('masses', 0.03383458646616541),\n",
       " ('x-ray', 0.02819548872180451)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(model['black', 'hole'].items(), key = lambda item: item[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iteratively use our model to generate sentences given a random piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(text: list[str]) -> str:\n",
    "    sentence_finished = False\n",
    "     \n",
    "    while not sentence_finished:\n",
    "\n",
    "        # Randomly pick a word from model given the last two words in text.\n",
    "        model_sorted_probabilities = sorted(model[tuple(text[-2:])].items(), key = lambda item: item[1], reverse=True)\n",
    "        word = model_sorted_probabilities[np.random.choice(np.arange(len(model_sorted_probabilities)))][0]\n",
    "        \n",
    "        text.append(word)\n",
    "    \n",
    "        # Finish once we have generated 20 words.\n",
    "        if len(text) == 20:\n",
    "            sentence_finished = True\n",
    "     \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'black hole rotation affected self-gravity collapsing matter cloud -lambda cosmological constant related energy diffusion may arise dense photon dominated region'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator([\"black\", \"hole\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are simple models, but they can get computationally expensive as you start considering the long-term context of the words in a sequence, or increase the N value to a larger number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cold start is also a huge problem with such models, for example, what should have happened if our model would have never encountered `black hole` in raw text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
