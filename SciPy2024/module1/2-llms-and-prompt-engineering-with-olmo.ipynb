{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LLMs, Prompt Engineering, and OLMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "An introduction to large language models and how they're trained is out of scope, but they have been trained over large amounts of textual information available on the Internet, including books, articles, websites, and other digital content. Getting into the weeds of how these models are trained is out of the scope of this tutorial, but we have added links to papers and tutorials if you'd like to understand how LLMs are trained. Do note that training LLMs is expensive; the cost can easily increase to millions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Early language models could predict the probability of a single word token or n-grams; modern large language models can predict the likelihood of sentences, paragraphs, or entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "However, LLMs are notoriously unable to retrieve and manipulate the knowledge they possess, which leads to issues like hallucination (i.e., generating factually incorrect information), knowledge cutoffs, and poor performance in domain-specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "For this entire tutorial, we will be using [Open Language Model: OLMo](https://allenai.org/olmo), an open LLM framework built by [Allen Institute for AI](https://allenai.org/). With this open framework, you can access its complete pretraining data ([dolma](https://github.com/allenai/dolma)), training code, model weights, and evaluation suite. Tracking openness, transparency, accountability, and risks in LLMs is a growing research area. Checkout this [tool](https://opening-up-chatgpt.github.io/) to understand the range of openness in these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b7a4e",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Throughout this tutorial, you will encounter imports from a utility library called `ssec_tutorials`. This library is a collection of utility functions that we have created to make it easier to interact with the models and datasets we use in our tutorials. You can find the source code for this library at https://github.com/uw-ssec/ssec_tutorials.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb88029",
   "metadata": {},
   "source": [
    "We will first download the model, if you haven't already, using the download script mentioned during the local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc9000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials import download_olmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at /Users/lsetiawan/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "# Downloads the OLMo model in ~/.cache/\n",
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/lsetiawan/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLMO_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OLMo-7B-Instruct-Q4_K_M.gguf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the name of the model\n",
    "OLMO_MODEL.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a293705",
   "metadata": {},
   "source": [
    "There are multiple things to note in the model name that gives us a lot of information about the model such as: `7B`, `Instruct`, `Q4_K_M`, and `.gguf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fef6d0",
   "metadata": {},
   "source": [
    "### `.gguf`\n",
    "\n",
    "We will cover each of these in the following sections. For now, let's focus on the file format, which is `.gguf`.\n",
    "\n",
    "We have chosen the [GGUF format](https://huggingface.co/ssec-uw/OLMo-7B-Instruct-GGUF) of the [`OLMo 7B-Instruct`](https://huggingface.co/allenai/OLMo-7B-Instruct-hf) model for this tutorial.\n",
    "\n",
    "[`GGUF`](https://huggingface.co/docs/hub/en/gguf) is a file format for storing models for inference with [GGML](https://github.com/ggerganov/ggml) and executors based on GGML, a tensor library for machine learning.\n",
    "\n",
    "This file format is optimized for fast inference on CPUs, which is why we have chosen it for this tutorial. To use the model in this format, we are utilizing [`llama.cpp`](https://github.com/ggerganov/llama.cpp), a popular C/C++ LLM inference framework. Instead of directly calling the C/C++ code, we will use the Python bindings of it called [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python).\n",
    "\n",
    "Let's start by loading the model to memory and interacting with it using the `llama-cpp-python` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c585cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90db94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = Llama(model_path=str(OLMO_MODEL), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d15fed1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_cpp.llama.Llama at 0x1167d6e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13a8f5",
   "metadata": {},
   "source": [
    "With just a few lines of code, now you have access to a local LLM at your fingertips!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7edba6",
   "metadata": {},
   "source": [
    "### `Q4_K_M`\n",
    "\n",
    "Before moving further, let's take a look at the `Q4_K_M` part of the model name.\n",
    "This signifies the model's *quantization* type. In other words, *compression* for a model.\n",
    "\n",
    "**Quantization** reduces a high-precision representation (usually the regular 32-bit floating-point) for weights and activations to a lower-precision data type,\n",
    "the `GGUF` format has many [quantization types](https://huggingface.co/docs/hub/en/gguf#quantization-types),\n",
    "in `Q4_K_M` each weight is reduced to a 4-bit representation.\n",
    "\n",
    "If you are curious about the details of Quantization, please refer to an excellent concept guide on [Quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization) by HuggingFace.\n",
    "\n",
    "For the sake of this tutorial, we have quantized the original OLMo model to the `Q4_K_M` type.\n",
    "You can explore the other types of quantization that we've done at [https://huggingface.co/ssec-uw/OLMo-7B-Instruct-GGUF/tree/main](https://huggingface.co/ssec-uw/OLMo-7B-Instruct-GGUF/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53b730",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "If you'd like to play around with the other quantization type,\n",
    "you can use the `download_olmo_model` function with a specific `model_file` input argument value.\n",
    "\n",
    "For example, to download the `Q5_K_M` model, you can use the following code:\n",
    "\n",
    "`OLMO_MODEL_Q5_K_M = download_olmo_model(model_file=\"OLMo-7B-Instruct-Q5_K_M.gguf\")`\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29aa88d",
   "metadata": {},
   "source": [
    "### `7B`\n",
    "\n",
    "**B** stands for billion, and 7B suggests that this specific model has 7 billion parameters.\n",
    "\n",
    "**Base models**, for example [AllenAi's OLMo-7B](https://huggingface.co/allenai/OLMo-7B), [AllenAi's OLMo-1B](https://huggingface.co/allenai/OLMo-1B), and [Meta's Llama-3-8B](meta-llama/Meta-Llama-3-8B) processes billions of words and texts. The training process is semi-supervised, meaning data is supplied without much annotation or labeling, but much effort is poured into improving the data quality. We have found that training the model with tremendous amount of text allows it to learn language patterns and general knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "When prompted, the model predicts the next tokens (words) statistically likely to follow.\n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66aeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials.scipy_conf import parse_text_generation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=\"Jupiter is the largest\", echo=True, max_tokens=1, temperature=0.8\n",
    ")  # Generate a completion, can also call olmo.create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is the largest planet\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "But when prompted with, `What is the capital of Washington state in the USA?`, a base model **could** generate logical text that may or may not contain the right answer. \n",
    "\n",
    "This is when `Instruction` fine-tuning comes into play, which enhances the base model's ability to execute specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd051d5c",
   "metadata": {},
   "source": [
    "### `Instruct`\n",
    "\n",
    "For `Instruction` fine-tuning, we can take the base models and further train them on much smaller and more specific datasets. For this tutorial, we the [OLMo-7B-Instruct](https://huggingface.co/allenai/OLMo-7B-Instruct-hf), which has been fine-tuned on [Tulu 2 SFT Mix](https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture) and [Ultrafeedback Cleaned](https://huggingface.co/datasets/allenai/ultrafeedback_binarized_cleaned) datasets. That is where the keyword **Instruct** comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=\"What is the capital of Washington state in the USA?\",\n",
    "    echo=True,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Washington state in the USA?\n",
      "Washington, D.C. -- (SBWIRE) -- 10/\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## LLM Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We typically interact with the LLM via an API through which we can send prompts, and we can configure different parameters to get different results from LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86a34282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'prompt': <Parameter \"prompt: 'str'\">,\n",
       "              'suffix': <Parameter \"suffix: 'Optional[str]' = None\">,\n",
       "              'max_tokens': <Parameter \"max_tokens: 'Optional[int]' = 16\">,\n",
       "              'temperature': <Parameter \"temperature: 'float' = 0.8\">,\n",
       "              'top_p': <Parameter \"top_p: 'float' = 0.95\">,\n",
       "              'min_p': <Parameter \"min_p: 'float' = 0.05\">,\n",
       "              'typical_p': <Parameter \"typical_p: 'float' = 1.0\">,\n",
       "              'logprobs': <Parameter \"logprobs: 'Optional[int]' = None\">,\n",
       "              'echo': <Parameter \"echo: 'bool' = False\">,\n",
       "              'stop': <Parameter \"stop: 'Optional[Union[str, List[str]]]' = []\">,\n",
       "              'frequency_penalty': <Parameter \"frequency_penalty: 'float' = 0.0\">,\n",
       "              'presence_penalty': <Parameter \"presence_penalty: 'float' = 0.0\">,\n",
       "              'repeat_penalty': <Parameter \"repeat_penalty: 'float' = 1.1\">,\n",
       "              'top_k': <Parameter \"top_k: 'int' = 40\">,\n",
       "              'stream': <Parameter \"stream: 'bool' = False\">,\n",
       "              'seed': <Parameter \"seed: 'Optional[int]' = None\">,\n",
       "              'tfs_z': <Parameter \"tfs_z: 'float' = 1.0\">,\n",
       "              'mirostat_mode': <Parameter \"mirostat_mode: 'int' = 0\">,\n",
       "              'mirostat_tau': <Parameter \"mirostat_tau: 'float' = 5.0\">,\n",
       "              'mirostat_eta': <Parameter \"mirostat_eta: 'float' = 0.1\">,\n",
       "              'model': <Parameter \"model: 'Optional[str]' = None\">,\n",
       "              'stopping_criteria': <Parameter \"stopping_criteria: 'Optional[StoppingCriteriaList]' = None\">,\n",
       "              'logits_processor': <Parameter \"logits_processor: 'Optional[LogitsProcessorList]' = None\">,\n",
       "              'grammar': <Parameter \"grammar: 'Optional[LlamaGrammar]' = None\">,\n",
       "              'logit_bias': <Parameter \"logit_bias: 'Optional[Dict[str, float]]' = None\">})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(olmo).parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Some standard parameters are:\n",
    "\n",
    "**prompt:** The prompt to generate text from.\n",
    "\n",
    "**max_tokens:** The maximum number of tokens to generate.\n",
    "\n",
    "**temperature:** A higher temperature produces more creative and diverse output, while a lower temperature produces more deterministic output. In practical terms, you should use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For creative tasks, it might be beneficial to increase the temperature value.\n",
    "\n",
    "**top_p:** This parameter, in conjunction with temperature, offers a powerful tool for controlling the model's output. Known as nucleus sampling, it allows you to determine the level of determinism in the responses. By using `top_p`, you can specify that only the tokens comprising the top_p probability mass are considered for responses. A low top_p value selects the most confident responses, while a higher value prompts the model to consider more possible words, leading to more diverse outputs. The general recommendation is to alter `temperature` or `top_p` but not both.\n",
    "\n",
    "**stop:** A list of strings to stop generation when encountered. This is another way to control the length and structure of the model's response. \n",
    "\n",
    "**frequency_penalty:** The frequency penalty applies a penalty on the next token based on how many times that token has already appeared in the generated response and prompt. The higher the frequency penalty, the less likely a word will reappear. This setting reduces the repetition of words in the generated response by giving tokens that appear more a higher penalty.\n",
    "\n",
    "**presence_penalty:** The presence penalty applies the same penalty for all repeated tokens. A token that appears twice and a token that appears n times are penalized the same. You may choose a higher presence penalty if you want the model to generate diverse or creative text. \n",
    "\n",
    "To learn more about other parameters, refer to [create_completion API reference.](https://llama-cpp-python.readthedocs.io/en/stable/api-reference/#llama_cpp.Llama.create_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=\"Write a sarcastic but nice poem about the city of Seattle\",\n",
    "    echo=True,\n",
    "    temperature=1,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a sarcastic but nice poem about the city of Seattle\n",
      "* I'm on holiday in New Zealand and my car has broken down in Seattle. I've spent all my savings on new tires and am now stuck here without transport or money for several days until it's fixed. Luckily, the people of Seattle are incredibly friendly and helpful! They invited me to their weekly poetry slam night where they recite sarcastic but nice poems about their city. So, what would a tourist from New Zealand think of your fine city? Let's write a sarcastic yet nice poem together!\n",
      "\n",
      "Seattle, known throughout the world as a “green” haven for the tech-savy and environmentally conscious, has managed to hide behind a thin veneer of charm and grace. I mean, really, who wouldn’t want to live in such a picturesque city? The only issue being that you can never truly appreciate it's beauty if you're stuck in your car with no wheels.\n",
      "\n",
      "From the moment I arrived here, I was greeted by a wave of kindness from the local population. I mean sure, most locals have never actually met a real Kiwi but their good nature and helpfulness has them believing they have! They couldn’t be more wrong, my friends. In fact, it's rather laughable that these people could actually believe they've got a handle on hospitality and customer service when all the while they're secretly plotting to make me suffer in silence with their kind acts.\n",
      "\n",
      "First of all there was the “free” coffee which turned out to be an enticing offer for those who'd have otherwise given me money; or more specifically, a donation to their weekly poetry slam night. As it so happens, I'm not much of a poet, but that didn't stop them from inviting me along anyhow! Now I sit in the local Starbucks (or should we call it StarBucks) sipping on some overpriced coffee whilst they write me an email for a donation to help pay for my new tires. It's all so very kind, don't you think?\n",
      "\n",
      "Moving on, let's talk about the city itself! Now, I know this may come as a bit of a shocker but, quite frankly, it's one damn ugly place to live and visit. Don't get me wrong, it has its own charm if you're prepared for your surroundings to look like they were designed by a team of blind architects who all took their daily anti-depressant at breakfast!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Another critical concept to understand is Context length. It is the number of tokens an LLM can process at once, the maximum length of the input sequence. You can interpret it as the model's memory or attention span.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Prompt engineering or prompting is a discipline for developing and optimizing prompts to use LLMs for various applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Prompt Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "In general, prompt could contain any of the following:\n",
    "\n",
    "**Instruction:** Text to explain a specific task or instructions for the model to perform.\n",
    "\n",
    "**Context:** Additional context that can help the model generate better responses.\n",
    "\n",
    "**Input Data:** The input or question a user is interested in finding a response for.\n",
    "\n",
    "**Output Indicator:** The type or format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "A use case for LLMs is chat. In a chat context, rather than prompting LLM with a single string of text, you prompt the model with a conversation that consists of one or more messages, each of which includes a role, like `user` or `assistant`, as well as text as `content`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "`llama-cpp-python` provides a [high-level API for chat completion.](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#chat-completion) \n",
    "\n",
    "The model typically formats the messages in the conversation into a single prompt using a chat template from the `gguf` model's metadata. Chat templates are part of the tokenizers (more on that in `Module 2`.) They specify how to convert a chat conversation, represented as lists of messages, into a single tokenizable string in the format that the model expects, i.e., a prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "For OLMo you can see its chat template using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{ eos_token }}{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo.metadata[\"tokenizer.chat_template\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Prompts can help you get results on different tasks with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "The zero-shot prompt directly instructs the model to perform a task without any additional knowledge, but entirely based on its pre-existing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Classify the following text into neutral, negative, or positive. Today's Seattle weather is beautiful.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70b30f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials.scipy_conf import parse_chat_completion_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Positive.\\nThe text \"Today\\'s Seattle weather is beautiful\" is positive as it expresses a pleasant and favorable weather condition in Seattle.'}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Note that in the prompt above, we didn't provide OLMo with any additional context; OLMo already understands the `sentiment`—that's zero-shot at work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "##### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "OLMo or other LLMs can demonstrate remarkable zero-shot capabilities, they can fail in more complex or specific tasks. In this case, we can introduce examples (shots) or additional context within the prompt to improve the OLMo's response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Let's try zero-shot to learn more about SciPy 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Did you hear about SciPy 2024 conference?\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"I'm not personally aware of the specific details of the SciPy 2024 conference, as my training data only goes up until September 2021. However, I can provide you with some general information about SciPy conferences.\\n\\nSciPy (Short for Scientific Packages in Python) is a yearly conference focused on the scientific applications developed using the Python programming language. The conference brings together researchers and developers working on scientific computing with Python to share their work, learn from each other, and collaborate on open-source projects.\\n\\nSciPy conferences typically feature talks, tutorials, workshops, and poster sessions covering various aspects of scientific computing in Python, including numerical analysis, optimization, signal processing, machine learning, data visualization, and more. The conferences also provide opportunities for networking with the community and discussing future developments in the field.\\n\\nAs SciPy 2024 has not yet taken place, I cannot provide specific details about this particular conference. However, you can visit the official website at https://scipy2024.scipy-conference.org/ to learn more about past and upcoming conferences, as well as their program, schedule, and registration information.\"}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Interpret the response before moving on. \n",
    "\n",
    "What if we provide relevant information to answer the prompt? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The 23rd annual SciPy conference will be held at the Tacoma Convention Center, July 8-14, 2024. SciPy brings together attendees from industry, academia and government to showcase their latest projects, learn from skilled users and developers, and collaborate on code development. \",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Did you hear about SciPy 2024 conference?\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Yes, I do have information about the 23rd annual SciPy conference, which will be held at the Tacoma Convention Center from July 8-14, 2024. The conference aims to bring together attendees from various sectors such as industry, academia, and government to showcase their latest projects, learn from skilled users and developers, and collaborate on code development.\\n\\nSciPy is a non-profit organization dedicated to the advancement of scientific computing in Python, an open-source programming language. The conference offers a diverse range of talks, tutorials, workshops, and networking opportunities for attendees interested in using Python for scientific research and data analysis.\\n\\nIf you're interested in attending SciPy 2024, be sure to mark your calendars for July 8-14, 2024, and keep an eye out for updates on the conference website as the event approaches.\"}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "OLMo is able to generate a response that's much more helpful to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Many other prompting techniques (e.g., chain-of-thought, ReAct, etc.) exist. For this tutorial, we will focus on **Retrieval-Augmented Generation**, which can enhance OLMo's responses by integrating information retrieved from external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "**References**\n",
    "1. https://news.ycombinator.com/item?id=35712334\n",
    "2. https://benjaminwarner.dev/2023/07/01/attention-mechanism\n",
    "3. [Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators](https://dl.acm.org/doi/10.1145/3571884.3604316)\n",
    "4. https://www.promptingguide.ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
