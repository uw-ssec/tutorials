{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb54058-bf40-4ecd-bb10-5eb3e6c5aa8b",
   "metadata": {},
   "source": [
    "# LLMs, Prompt Engineering, and OLMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75972fd4-1e17-40e2-865c-ad10e5caeace",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59c6a8-cddf-4308-9b7a-9d5b3f09edcb",
   "metadata": {},
   "source": [
    "An introduction to large language models and how they're trained is out of scope, but they have been trained over large amounts of textual information available on the Internet, including books, articles, websites, and other digital content. Getting into the weeds of how these models are trained is out of the scope of this tutorial, but we have added links to papers and tutorials if you'd like to understand how LLMs are trained. Do note that training LLMs is expensive; the cost can easily increase to millions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f76df-7a8d-4fe8-b983-86aa7783c286",
   "metadata": {},
   "source": [
    "Early language models could predict the probability of a single word token or n-grams; modern large language models can predict the likelihood of sentences, paragraphs, or entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4ce8a-4fde-495a-9dce-12cabfcd1501",
   "metadata": {},
   "source": [
    "However, LLMs are notoriously unable to retrieve and manipulate the knowledge they possess, which leads to issues like hallucination (i.e., generating factually incorrect information), knowledge cutoffs, and poor performance in domain-specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d95306-9a8f-45cb-b3ed-c02ed65b87eb",
   "metadata": {},
   "source": [
    "For this entire tutorial, we will be using [Open Language Model: OLMo](https://allenai.org/olmo), an open LLM framework built by [Allen Institute for AI](https://allenai.org/). With this open framework, you can access its complete pretraining data ([dolma](https://github.com/allenai/dolma)), training code, model weights, and evaluation suite. Tracking openness, transparency, accountability, and risks in LLMs is a growing research area. Checkout this [tool](https://opening-up-chatgpt.github.io/) to understand the range of openness in these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef1a9d-bde0-413a-87b6-d6e90b29324b",
   "metadata": {},
   "source": [
    "We have chosen a 7B instruction-tuned OLMo model that we have compressed to speed up its inference time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a6c106f-9670-48a9-a0ab-3e4d292b3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from llama_cpp import Llama # Python bindings for llama.cpp, to enable LLM inference with minimal setup\n",
    "from ssec_tutorials import download_olmo_model, OLMO_MODEL\n",
    "from ssec_tutorials.scipy_conf import * # Contains helper methods for tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd1cc0a-d100-4537-adb6-16337381c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the OLMo model in ~/.cache/\n",
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263006d8-91ed-4695-89b4-c4eb2420f587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/a42/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLMO_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63e8f30-078a-4f72-96eb-50157e6e7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = Llama(model_path=str(OLMO_MODEL), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811136b0-596f-4cb3-894c-adaba81c87b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OLMo-7B-Instruct-Q4_K_M.gguf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the name of the model\n",
    "str(OLMO_MODEL).split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49875930-23c6-4d7b-be5b-70fabc3cfbe5",
   "metadata": {},
   "source": [
    "Note the `7B,` `Instruct,` `GGUF,` and `Q4_K_M` keywords here.\n",
    "\n",
    "**7B**: B stands for billion, and 7B suggests that this specific model has 7 billion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1a4fc-5205-466f-b534-d766fa2e2509",
   "metadata": {},
   "source": [
    "**Base models**, for example [AllenAi's OLMo-7B](https://huggingface.co/allenai/OLMo-7B), [AllenAi's OLMo-1B](https://huggingface.co/allenai/OLMo-1B), and [Meta's Llama-3-8B](meta-llama/Meta-Llama-3-8B) processes billions of words and texts. The training process is semi-supervised, meaning data is supplied without much annotation or labeling, but much effort is poured into improving the data quality. We have found that training the model with tremendous amount of text allows it to learn language patterns and general knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869dbe54-4f2a-4720-89d0-66a36490d707",
   "metadata": {},
   "source": [
    "When prompted, the model predicts the next tokens (words) statistically likely to follow.\n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e87fc4ce-176f-4f3a-8bf8-565934d772f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(prompt=\"Jupiter is the largest\", echo=True, max_tokens=1, temperature=0.8) # Generate a completion, can also call olmo.create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368d12c1-2994-479a-ac41-6c68328add24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is the largest planet\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3803f08-a1d4-49bd-b4db-1d5cdea70e6c",
   "metadata": {},
   "source": [
    "But when prompted with, `What is the capital of Washington state in the USA?`, a base model **could** generate logical text that may or may not contain the right answer. \n",
    "\n",
    "This is when `Instruction` fine-tuning comes into play, which enhances the base model's ability to execute specific tasks. For `Instruction` fine-tuning, we can take the base models and further train them on much smaller and more specific datasets. For this tutorial, we are using a **quantized**, in other words **compressed** model version of [OLMo-7B-Instruct](https://huggingface.co/allenai/OLMo-7B-Instruct), which has been fine-tuned on [UltraFeedback Dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized). That is where the keyword **Instruct** comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92e22e-3e6d-428f-af5a-98671dcf967b",
   "metadata": {},
   "source": [
    "`GGUF` is a file format for storing models for inference with GGML and executors based on GGML, a tensor library for machine learning. \n",
    "\n",
    "**Quantization** reduces a high-precision representation (usually the regular 32-bit floating-point) for weights and activations to a lower-precision data type, in `Q4_K_M` each weight is reduced to a 4-bit representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327c0bbe-1d83-4a4f-88a4-df19e1973ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(prompt=\"What is the capital of Washington state in the USA?\", echo=True, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d32981-854d-4d0a-91a9-259850609995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Washington state in the USA?\n",
      "Washington, D.C. is not a state; it is the capital\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588714ce-5b4f-4f81-ac56-fd29c765e719",
   "metadata": {},
   "source": [
    "## LLM Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e438756-3ce3-480b-a6f8-2b6dde618893",
   "metadata": {},
   "source": [
    "We typically interact with the LLM via an API through which we can send prompts, and we can configure different parameters to get different results from LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c9df1d8-1fc0-4c32-acca-e75f367d57d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'prompt': <Parameter \"prompt: 'str'\">,\n",
       "              'suffix': <Parameter \"suffix: 'Optional[str]' = None\">,\n",
       "              'max_tokens': <Parameter \"max_tokens: 'Optional[int]' = 16\">,\n",
       "              'temperature': <Parameter \"temperature: 'float' = 0.8\">,\n",
       "              'top_p': <Parameter \"top_p: 'float' = 0.95\">,\n",
       "              'min_p': <Parameter \"min_p: 'float' = 0.05\">,\n",
       "              'typical_p': <Parameter \"typical_p: 'float' = 1.0\">,\n",
       "              'logprobs': <Parameter \"logprobs: 'Optional[int]' = None\">,\n",
       "              'echo': <Parameter \"echo: 'bool' = False\">,\n",
       "              'stop': <Parameter \"stop: 'Optional[Union[str, List[str]]]' = []\">,\n",
       "              'frequency_penalty': <Parameter \"frequency_penalty: 'float' = 0.0\">,\n",
       "              'presence_penalty': <Parameter \"presence_penalty: 'float' = 0.0\">,\n",
       "              'repeat_penalty': <Parameter \"repeat_penalty: 'float' = 1.1\">,\n",
       "              'top_k': <Parameter \"top_k: 'int' = 40\">,\n",
       "              'stream': <Parameter \"stream: 'bool' = False\">,\n",
       "              'seed': <Parameter \"seed: 'Optional[int]' = None\">,\n",
       "              'tfs_z': <Parameter \"tfs_z: 'float' = 1.0\">,\n",
       "              'mirostat_mode': <Parameter \"mirostat_mode: 'int' = 0\">,\n",
       "              'mirostat_tau': <Parameter \"mirostat_tau: 'float' = 5.0\">,\n",
       "              'mirostat_eta': <Parameter \"mirostat_eta: 'float' = 0.1\">,\n",
       "              'model': <Parameter \"model: 'Optional[str]' = None\">,\n",
       "              'stopping_criteria': <Parameter \"stopping_criteria: 'Optional[StoppingCriteriaList]' = None\">,\n",
       "              'logits_processor': <Parameter \"logits_processor: 'Optional[LogitsProcessorList]' = None\">,\n",
       "              'grammar': <Parameter \"grammar: 'Optional[LlamaGrammar]' = None\">,\n",
       "              'logit_bias': <Parameter \"logit_bias: 'Optional[Dict[str, float]]' = None\">})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(olmo).parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98c2c3-5921-4a59-aec7-434b905e261b",
   "metadata": {},
   "source": [
    "Some standard parameters are:\n",
    "\n",
    "**prompt:** The prompt to generate text from.\n",
    "\n",
    "**max_tokens:** The maximum number of tokens to generate.\n",
    "\n",
    "**temperature:** A higher temperature produces more creative and diverse output, while a lower temperature produces more deterministic output. In practical terms, you should use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For creative tasks, it might be beneficial to increase the temperature value.\n",
    "\n",
    "**top_p:** This parameter, in conjunction with temperature, offers a powerful tool for controlling the model's output. Known as nucleus sampling, it allows you to determine the level of determinism in the responses. By using `top_p`, you can specify that only the tokens comprising the top_p probability mass are considered for responses. A low top_p value selects the most confident responses, while a higher value prompts the model to consider more possible words, leading to more diverse outputs. The general recommendation is to alter `temperature` or `top_p` but not both.\n",
    "\n",
    "**stop:** A list of strings to stop generation when encountered. This is another way to control the length and structure of the model's response. \n",
    "\n",
    "**frequency_penalty:** The frequency penalty applies a penalty on the next token based on how many times that token has already appeared in the generated response and prompt. The higher the frequency penalty, the less likely a word will reappear. This setting reduces the repetition of words in the generated response by giving tokens that appear more a higher penalty.\n",
    "\n",
    "**presence_penalty:** The presence penalty applies the same penalty for all repeated tokens. A token that appears twice and a token that appears n times are penalized the same. You may choose a higher presence penalty if you want the model to generate diverse or creative text. \n",
    "\n",
    "To learn more about other parameters, refer to [create_completion API reference.](https://llama-cpp-python.readthedocs.io/en/stable/api-reference/#llama_cpp.Llama.create_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89175b91-30a3-40c9-aa97-0345c7249900",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(prompt=\"Write a sarcastic but nice poem about the city of Seattle\", echo=True, temperature=1, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68867647-0f7b-45f0-9ad4-f2dfc886e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a sarcastic but nice poem about the city of Seattle.\n",
      "\n",
      "Title: \"A Slight Angle on the Emerald City\"\n",
      "\n",
      "The city of Seattle, with its rainy days and its grays\n",
      "Where coffee shops are the norm, and its hipsters are quite the sight\n",
      "It's not exactly the Sunset, but it is a place to be seen\n",
      "With its rainforest-like parks and its chai teas, too\n",
      "\n",
      "But alas, this city of ours, it's not all that\n",
      "The hipster fashion, though charming, can get old fast\n",
      "And while we may enjoy our drizzly afternoons in the park\n",
      "Our coffee culture is still something to be desired.\n",
      "\n",
      "Seattle, you see, are a bunch of grumpy-faced natives\n",
      "With moody skies above and a love for that good brew\n",
      "But deep down, there's more to this city than meets the eye\n",
      "A little warmth, some sunshine, would make it all right.\n",
      "\n",
      "So if you're ever in Seattle, be prepared for rain\n",
      "And embrace the wet weather with open arms\n",
      "For despite its quirks and its flaws, this city is worth a visit or two.\n",
      "With its artsy scene and its quirky charm, there's no doubt it stands out.\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4fc28b-0538-4949-a68d-aadb878298b3",
   "metadata": {},
   "source": [
    "> Another critical concept to understand is Context length. It is the number of tokens an LLM can process at once, the maximum length of the input sequence. You can interpret it as the model's memory or attention span."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725a982-2e97-4c9e-bdb4-7176a31a2873",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5874e4e-68ca-44a4-bffc-afe12e3785cf",
   "metadata": {},
   "source": [
    "Prompt engineering or prompting is a discipline for developing and optimizing prompts to use LLMs for various applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00af491-c447-4cd9-a559-1335e00f368a",
   "metadata": {},
   "source": [
    "### Prompt Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea133f-54ce-42b2-a2fd-5f79841078be",
   "metadata": {},
   "source": [
    "In general, prompt could contain any of the following:\n",
    "\n",
    "**Instruction:** Text to explain a specific task or instructions for the model to perform.\n",
    "\n",
    "**Context:** Additional context that can help the model generate better responses.\n",
    "\n",
    "**Input Data:** The input or question a user is interested in finding a response for.\n",
    "\n",
    "**Output Indicator:** The type or format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a23eb9-0ff0-4a19-97d7-c631bd655409",
   "metadata": {},
   "source": [
    "### Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed3a7f-faf2-4c29-a00f-182e782796ef",
   "metadata": {},
   "source": [
    "A use case for LLMs is chat. In a chat context, rather than prompting LLM with a single string of text, you prompt the model with a conversation that consists of one or more messages, each of which includes a role, like `user` or `assistant`, as well as text as `content`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91fdd4-4c2a-4d78-b8b9-5736e4a9cfe6",
   "metadata": {},
   "source": [
    "The Python binding for Llama.cpp provides a [high-level API for chat completion.](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#chat-completion) \n",
    "\n",
    "The model typically formats the messages in the conversation into a single prompt using a chat template from the `gguf` model's metadata. Chat templates are part of the tokenizers (more on that in `Module 2`.) They specify how to convert a chat conversation, represented as lists of messages, into a single tokenizable string in the format that the model expects, i.e., a prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cfb064-7302-436e-8c46-8839c78fa996",
   "metadata": {},
   "source": [
    "For OLMo you can see its chat template using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaae0770-6d55-4695-8486-38ef4c552202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{ eos_token }}{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo.metadata[\"tokenizer.chat_template\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929c8ee-0376-4878-b36f-bb64885de2f7",
   "metadata": {},
   "source": [
    "### Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b1d78-d334-44d3-b061-8ce681379d2f",
   "metadata": {},
   "source": [
    "Prompts can help you get results on different tasks with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c243d33d-2cec-4603-8ada-8169dfa25ae8",
   "metadata": {},
   "source": [
    "##### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973a43d-b1ae-4a2d-9082-280c08b454e4",
   "metadata": {},
   "source": [
    "The zero-shot prompt directly instructs the model to perform a task without any additional knowledge, but entirely based on its pre-existing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12a0da38-adc8-48c4-b017-c1a409c2c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Classify the following text into neutral, negative, or positive. Today's Seattle weather is beautiful.\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca2b53e8-a0b1-4fc4-bd71-4befc14884b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Positive.\\nThe text \"Today\\'s Seattle weather is beautiful\" is positive as it describes the current weather in a positive manner by saying that it is beautiful.'}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf7f808-1977-4177-9470-5124b154ab68",
   "metadata": {},
   "source": [
    "Note that in the prompt above, we didn't provide OLMo with any additional context; OLMo already understands the `sentiment`—that's zero-shot at work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4d8b6-9335-4d3f-9c47-425f1c8382a0",
   "metadata": {},
   "source": [
    "##### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20968a7e-6a0c-4b57-b7d9-7fcb8da872cf",
   "metadata": {},
   "source": [
    "OLMo or other LLMs can demonstrate remarkable zero-shot capabilities, they can fail in more complex or specific tasks. In this case, we can introduce examples (shots) or additional context within the prompt to improve the OLMo's response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722f591-41b0-4a86-b69c-ef39a04b59f2",
   "metadata": {},
   "source": [
    "Let's try zero-shot to learn more about SciPy 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22be8326-3be2-4025-9142-f8d579c355fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Did you hear about SciPy 2024 conference?\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d9e4821-3f74-413e-88b7-c9f283dce16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"I'm not personally aware of the specific details of the SciPy 2024 conference, as I am just an AI model designed to provide general information. However, SciPy is a community of scientists, engineers, and researchers using the Python language for scientific computing, and they organize an annual conference called SciPy Conference. The next SciPy Conference is scheduled to take place in July 2024 in New Orleans, Louisiana, USA. The conference typically features talks, tutorials, and workshops on scientific computing using Python, as well as discussions of recent advances in various scientific domains.\"}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bd52f-0ddf-4ffb-a6a9-eece1cbde053",
   "metadata": {},
   "source": [
    "Interpret the response before moving on. \n",
    "\n",
    "What if we provide relevant information to answer the prompt? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7ff5a85-d159-461f-a15f-9d01930434b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"The 23rd annual SciPy conference will be held at the Tacoma Convention Center, July 8-14, 2024. SciPy brings together attendees from industry, academia and government to showcase their latest projects, learn from skilled users and developers, and collaborate on code development. \"},\n",
    "        {\"role\": \"user\", \"content\": \"Did you hear about SciPy 2024 conference?\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50eeb967-d34b-4032-ae53-252bdfb2b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Yes, I do have information about the 23rd annual SciPy conference, which will be held at the Tacoma Convention Center from July 8-14, 2024. The conference aims to bring together attendees from various backgrounds such as industry, academia, and government to showcase their projects, learn from skilled users and developers, and collaborate on code development.\\n\\nSciPy is a non-profit organization dedicated to the advancement of scientific computing in Python, an open-source programming language. The conference offers a diverse range of talks, tutorials, workshops, and networking opportunities for attendees interested in using Python for scientific research and data analysis.\\n\\nIf you're interested in attending SciPy 2024, be sure to mark your calendars and stay updated on the official website for registration information and program updates.\"}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d82157-dd59-41cb-ad18-5f171158e631",
   "metadata": {},
   "source": [
    "OLMo is able to generate a response that's much more helpful to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d23573-42dd-431c-ab53-d0aab3958526",
   "metadata": {},
   "source": [
    "Many other prompting techniques (e.g., chain-of-thought, ReAct, etc.) exist. For this tutorial, we will focus on **Retrieval-Augmented Generation**, which can enhance OLMo's responses by integrating information retrieved from external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b54c9-f225-461f-a7d2-18315a667052",
   "metadata": {},
   "source": [
    "**References**\n",
    "1. https://news.ycombinator.com/item?id=35712334\n",
    "2. https://benjaminwarner.dev/2023/07/01/attention-mechanism\n",
    "3. [Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators](https://dl.acm.org/doi/10.1145/3571884.3604316)\n",
    "4. https://www.promptingguide.ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
