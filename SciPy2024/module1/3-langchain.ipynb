{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LangChain: The LLM Application Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "LangChain, an open-source library, empowers developers by providing a standardized and structured interface for building and integrating various components of an LLM Application. Its model-agnostic nature allows for compatibility with models from multiple LLM providers, including OpenAI, HuggingFace, and others. \n",
    "\n",
    "Using Langchain allows us to build (\"like a chain\") reusable components as part of complex multi-step LLM-based applications clearly and succinctly. \n",
    "\n",
    "You can learn about different [LangChain components here.](https://python.langchain.com/v0.2/docs/concepts/#components)\n",
    "\n",
    "This tutorial will focus on a few LangChain components and learn about `chaining`, one of its powerful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "[Prompt Templates](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates) provides templates for designing prompts fed as inputs to the LLM models.\n",
    "It helps us design templates with multiple inputs that are parameterized and reusable.\n",
    "\n",
    "```{note}\n",
    "For this tutorial, we will only cover the use of `String PromptTemplates` as this gives us a finer control over the template string structure, unlike the `ChatPromptTemplate`.\n",
    "```\n",
    "\n",
    "Below is an example of how to use a prompt template. We can import this class from the `langchain-core` package.\n",
    "\n",
    "The `langchain-core` package contains base abstractions of different components and ways to compose them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcf656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### String PromptTemplates\n",
    "\n",
    "The [String PromptTemplates](https://python.langchain.com/v0.2/docs/concepts/#string-prompttemplates) is used to format a string input. By default, the template takes Python `f-string` format. There are currently 2 choices of `template_format` available: `f-string` and `jinja2`. Later we will see the use of `jinja2` format. In the example below, we will use the `f-string` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mars in the solar system is the '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{planet_name} in the solar system is the \"\n",
    ")\n",
    "\n",
    "prompt_template.format(planet_name=\"Mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ac98b",
   "metadata": {},
   "source": [
    "Let's instantiate our OLMo model like in the [previous section](./2-llms-and-prompt-engineering-with-olmo.ipynb#introduction) of the tutorial with `llama-cpp-python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cf3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from ssec_tutorials import download_olmo_model\n",
    "from ssec_tutorials.scipy_conf import parse_text_generation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at /Users/lsetiawan/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "OLMO_MODEL = (\n",
    "    download_olmo_model()\n",
    ")  # It won't actually download again if it's already there\n",
    "olmo = Llama(model_path=str(OLMO_MODEL), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf3d2e",
   "metadata": {},
   "source": [
    "Now that we have our model ready to go, let's try different prompt templating starting from the previous prompt template with an input of `planet_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=prompt_template.format(planet_name=\"Mars\"),\n",
    "    temperature=0.2,\n",
    "    max_tokens=8,\n",
    "    echo=True,\n",
    ")  # Generate a completion, can also call olmo.create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars in the solar system is the \n",
      "4th planet from the sun.\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Size of the planet Earth is '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{entity_1} of the planet {entity_2} is \"\n",
    ")\n",
    "prompt_template.format(entity_1=\"Size\", entity_2=\"Earth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=prompt_template.format(entity_1=\"Size\", entity_2=\"Earth\"),\n",
    "    temperature=0.2,\n",
    "    echo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the planet Earth is \n",
      "about 12,742 km in diameter. The diameter of the orbit of\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10abf9",
   "metadata": {},
   "source": [
    "#### Your turn üòé\n",
    "\n",
    "Create a `StringPromptTemplate` that outputs some text generation prompt, for example, \"Sun is part of galaxy ...\".\n",
    "\n",
    "Feel free to experiment with the built in [Python `f-string` ](https://docs.python.org/3.11/tutorial/inputoutput.html#formatted-string-literals) for the `prompt` input argument to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cdc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your prompt_template and model_response code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## LLM Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "LangChain provides us with a standardized interface for loading the LLM model. Once standardized, we can use the same methods across models from different providers to call/invoke functions, enabling reusability.\n",
    "\n",
    "Loading the model via [LangChain's LlamaCpp](https://python.langchain.com/v0.2/docs/integrations/llms/llamacpp/) abstraction enables us to use the `chaining` feature. This class is part of the `langchain-community` package, which contains third party integrations that are maintained by the LangChain community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    temperature=0.8,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5710a",
   "metadata": {},
   "source": [
    "As you can see below, we now have a `LlamaCpp` Langchain object rather than the `Llama` llama-cpp-python object from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e617a3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.llms.llamacpp.LlamaCpp"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(olmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b730d9e",
   "metadata": {},
   "source": [
    "If you'd like to access the base object `Llama` object from the `llama-cpp-python` package, you can access it via the `.client` attribute of the `LlamaCpp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d56ddd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_cpp.llama.Llama"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(olmo.client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a1da2",
   "metadata": {},
   "source": [
    "With access to the underlying `Llama` object, you can directly retrieve any metadata information. In this example, we are retrieving OLMo's tokenizer chat template we saw in the [previous notebook](./2-llms-and-prompt-engineering-with-olmo.ipynb#chat-completion) to setup a String PromptTemplate.\n",
    "\n",
    "The built in model's chat template is using [`jinja2`](https://jinja.palletsprojects.com/en/3.1.x/) templating syntax, which is a popular templating engine for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"], template_format=\"jinja2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31784193",
   "metadata": {},
   "source": [
    "The `PromptTemplate` object has 2 main attributes that are very useful to explore the built-in prompt template of the model:\n",
    "- `input_variables`: This is a list of all the input variables that the prompt template expects.\n",
    "- `template`: This is the actual template string that the model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d801b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_generation_prompt', 'eos_token', 'messages']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3af2bc",
   "metadata": {},
   "source": [
    "For this particular template, we can see that it expects `add_generation_prompt`, `eos_token` and `messages`. But what are the variable types for these inputs? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ebacd",
   "metadata": {},
   "source": [
    "We can answer the questions above by looking at the template string itself. The template string is using the jinja2 templating engine syntax, so it may look confusing at first, but at the end of the day it's essentially just some python code in a template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4822b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ eos_token }}{% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79759be0",
   "metadata": {},
   "source": [
    "As we can see above, the template reads as follows:\n",
    "- `eos_token` is a string that is added at the top of the resulting string after prompt is formatted.\n",
    "You can also see that `eos_token` is used to append `content` string values from an `assistant` `role`.\n",
    "You can find this value by going to the Model's [`tokenizer_config.json`](https://huggingface.co/allenai/OLMo-7B-Instruct-hf/blob/main/tokenizer_config.json#L233) file and looking for the `eos_token` key. *Unfornately, this is currently the only way to get this information, you can go to https://github.com/ggerganov/llama.cpp/issues/5040 for more details.* In our case, the `eos_token` is `<|endoftext|>`.\n",
    "- `messages` is a list of dictionary that is iterated over. As you can see that this dictionary should contain a `role` and `content` key.\n",
    "- `add_generation_prompt` is a boolean that is used to determine whether to add a generation prompt or not. In this case, when it's the last message and `add_generation_prompt` is `True`, it will add `<|assistant|>` string to the end of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad5f5c",
   "metadata": {},
   "source": [
    "Now that we know what the template expects we can create the final prompt string by passing in the expected input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\n\\n<|user|>\\nYou are a helpful assistant. Tell me a joke about cats\\n\\n\\n<|assistant|>\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "        }\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9525fc6",
   "metadata": {},
   "source": [
    "The output string above contains the necessary signifier tokens for the OLMo Model to understand what the user input is and where the model should put generated responses. This whole string output will then become the full prompt for the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "\n",
    "Chaining allows us to combine multiple components, as described above, in series or parallel to develop a multi-step LLM pipeline.\n",
    "As shown in the image below, any number of components can be linked together to form a chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "![LancChain Chain](../../images/langchain-chain.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "\n",
    "Image Source: [www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2023/10/a-comprehensive-guide-to-using-chains-in-langchain/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Internally, the chain works like below:\n",
    "\n",
    "STEP 1: Dictionary is processed as an input to the prompt template.  \n",
    "STEP 2: Prompt Template reads the variables to form the prompt text as output - \"What are stars and moon?\"  \n",
    "STEP 3: The prompt is given as input to the LLM model.  \n",
    "STEP 4: LLM Model produces output.  \n",
    "STEP 5: The output goes through StrOutputParser that parses it into string and gives the result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We can use the pipe operator (\"|\"), which is part of the [LCEL(Lang Chain Expression Language)](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel). The pipe operator sequentially arranges each component, similar to the above image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Why did the cat like to listen to jazz music? Because it meows a lot, but in a good way! üòâ\\n\\nI hope this silly joke made you smile! Do you want to hear another one? ‚ùì If not, no worries ‚Äî I'm here to help with whatever information or assistance you need. üòä\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the prompt as expected by OLMo\n",
    "llm_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "            }\n",
    "        ],\n",
    "        \"add_generation_prompt\": True,\n",
    "        \"eos_token\": \"<|endoftext|>\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Instead of having to invoke `llm_chain` repeatedly with `add_generation_prompt` and `eos_token`, we can update our `prompt_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using OLMo's tokenizer chat template we saw in module 1, but this time use partial variables.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Let's stream the output instead of waiting for OLMo to generate and display the text. We can use [Callbacks](https://python.langchain.com/v0.2/docs/concepts/#callbacks) to subscribe to various events in your LLM application pipeline. Check [this out](https://python.langchain.com/v0.1/docs/modules/callbacks/#callback-handlers) for a list of events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the cat like to listen to music on its telephone? Because it made a cute phoneoony when you called! (Did I do that right?) Cats and phones have been making the sound of the tunee (that's an up down beep sounds combined into one, for those not in the know!) since before the invention of the telephone, so this one might be a bit old-school. But it's still purrfectly fitting! üòªüêàüì±üéâüí¨"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Why did the cat like to listen to music on its telephone? Because it made a cute phoneoony when you called! (Did I do that right?) Cats and phones have been making the sound of the tunee (that's an up down beep sounds combined into one, for those not in the know!) since before the invention of the telephone, so this one might be a bit old-school. But it's still purrfectly fitting! üòªüêàüì±üéâüí¨\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config={\"callbacks\": [StreamingStdOutCallbackHandler()]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "We will cover more LangChain concepts in upcoming notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90d26f",
   "metadata": {},
   "source": [
    "#### Your turn üòé\n",
    "\n",
    "Try different messages value(s) and see how the output changes. But remember to follow the template structure.\n",
    "The dictionary keys must contain `role` and `content` and the allowed `role` values are only `user` and `assistant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0ee3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your llm_chain.invoke code here, feel free to also, create your own template and try partial_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
