{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb54058-bf40-4ecd-bb10-5eb3e6c5aa8b",
   "metadata": {},
   "source": [
    "# LangChain: The LLM Application Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59c6a8-cddf-4308-9b7a-9d5b3f09edcb",
   "metadata": {},
   "source": [
    "LangChain, an open-source library, empowers developers by providing a standardized and structured interface for building and integrating various components of an LLM Application. Its model-agnostic nature allows for compatibility with models from multiple LLM providers, including OpenAI, HuggingFace, and others. \n",
    "\n",
    "Using Langchain allows us to build (\"like a chain\") reusable components as part of complex multi-step LLM-based applications clearly and succinctly. \n",
    "\n",
    "You can learn about different [LangChain components here.](https://python.langchain.com/v0.2/docs/concepts/#components)\n",
    "\n",
    "This tutorial will focus on a few LangChain components and learn about `chaining,` one of its powerful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a8931-4611-430d-9658-22bad9d4373f",
   "metadata": {},
   "source": [
    "## [Prompt templates](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e409cf4-0799-4e03-8a7f-94e851270fe6",
   "metadata": {},
   "source": [
    "Prompt Templates provides templates for designing prompts fed as inputs to the LLM models.\n",
    "It helps us design templates with multiple inputs that are parameterized and reusable.\n",
    "\n",
    "Below is an example of how to use a prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec2dd16-014b-46ef-ae37-82fa4356eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38941d0-8f60-4cd3-a7b9-b6a7aee392a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from llama_cpp import Llama\n",
    "from ssec_tutorials import OLMO_MODEL\n",
    "from ssec_tutorials.scipy_conf import * # Contains helper methods for tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a69bd2-8999-47b3-8226-e68144f372be",
   "metadata": {},
   "source": [
    "### [String PromptTemplates](https://python.langchain.com/v0.2/docs/concepts/#string-prompttemplates)\n",
    "\n",
    "Prompt templates to format a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107d0541-b8da-4520-9f0e-a3f674188753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mars in the solar system is the '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"{planet_name} in the solar system is the \")\n",
    "\n",
    "prompt_template.format(planet_name=\"Mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd0f0165-ec47-4794-9cef-f9859e038891",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = Llama(model_path=str(OLMO_MODEL), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87fc4ce-176f-4f3a-8bf8-565934d772f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=prompt_template.format(planet_name=\"Mars\"), \n",
    "    temperature=0.2, \n",
    "    max_tokens = 8, \n",
    "    echo=True\n",
    ") # Generate a completion, can also call olmo.create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d96cd4-ba97-4568-96c7-63fcfc64ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars in the solar system is the \n",
      "* fourth planet from the sun.\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368d12c1-2994-479a-ac41-6c68328add24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Size of the planet Earth is '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example\n",
    "prompt_template = PromptTemplate.from_template(\"{entity_1} of the planet {entity_2} is \")\n",
    "prompt_template.format(entity_1 = \"Size\",  entity_2 = \"Earth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce77dcf-90a3-4141-9225-09c64768657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=prompt_template.format(entity_1 = \"Size\",  entity_2 = \"Earth\"), \n",
    "    temperature=0.2, \n",
    "    echo=True\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248589f7-ebed-45d3-8762-07bf54bd0eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the planet Earth is \n",
      "5,147 km (3,175 mi) in diameter. The diameter\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588714ce-5b4f-4f81-ac56-fd29c765e719",
   "metadata": {},
   "source": [
    "## LLM Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e438756-3ce3-480b-a6f8-2b6dde618893",
   "metadata": {},
   "source": [
    "LangChain provides us with a standardized interface for loading the LLM model. Once standardized, we can use the same methods across models from different providers to call/invoke functions, enabling reusability.\n",
    "\n",
    "Loading the model via [LangChain's LlamaCpp](https://python.langchain.com/v0.1/docs/integrations/llms/llamacpp/) abstraction enables us to use the `chaining` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d160d1ac-0e25-4910-8022-48502ba561c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc31e535-73b0-4478-86d0-e74ac2c2d9a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    temperature=0.8,\n",
    "    #stop=[\".\"],\n",
    "    verbose=False,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3425c847-c58a-471b-b8e3-4ac48be1b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using OLMo's tokenizer chat template we saw in module 1.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata['tokenizer.chat_template'], \n",
    "    template_format=\"jinja2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8b9d8e7-29a8-463e-adc0-83db581bdbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\n\\n<|user|>\\nYou are a helpful assistant. Tell me a joke about cats\\n\\n\\n<|assistant|>\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how you can format the prompt message\n",
    "prompt_template.format(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"You are a helpful assistant. Tell me a joke about cats\"}\n",
    "    ], \n",
    "    add_generation_prompt=True, \n",
    "    eos_token=\"<|endoftext|>\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63087c58-9b6d-4d9e-9fc9-813686727d85",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "\n",
    "Chaining allows us to combine multiple components, as described above, in series or parallel to develop a multi-step LLM pipeline.\n",
    "As shown in the image below, any number of components can be linked together to form a chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c421c57-2b64-43f1-9ad0-88bdc61b660a",
   "metadata": {},
   "source": [
    "![LancChain Chain](../../images/langchain-chain.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "396fae7f-aa38-4361-a9b9-c474ddd73c4d",
   "metadata": {},
   "source": [
    "\n",
    "Image Source: [www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2023/10/a-comprehensive-guide-to-using-chains-in-langchain/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb666227-7ab5-4e8e-91f7-a0973d41891a",
   "metadata": {},
   "source": [
    "Internally, the chain works like below:\n",
    "\n",
    "STEP 1: Dictionary is processed as an input to the prompt template.  \n",
    "STEP 2: Prompt Template reads the variables to form the prompt text as output - \"What are stars and moon?\"  \n",
    "STEP 3: The prompt is given as input to the LLM model.  \n",
    "STEP 4: LLM Model produces output.  \n",
    "STEP 5: The output goes through StrOutputParser that parses it into string and gives the result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7689b-41e2-4644-adc2-77585ae26e90",
   "metadata": {},
   "source": [
    "We can use the pipe operator (\"|\"), which is part of the [LCEL(Lang Chain Expression Language)](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel). The pipe operator sequentially arranges each component, similar to the above image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03519456-c823-43d0-97de-8223a2c3a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a121065f-60fd-4f66-a9c9-9394bb7b623c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sure, here's a classic cat joke:\\n\\nWhy don't cats play poker in the jungle? Too many leopards!\\n\\nI hope you found this joke amusing. Do you want to hear another one?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the prompt as expected by OLMo\n",
    "llm_chain.invoke({\n",
    "    \"messages\":\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"You are a helpful assistant. Tell me a joke about cats\"\n",
    "        }\n",
    "    ], \n",
    "    \"add_generation_prompt\": True, \n",
    "    \"eos_token\": \"<|endoftext|>\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e054c9c-bc53-4c9b-9475-0246ed5363be",
   "metadata": {},
   "source": [
    "Instead of having to invoke `llm_chain` repeatedly with `add_generation_prompt` and `eos_token`, we can update our `prompt_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a62c8a0c-5132-4161-9e13-3cdcec6ded8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using OLMo's tokenizer chat template we saw in module 1, but this time use partial variables. \n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata['tokenizer.chat_template'], \n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aa75f57-da6f-469f-8f6f-3c79574bd937",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef754c6-490f-4f0a-a96c-fa5eca69b366",
   "metadata": {},
   "source": [
    "Let's stream the output instead of waiting for OLMo to generate and display the text. We can use [Callbacks](https://python.langchain.com/v0.2/docs/concepts/#callbacks) to subscribe to various events in your LLM application pipeline. Check [this out](https://python.langchain.com/v0.1/docs/modules/callbacks/#callback-handlers) for a list of events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e85cfb99-3602-4ade-b613-129c4f1f5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "effb3f60-fe64-4b7c-881f-2628a1d46067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here's a cute cat joke for you:\n",
      "\n",
      "Why did the cat avoid the laser pointer? Because he knew you'd be there when he came down!\n",
      "\n",
      "I hope that made you smile. Do you want to hear another one? 🐱😊"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure, here's a cute cat joke for you:\\n\\nWhy did the cat avoid the laser pointer? Because he knew you'd be there when he came down!\\n\\nI hope that made you smile. Do you want to hear another one? 🐱😊\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\n",
    "    {\n",
    "        \"messages\":\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config={\n",
    "        'callbacks' : [StreamingStdOutCallbackHandler()]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcd203-d279-4d00-ae91-73a4c42c7390",
   "metadata": {},
   "source": [
    "We will cover more LangChain concepts in upcoming notebooks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
