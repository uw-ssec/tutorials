{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Astrophysics Chat Application "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This module will create an application that allows other users to interact with the application built in the previous module.\n",
    "This can be accomplished via any frontend such as Django, Flask or Streamlit. \n",
    "\n",
    "The example code below uses Panel, a powerful open source Python library to create web-based apps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include relevant libraries and packages\n",
    "\n",
    "import textwrap\n",
    "from uuid import uuid4\n",
    "import warnings\n",
    "\n",
    "import panel as pn\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.callbacks import CallbackManager\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from ssec_tutorials import OLMO_MODEL, QDRANT_PATH, QDRANT_COLLECTION_NAME\n",
    "from ssec_tutorials import download_olmo_model, download_qdrant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_olmo_model()\n",
    "download_qdrant_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_path = QDRANT_PATH\n",
    "qdrant_collection = QDRANT_COLLECTION_NAME\n",
    "model_path = OLMO_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke caching in panel using python cache decorator for the expensive db load operation that follows\n",
    "@pn.cache\n",
    "def get_vector_store():\n",
    "    # If the Qdrant Vector Database Collection already exists, load it\n",
    "    client = QdrantClient(path=str(qdrant_path))\n",
    "    db = Qdrant(client=client, collection_name=qdrant_collection, embeddings=embedding)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(callbacks):\n",
    "    # Set up retriever using mmr (maximal marginal relevance, a typical information retrieval tactic) search to return top 2 results\n",
    "    retriever = db.as_retriever(\n",
    "        callbacks=callbacks, search_type=\"mmr\", search_kwargs={\"k\": 2}\n",
    "    )\n",
    "\n",
    "    # Callbacks support token-wise streaming\n",
    "    callback_manager = CallbackManager(callbacks)\n",
    "    # LLaMacpp is a Python library built around LLaMa.cpp that implements Metaâ€™s LLaMa architecture in efficient C/C++\n",
    "    olmo = LlamaCpp(\n",
    "        model_path=str(model_path),\n",
    "        callback_manager=callback_manager,\n",
    "        # Temperature controls the randomness of the model's output\n",
    "        temperature=0.8,\n",
    "        # n_ctx limits the length of the input context\n",
    "        n_ctx=4096,\n",
    "        # max_tokens limits the length of the generated text\n",
    "        max_tokens=512,\n",
    "        verbose=False,\n",
    "        # echo determines whether the input prompt should be included in the output\n",
    "        echo=False,\n",
    "    )\n",
    "\n",
    "    # Set up the prompt template using text based jinja2 format\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "        template_format=\"jinja2\",\n",
    "        partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    "    )\n",
    "\n",
    "    transformed_prompt_template = PromptTemplate.from_template(\n",
    "        prompt_template.partial(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": textwrap.dedent(\n",
    "                        \"\"\"\\\n",
    "                    You are an astrophysics expert. Please answer the question on astrophysics based on the following context:\n",
    "                    \n",
    "                    {context}\n",
    "                    \n",
    "                    Question: {question}\"\"\"\n",
    "                    ),\n",
    "                }\n",
    "            ]\n",
    "        ).format()\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        return text\n",
    "\n",
    "    def show_docs(docs):\n",
    "        # https://github.com/langchain-ai/langchain/issues/7290\n",
    "        for callback in callbacks:\n",
    "            callback.on_retriever_end(docs, run_id=uuid4())\n",
    "        return docs\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever | show_docs | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | transformed_prompt_template\n",
    "        | olmo\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(contents, user, instance):\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(\n",
    "        instance, user=\"OLMo\", avatar=\"ðŸŒ³\"\n",
    "    )\n",
    "    # Not return the result at the end of the generation\n",
    "    # this prevents the model from repeating the result\n",
    "    callback_handler.on_llm_end = lambda response, *args, **kwargs: None\n",
    "    chain = get_chain(callbacks=[callback_handler])\n",
    "    _ = await chain.ainvoke(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.chat.ChatInterface(callback=callback).servable()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
