{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Astrophysics Chat Application "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This module will create an application that allows other users to interact with the application with the previous knowledge from earlier modules.\n",
    "This can be accomplished via any Python frontend framework such as Streamlit or Panel. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb355b",
   "metadata": {},
   "source": [
    "For this demo we will use Panel, a powerful open source Python library to create interactive dashboards in Jupyter notebooks or standalone apps.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1c043",
   "metadata": {},
   "source": [
    "## What is Panel?\n",
    "\n",
    "[Panel](https://panel.holoviz.org/) is a part of the [HoloViz](https://holoviz.org/) ecosystem, which is a set of tools that make it easier to work with large datasets and create interactive visualizations. This ecosystem consists of the following tools beyond Panel:\n",
    "\n",
    "- [hvPlot](https://hvplot.holoviz.org/) to quickly generate interactive plots from your data\n",
    "- [HoloViews](https://holoviews.org/) to help you make all of your data instantly visualizable\n",
    "- [GeoViews](https://geoviews.org/) to extend HoloViews for geographic data\n",
    "- [Datashader](https://datashader.org/) for rendering even the largest datasets\n",
    "- [Lumen](https://lumen.holoviz.org/) to build data-driven dashboards from a simple YAML specification\n",
    "- [Param](https://param.holoviz.org/) to create declarative user-configurable objects\n",
    "- [Colorcet](https://colorcet.holoviz.org/) for perceptually uniform colormaps\n",
    "\n",
    "The nice thing about this ecosystem is that all of these tools are designed to work together as part of the [Pandata scalable open-source analysis stack](https://github.com/panstacks/pandata?tab=readme-ov-file), so you can easily integrate them as needed.\n",
    "\n",
    "![Pandata Stack](https://raw.githubusercontent.com/panstacks/pandata/main/assets/pandata_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31fff8",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d61ec8",
   "metadata": {},
   "source": [
    "Now that you've learned the various parts of the Retrievel Augmented Generation (RAG) based approach, let's put it all together in a simple application that allows users to interact with the model and ask questions about astrophysics, rather than just having a bunch of code that you have to run over and over for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711e538",
   "metadata": {},
   "source": [
    "### First, Include relevant libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from uuid import uuid4\n",
    "import warnings\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.callbacks import CallbackManager, BaseCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from ssec_tutorials import OLMO_MODEL, QDRANT_PATH, QDRANT_COLLECTION_NAME\n",
    "from ssec_tutorials import download_olmo_model, download_qdrant_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525219ec",
   "metadata": {},
   "source": [
    "Because langchain is a fast developing library, things might change, and we'll get a lot of warnings.\n",
    "For now, we'll ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73432941",
   "metadata": {},
   "source": [
    "Panel has many pre-built [frontend components](https://panel.holoviz.org/reference/index.html#) you can put together like legos with Python.\n",
    "So this allows us to create frontend without any Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec980ef",
   "metadata": {},
   "source": [
    "### Download the model and vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61761341",
   "metadata": {},
   "source": [
    "If you've gone through the previous modules, you should have the model and vector database downloaded.\n",
    "The below code will try to download the model and vector database if you haven't already,\n",
    "otherwise it will load them from their previous location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = download_olmo_model()\n",
    "qdrant_path = download_qdrant_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a0f39",
   "metadata": {},
   "source": [
    "Now we're setting the variables to the Qdrant Vector Database collection name.\n",
    "This value is retrieved from global variables within the `ssec_tutorials` utility library that we've provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_collection = QDRANT_COLLECTION_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2c033",
   "metadata": {},
   "source": [
    "### Loading the embedding model and vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caad4c",
   "metadata": {},
   "source": [
    "As seen in module 2, we can't have a RAG system without an embedding model and a vector database.\n",
    "So let's now load the [`all-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) model from HuggingFace and the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0875c49",
   "metadata": {},
   "source": [
    "Now we can create a Langchain Qdrant Object that allows for retrieval of documents\n",
    "from the Qdrant database using the specified embedding model instantiated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Qdrant.from_existing_collection(\n",
    "    collection_name=qdrant_collection, embedding=embedding, path=qdrant_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e33759",
   "metadata": {},
   "source": [
    "### Setup the Langchain Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e8550",
   "metadata": {},
   "source": [
    "The function `get_chain` below will setup the Langchain chain with the model and vector database using the various learnings from the previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(callback_handlers: list[BaseCallbackHandler], input_prompt_template: str):\n",
    "    # 1. Set up the vector database retriever.\n",
    "    # This line of code will create a retriever object that\n",
    "    # will be used to retrieve documents from the vector database.\n",
    "    retriever = db.as_retriever(\n",
    "        callbacks=callback_handlers,  # pass the result of the retrieval to the callback handler\n",
    "        search_type=\"mmr\",  # the mmr (maximal marginal relevance, a typical information retrieval tactic) search\n",
    "        search_kwargs={\"k\": 2},  # return top 2 results\n",
    "    )\n",
    "\n",
    "    # 2. Setup the Langchain callback manager to handle callbacks from Langchain LLM object.\n",
    "    # At which results are passed to the callback handler.\n",
    "    callback_manager = CallbackManager(callback_handlers)\n",
    "\n",
    "    # 3. Setup the Langchain llama.cpp model object.\n",
    "    # In our case, we are using the `OLMo-7B-Instruct` model.\n",
    "    # llama-cpp-python is a Python binding for llama.cpp C++ library as mentioned in previous modules.\n",
    "    olmo = LlamaCpp(\n",
    "        model_path=str(model_path),  # the path to the OLMo model in GGUF file format\n",
    "        callback_manager=callback_manager,  # set the callback manager to handle callbacks\n",
    "        temperature=0.8,  # set the randomness of the model's output\n",
    "        n_ctx=4096,  # set limit for the length of the input context\n",
    "        max_tokens=512,  # set limit for the length of the generated text\n",
    "        verbose=False,  # determines whether the model should print out debug information\n",
    "        echo=False,  # determines whether the input prompt should be included in the output\n",
    "    )\n",
    "\n",
    "    # 4. Set up the initial Langchain Prompt Template using text based jinja2 format\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        template=olmo.client.metadata[\n",
    "            \"tokenizer.chat_template\"\n",
    "        ],  # get the chat template from the model metadata\n",
    "        template_format=\"jinja2\",  # set the template format to jinja2\n",
    "        partial_variables={\n",
    "            \"add_generation_prompt\": True,  # add generation prompt to the template, this option is from the model metadata\n",
    "            \"eos_token\": \"<|endoftext|>\",  # set the end of sentence token\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 5. Transform the Prompt Template to include the user role and the context\n",
    "    # This will allow the model to generate text based on the context provided.\n",
    "    # However, after setting this new template, the model will be limited to\n",
    "    # generating text based on the created prompt template with input of\n",
    "    # `context` and `question` keys.\n",
    "    transformed_prompt_template = PromptTemplate.from_template(\n",
    "        prompt_template.partial(\n",
    "            # The default chat template takes a list of messages with a role and content\n",
    "            # to setup this particular app, we will only pass a single message with the user role\n",
    "            # and the input prompt content\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",  # set the role to user, this allows for user input to be passed to the model\n",
    "                    \"content\": input_prompt_template,  # the input prompt template, must have `context` and `question` keys to work\n",
    "                }\n",
    "            ]\n",
    "        ).format()\n",
    "    )\n",
    "\n",
    "    # 6. Define the `format_docs` function to format the retrieved Langchain documents object to simple string\n",
    "    def format_docs(docs):\n",
    "        text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        return text\n",
    "\n",
    "    # 7. Define the `show_docs` function to display the retrieved documents to app panel\n",
    "    # this is currently a small hack to display the retrieved documents to the app panel\n",
    "    # as mentioned in https://github.com/langchain-ai/langchain/issues/7290\n",
    "    def show_docs(docs):\n",
    "        for callback_handler in callback_handlers:\n",
    "            callback_handler.on_retriever_end(\n",
    "                docs,  # pass the retrieved documents to the callback handler\n",
    "                run_id=uuid4(),  # generate a random run id\n",
    "            )\n",
    "        return docs\n",
    "\n",
    "    # 8. Return the Langchain chain object\n",
    "    # The way the chain reads is as follows:\n",
    "    return (\n",
    "        {\n",
    "            # The Vector Database retriever documents,\n",
    "            # which is then passed to the `show_docs` function,\n",
    "            # which is then passed to the `format_docs` function for formatting\n",
    "            \"context\": retriever | show_docs | format_docs,\n",
    "            # The Question asked by the user from the Chat Text Input Interface is passed in as well\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        # The dictionary above that contains text values for `context` and `question` is now passed\n",
    "        # to the transformed prompt template so that the final prompt text can be generated\n",
    "        | transformed_prompt_template\n",
    "        # The full final prompt text with both context and question is passed to the OLMo model\n",
    "        # for generation of the final output. Note that this final prompt text cannot exceed the maximum\n",
    "        # `n_ctx` input context value set in the OLMo model above.\n",
    "        | olmo\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0e264",
   "metadata": {},
   "source": [
    "That was a lot of code to get through, but now we have setup the Langchain chain for a RAG system. However, at this point, we still need an input prompt template that include some instruction and placeholders for the input `question` and `context`. The example below, for an Astrophysics Chat Application, we will give an instruction of: `You are an astrophysics expert. Please answer the question on astrophysics based on the following context:`, with a place for where the context text should go as well as the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0820c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt_template = textwrap.dedent(\n",
    "    \"\"\"\\\n",
    "You are an astrophysics expert. Please answer the question on astrophysics based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a991ce9",
   "metadata": {},
   "source": [
    "### Setup the Panel Chat Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4adf4",
   "metadata": {},
   "source": [
    "At this point, we've setup the LLM model, the embedding model, and the vector database in a chain.\n",
    "In the most basic form, these are the components needed to create a RAG system.\n",
    "\n",
    "![RAG System](https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-scaled.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615e0bf",
   "metadata": {},
   "source": [
    "Now we will use that chain workflow to create a simple chat application using Panel. In the diagram above, this would be our \"Enterprise App\", but obviously much simpler and not ready for production at this stage.\n",
    "\n",
    "To begin, we will setup the asynchronous `callback` function for the [`pn.chat.ChatInterface`](https://panel.holoviz.org/reference/chat/ChatInterface.html) layout component. This will allow us to interact with the chat interface and ask questions.\n",
    "\n",
    "The `ChatInterface` is a high-level layout, it provides front-end interface for inputting different kinds of messages: text, images, PDFs, etc.\n",
    "\n",
    "This layout provides front-end methods to:\n",
    "\n",
    "- Input (append) messages to the chat log.\n",
    "- Re-run (resend) the most recent user input ChatMessage.\n",
    "- Remove messages until the previous user input ChatMessage.\n",
    "- Clear the chat log, erasing all ChatMessage objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(contents, user, instance):\n",
    "    # 1. Create a panel callback handler\n",
    "    # The Langchain PanelCallbackHandler is useful for rendering and streaming the chain of thought\n",
    "    # from Langchain objects like Tools, Agents, and Chains.\n",
    "    # It inherits from Langchain’s BaseCallbackHandler.\n",
    "    # Here we set the user to be the model name \"OLMo\" with an avatar of a tree emoji \"🌳\"\n",
    "    # for the tree of knowledge.\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(\n",
    "        instance, user=\"OLMo\", avatar=\"🌳\"\n",
    "    )\n",
    "\n",
    "    # 2. Set to not return the full generated result at the end of the generation;\n",
    "    # this prevents the model from repeating the result in the interface\n",
    "    callback_handler.on_llm_end = lambda response, *args, **kwargs: None\n",
    "\n",
    "    # 3. Create and setup the Langchain chain object with the callback handler and input prompt template\n",
    "    chain = get_chain(\n",
    "        callback_handlers=[callback_handler],\n",
    "        input_prompt_template=input_prompt_template,\n",
    "    )\n",
    "\n",
    "    # 4. Run the chain with the input contents\n",
    "    _ = await chain.ainvoke(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ec111",
   "metadata": {},
   "source": [
    "Once we have a `callback` function, now we're ready to pass that to the `ChatInterface` layout component.\n",
    "\n",
    "The code below takes in the asynchronous `callback` function from above and serves the chat interface to the user. This callback function will run every time the user sends a message in the chat interface. The callback function `callback` will receive the input text as part of the `contents`. The `contents` will be passed to the Langchain chain where:\n",
    "1. the retriever will fetch document based on the input text\n",
    "2. generate prompt with instruction, document results, and question input text\n",
    "3. generate answer based on the prompt\n",
    "4. return the generated answer and retrieved document text to the user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.chat.ChatInterface(callback=callback).servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8805d5",
   "metadata": {},
   "source": [
    "If you're running this notebook on JupyterLab, there should be a Panel logo in the menu bar of your notebook.\n",
    "You can clear output and restart the kernel, then enable the *Preview* for this app by clicking on Panel’s logo in the menu bar of your notebook. Once clicked, you should see a new tab being opened next to your notebook tab, and after some moment your app will be rendered in this tab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
