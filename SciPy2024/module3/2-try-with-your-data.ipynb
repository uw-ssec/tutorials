{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Try with your Data\n",
    "\n",
    "Now it's your turn to apply your data and specific domain knowledge.\n",
    "\n",
    "You can use this notebook as a starting point and adapt it to your needs.\n",
    "You will need to develop the pre-processing stage for a RAG system.\n",
    "This includes document retrieval, cleaning, chunking,\n",
    "and ingestion into the vector database using an embedding model.\n",
    "\n",
    "To help you, we've provided a few example code snippets in Jupyter notebooks found in the \n",
    "[`appendix`](../appendix/index.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624653f",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "A section for whatever utility functions you need. We have packaged up our utility functions in a Python package called `ssec_tutorials`. You can find the source code in this [GitHub repository](https://github.com/uw-ssec/ssec_tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc327339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for whatever utility functions you need. This can be anything such as\n",
    "# cleaning up document format, setting up prompt templates, etc.\n",
    "\n",
    "# Uncomment the following for a simple document formatting function\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105a812",
   "metadata": {},
   "source": [
    "## Retrieve documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf9ba5",
   "metadata": {},
   "source": [
    "A section for document retrieval. This just means getting your document from whatever sources,\n",
    "in your local computer or the internet. See the [Document Loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/) integration list from Langchain for an extensive list of what's possible.\n",
    "\n",
    "For the purpose of this tutorial, we recommend a simple example of loading a piece of text from a file such as PDF. Also, if you have a large piece of text, you can split it into smaller chunks using Langchains's [RecursiveTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/).\n",
    "\n",
    "If you don't have any data with you, you can try out with this [Algorithm Textbook by Jeff Erickson](http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf). This textbook has been generously made available by Jeff Erickson under the [Creative Commons Attribution 4.0 International license](http://creativecommons.org/licenses/by/4.0/), you can find more information about the textbook at [https://jeffe.cs.illinois.edu/teaching/algorithms/](https://jeffe.cs.illinois.edu/teaching/algorithms/).\n",
    "\n",
    "```{note}\n",
    "If you're running things on Codespace, [refer to this link](https://stackoverflow.com/questions/62284623/how-can-i-upload-a-file-to-a-github-codespaces-environment) and upload your data to `resources/` folder. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6994afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for your retrieval step,\n",
    "# see the documentation on PyMuPDF for more information:\n",
    "# https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/#using-pymupdf\n",
    "\n",
    "# Uncomment below for code to download the textbook\n",
    "# import os\n",
    "# from urllib.request import urlretrieve\n",
    "# url = \"http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf\"\n",
    "# filename = os.path.basename(url)\n",
    "\n",
    "# if not os.path.exists(filename):\n",
    "#     # Download if file doesn't exist\n",
    "#     pdf_path, headers = urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434737ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to load the PDF document as a Langchain Document objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca40cd",
   "metadata": {},
   "source": [
    "## Document Embeddings to Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c53d1",
   "metadata": {},
   "source": [
    "Once you've figured out how to retrieve and load your documents to Langchain Document objects, you can then proceed to loading these documents to Qdrant Vector Database collection.\n",
    "\n",
    "See the following documentation for some guidance on [Langchain Qdrant integration](https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35899c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d199a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657c623",
   "metadata": {},
   "source": [
    "### Setup Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to load your data into the database\n",
    "\n",
    "# uncomment below to set the Qdrant path and collection name\n",
    "# for an \"local mode\" on-disk storage\n",
    "# See https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/#on-disk-storage\n",
    "# qdrant_path = \"./my_qdrant_database\"\n",
    "# qdrant_collection = \"algorithms_book\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a6fd7",
   "metadata": {},
   "source": [
    "### Test out the Qdrant collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a6de6",
   "metadata": {},
   "source": [
    "At this step, you should have a Qdrant object (`langchain_qdrant.vectorstores.Qdrant`) that has your document loaded into it in a collection. You can test out the collection by querying for a documents and checking if the results are as expected.\n",
    "\n",
    "To do this, you'll need to create a [`VectorStoreRetriever`](https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82691eec",
   "metadata": {},
   "source": [
    "```{note}\n",
    "A sample question example to ask from the document can be `\"What is the most familiar method for multiplying large numbers?\"`.\n",
    "An answer to this question can be found on page 3, section 0.2 Multiplication, Lattice Multiplication.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934d230",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "You'll probably need to tweak the arguments for creating a `VectorStoreRetriever` object for the best search type and limiting the number of documents. This part is a bit of trial and error, so don't be afraid to experiment. It is a critical part of RAG system to get the right documents for the question as that is what the LLM would use to generate the answer.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to try out the vector database retrieval with a question query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Setup OLMo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb84bc",
   "metadata": {},
   "source": [
    "At this stage now we have the Retrieval-Augmented (RA) in RAG system. Let's now setup the Generation (G) part with the OLMo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b202679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials import download_olmo_model\n",
    "\n",
    "# This will download the OLMO model to the cache directory\n",
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this line to understand your available options for LlamaCpp Class\n",
    "# LlamaCpp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Here we've setup the LlamaCpp model,\n",
    "# but you'll need to add additional arguments to `LlamaCpp`\n",
    "# to make it work for your specific use case\n",
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3181c",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Try asking some questions to OLMo about any content of the document you've loaded in the Qdrant collection.\n",
    "You will find that the OLMo model is not trained on your specific domain, so it might not give you the best results.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dccb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = olmo.invoke(input=\"What is the most familiar method for multiplying large numbers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8e5a9",
   "metadata": {},
   "source": [
    "Rather than a just a simple question, we'll need to refine the prompt to include instruction and context for the model to generate the answer. To do this, we'll need to setup the proper string [PromptTemplate](https://python.langchain.com/v0.2/docs/concepts/#string-prompttemplates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create the initial prompt template using OLMo's tokenizer chat template we saw in module 1.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd751a28",
   "metadata": {},
   "source": [
    "Set the question for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the most familiar method for multiplying large numbers?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef522c06",
   "metadata": {},
   "source": [
    "Set the context for the prompt.\n",
    "This is where you'll need to use the `VectorStoreRetriever` and format the document object with `format_docs`\n",
    "or simply add your own text to the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757265b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment variable below to set the context\n",
    "# context = <Enter code or string here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9569a7",
   "metadata": {},
   "source": [
    "Set the instruction for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06338896",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3c7b",
   "metadata": {},
   "source": [
    "The original OLMo chat template takes in multiple messages with a `role` and `content` key. You can use this template to ask questions to the model. For simplicity, we'll just use a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set the input text template\n",
    "# input_text_template = f\"\"\"\\\n",
    "# {instruction}\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c540731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set the message dictionary\n",
    "# message = {\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": input_text_template,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to try out the prompt template\n",
    "# print(prompt_template.format(\n",
    "#     messages=[message]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4fcab",
   "metadata": {},
   "source": [
    "You can see above what the final prompt looks like. There are tags like `<|user|>` that signify the model that this is a user input and so on. This final string is sent to the model for generating the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4584fdb",
   "metadata": {},
   "source": [
    "At this point you have all the parts for RAG system setup. Now let's chain the prompt engineering, OLMo model and the Qdrant collection to get a more accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to create the retrieval chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f3978",
   "metadata": {},
   "source": [
    "```{admonition} Answer Example Code\n",
    ":class: hint dropdown\n",
    "\n",
    "```{code-block} python\n",
    "# 1. Set the question\n",
    "question = \"What is the most familiar method for multiplying large numbers?\"\n",
    "\n",
    "# 2. Set the context\n",
    "context = format_docs(retriever.invoke(question))\n",
    "\n",
    "# 3. Set the instruction\n",
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\"\n",
    "\n",
    "# 4. Set the input text template\n",
    "input_text_template = f\"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Set the message dictionary\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text_template,\n",
    "}\n",
    "\n",
    "# 6. Chain the prompt template and olmo model\n",
    "llm_chain = prompt_template | olmo\n",
    "\n",
    "# 7. Invoke the chain\n",
    "llm_chain.invoke(input={\"messages\": [message]})\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7df7cd",
   "metadata": {},
   "source": [
    "**Bonus: Try to create a simple chat app, by modifying the [1-olmo-chat-rag.ipynb](./1-olmo-chat-rag.ipynb) notebook with your use case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a5e52",
   "metadata": {},
   "source": [
    "Please fill out the [survey feedback form](https://tinyurl.com/ssecfeedback) to help us improve the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
