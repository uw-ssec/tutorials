{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Try with your Data\n",
    "\n",
    "Now it's your turn to apply your data and specific domain knowledge.\n",
    "\n",
    "You can use this notebook as a starting point and adapt it to your needs.\n",
    "You will need to develop the pre-processing stage for a RAG system.\n",
    "This includes document retrieval, cleaning, chunking,\n",
    "and ingestion into the vector database using an embedding model.\n",
    "\n",
    "To help you, we've provided a few example code snippets in Jupyter notebooks found in the \n",
    "[`appendix`](../appendix/index.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624653f",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "A section for whatever utility functions you need. We have packaged up our utility functions in a Python package called `ssec_tutorials`. You can find the source code in this [GitHub repository](https://github.com/uw-ssec/ssec_tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc327339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for whatever utility functions you need. This can be anything such as\n",
    "# cleaning up document format, setting up prompt templates, etc.\n",
    "\n",
    "\n",
    "# Uncomment the following for a simple document formatting function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105a812",
   "metadata": {},
   "source": [
    "## Retrieve documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf9ba5",
   "metadata": {},
   "source": [
    "A section for document retrieval. This just means getting your document from whatever sources,\n",
    "in your local computer or the internet. See the [Document Loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/) integration list from Langchain for an extensive list of what's possible.\n",
    "\n",
    "For the purpose of this tutorial, we recommend a simple example of loading a piece of text from a file such as PDF. Also, if you have a large piece of text, you can split it into smaller chunks using Langchains's [RecursiveTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/).\n",
    "\n",
    "If you don't have any data with you, you can try out with this [Algorithm Textbook by Jeff Erickson](http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf). This textbook has been generously made available by Jeff Erickson under the [Creative Commons Attribution 4.0 International license](http://creativecommons.org/licenses/by/4.0/), you can find more information about the textbook at [https://jeffe.cs.illinois.edu/teaching/algorithms/](https://jeffe.cs.illinois.edu/teaching/algorithms/).\n",
    "\n",
    "```{note}\n",
    "If you're running things on Codespace, [refer to this link](https://stackoverflow.com/questions/62284623/how-can-i-upload-a-file-to-a-github-codespaces-environment) and upload your data to `resources/` folder. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6994afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for your retrieval step,\n",
    "# see the documentation on PyMuPDF for more information:\n",
    "# https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/#using-pymupdf\n",
    "\n",
    "# Uncomment below for code to download the textbook\n",
    "# import os\n",
    "# from urllib.request import urlretrieve\n",
    "# url = \"http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf\"\n",
    "# filename = os.path.basename(url)\n",
    "\n",
    "# if not os.path.exists(filename):\n",
    "#     # Download if file doesn't exist\n",
    "#     pdf_path, headers = urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434737ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to load the PDF document as a Langchain Document objects\n",
    "from langchain_community.document_loaders import ReadTheDocsLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = ReadTheDocsLoader(\"../../rtdocs\")\n",
    "hls4ml_docs = loader.load_and_split(\n",
    "    RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca40cd",
   "metadata": {},
   "source": [
    "## Document Embeddings to Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c53d1",
   "metadata": {},
   "source": [
    "Once you've figured out how to retrieve and load your documents to Langchain Document objects, you can then proceed to loading these documents to Qdrant Vector Database collection.\n",
    "\n",
    "See the following documentation for some guidance on [Langchain Qdrant integration](https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35899c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d199a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/notebook/envs/notebook/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657c623",
   "metadata": {},
   "source": [
    "### Setup Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d7fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to load your data into the database\n",
    "\n",
    "# uncomment below to set the Qdrant path and collection name\n",
    "# for an \"local mode\" on-disk storage\n",
    "# See https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/#on-disk-storage\n",
    "# qdrant_path = \"./my_qdrant_database\"\n",
    "# qdrant_collection = \"algorithms_book\"\n",
    "\n",
    "from langchain_qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from ssec_tutorials import TUTORIAL_CACHE\n",
    "\n",
    "qdrant_collection = \"hls4ml_docs\"\n",
    "qdrant_path = TUTORIAL_CACHE / \"hls4ml_docs\"\n",
    "\n",
    "client = QdrantClient(path=str(qdrant_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54b934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Qdrant collection 'hls4ml_docs' from 3721 documents\n"
     ]
    }
   ],
   "source": [
    "if qdrant_path.exists():\n",
    "    print(f\"Qdrant Vector Database Collection already exists in {qdrant_path}, load it\")\n",
    "    client = QdrantClient(path=str(qdrant_path))\n",
    "    qdrant = Qdrant(\n",
    "        client=client, collection_name=qdrant_collection, embeddings=embedding\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Creating new Qdrant collection '{qdrant_collection}' from {len(hls4ml_docs)} documents\"\n",
    "    )\n",
    "\n",
    "    # Load the documents into a Qdrant Vector Database Collection\n",
    "    # this will save locally in the qdrant_path as sqlite\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents=hls4ml_docs,\n",
    "        embedding=embedding,\n",
    "        path=str(qdrant_path),\n",
    "        collection_name=qdrant_collection,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a6fd7",
   "metadata": {},
   "source": [
    "### Test out the Qdrant collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a6de6",
   "metadata": {},
   "source": [
    "At this step, you should have a Qdrant object (`langchain_qdrant.vectorstores.Qdrant`) that has your document loaded into it in a collection. You can test out the collection by querying for a documents and checking if the results are as expected.\n",
    "\n",
    "To do this, you'll need to create a [`VectorStoreRetriever`](https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82691eec",
   "metadata": {},
   "source": [
    "```{note}\n",
    "A sample question example to ask from the document can be `\"What is the most familiar method for multiplying large numbers?\"`.\n",
    "An answer to this question can be found on page 3, section 0.2 Multiplication, Lattice Multiplication.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934d230",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "You'll probably need to tweak the arguments for creating a `VectorStoreRetriever` object for the best search type and limiting the number of documents. This part is a bit of trial and error, so don't be afraid to experiment. It is a critical part of RAG system to get the right documents for the question as that is what the LLM would use to generate the answer.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbd18af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Profiling\\uf0c1\\nIn the hls4ml configuration file, it is possible to specify the model Precision and ReuseFactor with fine granularity.', metadata={'source': '../../rtdocs/fastmachinelearning.org/hls4ml/advanced/profiling.html', '_id': 'c55c4f12d72444e094a9221078945c55', '_collection_name': 'hls4ml_docs'}),\n",
       " Document(page_content='Size/Compression - Though not explicitly part of the hls4ml package, this is an important optimization to efficiently use the FPGA resources', metadata={'source': '../../rtdocs/fastmachinelearning.org/hls4ml/api/concepts.html', '_id': '5b5d17f8d6ed4ee2ad4f4f0d8abd5568', '_collection_name': 'hls4ml_docs'}),\n",
       " Document(page_content='(hls4ml.model.optimizer.passes.resize_remove_constants.ResizeRemoveConstants method)\\n(hls4ml.model.optimizer.passes.seperable_to_dw_conv.SeparableToDepthwiseAndConv method)', metadata={'source': '../../rtdocs/fastmachinelearning.org/hls4ml/genindex.html', '_id': 'db4ac50b23e94248ad5098b3c2a374c9', '_collection_name': 'hls4ml_docs'}),\n",
       " Document(page_content='Pruning and weight sharing are effective techniques to reduce model footprint and computational requirements. The hls4ml Optimization API introduces hardware-aware pruning and weight sharing.', metadata={'source': '../../rtdocs/fastmachinelearning.org/hls4ml/advanced/model_optimization.html', '_id': 'd26b48e5701b41c4a4b5db8723e68035', '_collection_name': 'hls4ml_docs'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here to try out the vector database retrieval with a question query\n",
    "\n",
    "retriever = qdrant.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "\n",
    "retriever.invoke(\"What techniques can be used to reduce model footprint in hls4ml?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Setup OLMo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb84bc",
   "metadata": {},
   "source": [
    "At this stage now we have the Retrieval-Augmented (RA) in RAG system. Let's now setup the Generation (G) part with the OLMo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b202679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at /home/mambauser/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "from ssec_tutorials import download_olmo_model\n",
    "\n",
    "# This will download the OLMO model to the cache directory\n",
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bba3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this line to understand your available options for LlamaCpp Class\n",
    "# LlamaCpp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Here we've setup the LlamaCpp model,\n",
    "# but you'll need to add additional arguments to `LlamaCpp`\n",
    "# to make it work for your specific use case\n",
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    verbose=False,\n",
    "    n_ctx=1024,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3181c",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Try asking some questions to OLMo about any content of the document you've loaded in the Qdrant collection.\n",
    "You will find that the OLMo model is not trained on your specific domain, so it might not give you the best results.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39dccb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The steps involve preparing the QONNX model, converting it to ONNX, and then using the converted ONNX model with hls4ml. Here's a high-level overview of the process:\n",
      "\n",
      "1. Prepare your QONNX model: Ensure that the QONNX model is optimized for inference and meets the requirements specified by hls4ml (such as input shape, precision, etc.). Use the QONNX conversion tools to convert it to ONNX format.\n",
      "\n",
      "2. Convert the QONNX model to ONNXML tool:\n",
      "   a. Install the required dependencies (`pip install onnx --upgrade`).\n",
      "   b. Run `onnx2mltools` (provided in hls4ml) on your QONNX model file (`.q6`, .q7, etc.). This will generate an MLIR file and a text-based ONNXML tool representation of the model.\n",
      "\n",
      "   Example:\n",
      "\n",
      "   ```\n",
      "   python -m onnx2mltools --input model.q6\n",
      "   ```\n",
      "\n",
      "   This will create `model.mlir` and `model_onnx.txt`.\n",
      "\n",
      "3. Convert the ONNXML tool representation to MLIR:\n",
      "   a. Install the required dependencies (`pip install onnx --upgrade`).\n",
      "   b. Run `onnxmltools convert` (provided in hls4ml) on your ONNXML model file (`.onnx`).\n",
      "\n",
      "   Example:\n",
      "\n",
      "   ```\n",
      "   python -m onnx2mltools --input model_onnx.txt\n",
      "   python -m onnxmltools convert --mlir-output=model.mlir\n",
      "   ```\n",
      "\n",
      "   This will create `model.mlir` and any required subgraphs in the `model.mlir` file (if applicable).\n",
      "\n",
      "4. Convert the MLIR model to HLS IR:\n",
      "   a. Run `hls4ml convert` on your MLIR model file (`model.mlir`)\n",
      "\n",
      "    Example:\n",
      "\n",
      "    ```\n",
      "    python -m hls4ml convert --input model.mlir\n",
      "    ```\n",
      "\n",
      "   This will generate a HLS IR representation of the model, which can be used to compile it for FPGA hardware.\n",
      "\n",
      "5. Build and run your application on an FPGA board (e.g., Xilinx Zynq or Intel AgileFPGA):"
     ]
    }
   ],
   "source": [
    "_ = olmo.invoke(input=\"What are the steps to use QONNX models in hls4ml?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8e5a9",
   "metadata": {},
   "source": [
    "Rather than a just a simple question, we'll need to refine the prompt to include instruction and context for the model to generate the answer. To do this, we'll need to setup the proper string [PromptTemplate](https://python.langchain.com/v0.2/docs/concepts/#string-prompttemplates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create the initial prompt template using OLMo's tokenizer chat template we saw in module 1.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd751a28",
   "metadata": {},
   "source": [
    "Set the question for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d1b5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What frontends are available for hls4ml?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef522c06",
   "metadata": {},
   "source": [
    "Set the context for the prompt.\n",
    "This is where you'll need to use the `VectorStoreRetriever` and format the document object with `format_docs`\n",
    "or simply add your own text to the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757265b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment variable below to set the context\n",
    "# context = format_docs(retriever.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9569a7",
   "metadata": {},
   "source": [
    "Set the instruction for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06338896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction = \"\"\"You are a computer science professor.\n",
    "# Please answer the following question based on the given context.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3c7b",
   "metadata": {},
   "source": [
    "The original OLMo chat template takes in multiple messages with a `role` and `content` key. You can use this template to ask questions to the model. For simplicity, we'll just use a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set the input text template\n",
    "# input_text_template = f\"\"\"\\\n",
    "# {instruction}\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c540731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set the message dictionary\n",
    "# message = {\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": input_text_template,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to try out the prompt template\n",
    "# print(prompt_template.format(\n",
    "#     messages=[message]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4fcab",
   "metadata": {},
   "source": [
    "You can see above what the final prompt looks like. There are tags like `<|user|>` that signify the model that this is a user input and so on. This final string is sent to the model for generating the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4584fdb",
   "metadata": {},
   "source": [
    "At this point you have all the parts for RAG system setup. Now let's chain the prompt engineering, OLMo model and the Qdrant collection to get a more accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to create the retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9081d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use QONNX models in hls4ml, follow these general steps:\n",
      "\n",
      "1. Conversion from ONNX: First, convert an ONNX model into a format supported by hls4ml using the QONNX frontend and tools like onnx2hll, openvino toolkit, or any other ONNX-compatible converter.\n",
      "\n",
      "   a) YAML file: When passing ONNX configuration through API, you can use yaml files to specify the model's parameters and structure.\n",
      "   b) ONNX model: You may also pass the ONNX model directly using the API.\n",
      "\n",
      "2. Initialize hls4ml models: The QONNX layer is already included in the package; however, it needs initialization before HLS production. To do this, you should have a class-based Quant layer that inherits from Layer.\n",
      "\n",
      "Here's an example of how to initialize an hls4ml model with a QONNX quantization layer:\n",
      "\n",
      "    from hls4ml.model import layers\n",
      "    from hls4ml.model import Program\n",
      "    ...\n",
      "    class CustomLayer(layers.Base, Quant):\n",
      "        pass\n",
      "\n",
      "    config = {\"Input\": (2, 224, 224), \"Stage\": [{\"Block\": 2}], \"Stage\": 0, \"Output\": (2, 224, 224)}\n",
      "    model = Program()\n",
      "\n",
      "    input_shape = (224, 224, 3)\n",
      "    quant_layer = CustomLayer(model, \"InputQuant\", input_shape, config=config)\n",
      "    output_layer = model.apply(quant_layer)\n",
      "\n",
      "    print(\"HLS Model:\", model.to_yaml())\n",
      "\n",
      "3. Optimize the HLS model: After initializing the hls4ml models, you should use optimization techniques provided by hls4ml to reduce the size and improve performance of the resulting binary.\n",
      "\n",
      "For more information on using hls4ml with QONNX models, please refer to our tutorials section. You can find detailed tutorials on how to convert ONNX models into hls4ml here: https://github.com/eliarouss/hls4ml-tutorial.\n",
      "\n",
      "In summary, the steps for using QONNX models in hls4ml are:\n",
      "\n",
      "1. Convert an ONNX model into a supported format (yaml file or passed through API).\n",
      "2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To use QONNX models in hls4ml, follow these general steps:\\n\\n1. Conversion from ONNX: First, convert an ONNX model into a format supported by hls4ml using the QONNX frontend and tools like onnx2hll, openvino toolkit, or any other ONNX-compatible converter.\\n\\n   a) YAML file: When passing ONNX configuration through API, you can use yaml files to specify the model\\'s parameters and structure.\\n   b) ONNX model: You may also pass the ONNX model directly using the API.\\n\\n2. Initialize hls4ml models: The QONNX layer is already included in the package; however, it needs initialization before HLS production. To do this, you should have a class-based Quant layer that inherits from Layer.\\n\\nHere\\'s an example of how to initialize an hls4ml model with a QONNX quantization layer:\\n\\n    from hls4ml.model import layers\\n    from hls4ml.model import Program\\n    ...\\n    class CustomLayer(layers.Base, Quant):\\n        pass\\n\\n    config = {\"Input\": (2, 224, 224), \"Stage\": [{\"Block\": 2}], \"Stage\": 0, \"Output\": (2, 224, 224)}\\n    model = Program()\\n\\n    input_shape = (224, 224, 3)\\n    quant_layer = CustomLayer(model, \"InputQuant\", input_shape, config=config)\\n    output_layer = model.apply(quant_layer)\\n\\n    print(\"HLS Model:\", model.to_yaml())\\n\\n3. Optimize the HLS model: After initializing the hls4ml models, you should use optimization techniques provided by hls4ml to reduce the size and improve performance of the resulting binary.\\n\\nFor more information on using hls4ml with QONNX models, please refer to our tutorials section. You can find detailed tutorials on how to convert ONNX models into hls4ml here: https://github.com/eliarouss/hls4ml-tutorial.\\n\\nIn summary, the steps for using QONNX models in hls4ml are:\\n\\n1. Convert an ONNX model into a supported format (yaml file or passed through API).\\n2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Set the question\n",
    "# question = \"What techniques can be used to reduce model footprint in hls4ml?\"\n",
    "question = \"What are the steps to use QONNX models in hls4ml?\"\n",
    "\n",
    "# 2. Set the context\n",
    "context = format_docs(retriever.invoke(question))\n",
    "\n",
    "# 3. Set the instruction\n",
    "instruction = \"\"\"You are an expert on the software package hls4ml.\n",
    "Please answer the following question based on the given context.\"\"\"\n",
    "\n",
    "# 4. Set the input text template\n",
    "input_text_template = f\"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Set the message dictionary\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text_template,\n",
    "}\n",
    "\n",
    "# 6. Chain the prompt template and olmo model\n",
    "llm_chain = prompt_template | olmo\n",
    "\n",
    "# 7. Invoke the chain\n",
    "llm_chain.invoke(input={\"messages\": [message]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f3978",
   "metadata": {},
   "source": [
    "```{admonition} Answer Example Code\n",
    ":class: hint dropdown\n",
    "\n",
    "```{code-block} python\n",
    "# 1. Set the question\n",
    "question = \"What is the most familiar method for multiplying large numbers?\"\n",
    "\n",
    "# 2. Set the context\n",
    "context = format_docs(retriever.invoke(question))\n",
    "\n",
    "# 3. Set the instruction\n",
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\"\n",
    "\n",
    "# 4. Set the input text template\n",
    "input_text_template = f\"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Set the message dictionary\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text_template,\n",
    "}\n",
    "\n",
    "# 6. Chain the prompt template and olmo model\n",
    "llm_chain = prompt_template | olmo\n",
    "\n",
    "# 7. Invoke the chain\n",
    "llm_chain.invoke(input={\"messages\": [message]})\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7df7cd",
   "metadata": {},
   "source": [
    "**Bonus: Try to create a simple chat app, by modifying the [1-olmo-chat-rag.ipynb](./1-olmo-chat-rag.ipynb) notebook with your use case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a5e52",
   "metadata": {},
   "source": [
    "Please fill out the [survey feedback form](https://tinyurl.com/ssecfeedback) to help us improve the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
