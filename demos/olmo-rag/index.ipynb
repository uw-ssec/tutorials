{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# OLMO RAG DEMO\n",
    "\n",
    "Now it's your turn to apply your data and specific domain knowledge.\n",
    "\n",
    "You can use this notebook as a starting point and adapt it to your needs.\n",
    "You will need to develop the pre-processing stage for a RAG system.\n",
    "This includes document retrieval, cleaning, chunking,\n",
    "and ingestion into the vector database using an embedding model.\n",
    "\n",
    "To help you, we've provided a few example code snippets in Jupyter notebooks found in the \n",
    "[`appendix`](../appendix/index.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from testcontainers.qdrant import QdrantContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628a08b-b9c4-4759-9954-732b543dc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01106fa-2eea-4d65-ab3c-24a4cfdf2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d0a74-2e0c-4c77-8464-2169de64731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624653f",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "A section for whatever utility functions you need. We have packaged up our utility functions in a Python package called `ssec_tutorials`. You can find the source code in this [GitHub repository](https://github.com/uw-ssec/ssec_tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc327339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for whatever utility functions you need. This can be anything such as\n",
    "# cleaning up document format, setting up prompt templates, etc.\n",
    "\n",
    "\n",
    "# Uncomment the following for a simple document formatting function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd81f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4105a812",
   "metadata": {},
   "source": [
    "## Retrieve documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf9ba5",
   "metadata": {},
   "source": [
    "A section for document retrieval. This just means getting your document from whatever sources,\n",
    "in your local computer or the internet. See the [Document Loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/) integration list from Langchain for an extensive list of what's possible.\n",
    "\n",
    "For the purpose of this tutorial, we recommend a simple example of loading a piece of text from a file such as PDF. Also, if you have a large piece of text, you can split it into smaller chunks using Langchains's [RecursiveTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/).\n",
    "\n",
    "If you don't have any data with you, you can try out with this [Algorithm Textbook by Jeff Erickson](http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf). This textbook has been generously made available by Jeff Erickson under the [Creative Commons Attribution 4.0 International license](http://creativecommons.org/licenses/by/4.0/), you can find more information about the textbook at [https://jeffe.cs.illinois.edu/teaching/algorithms/](https://jeffe.cs.illinois.edu/teaching/algorithms/).\n",
    "\n",
    "```{note}\n",
    "If you're running things on Codespace, [refer to this link](https://stackoverflow.com/questions/62284623/how-can-i-upload-a-file-to-a-github-codespaces-environment) and upload your data to `resources/` folder. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6994afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for your retrieval step,\n",
    "# see the documentation on PyMuPDF for more information:\n",
    "# https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/#using-pymupdf\n",
    "\n",
    "# Uncomment below for code to download the textbook\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf\"\n",
    "filename = os.path.basename(url)\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    # Download if file doesn't exist\n",
    "    pdf_path, headers = urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37fedc-7003-47f0-9753-58a668922e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_folder_path = \".\"  # update path to point to the relevant directory\n",
    "documents = []\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder_path, file)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "for each in documents:\n",
    "    #     # print(each.page_content) # Uncomment this line to see the individual page_content\n",
    "    print(each.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434737ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to load the PDF document as a Langchain Document objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca40cd",
   "metadata": {},
   "source": [
    "## Document Embeddings to Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c53d1",
   "metadata": {},
   "source": [
    "Once you've figured out how to retrieve and load your documents to Langchain Document objects, you can then proceed to loading these documents to Qdrant Vector Database collection.\n",
    "\n",
    "See the following documentation for some guidance on [Langchain Qdrant integration](https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35899c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d199a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657c623",
   "metadata": {},
   "source": [
    "### Setup Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec3337-446a-4078-a78d-513ea301114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to load your data into the database\n",
    "\n",
    "# uncomment below to set the Qdrant path and collection name\n",
    "# for an \"local mode\" on-disk storage\n",
    "# See https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/#on-disk-storage\n",
    "# qdrant_path = \"./my_qdrant_database\"\n",
    "qdrant_collection = \"algorithms_book\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f040608-8c4d-4030-8c43-8b0fc6679a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.collection_exists(qdrant_collection):\n",
    "    print(\"Creating collection:\", qdrant_collection)\n",
    "    client.create_collection(\n",
    "        qdrant_collection,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=embedding.client.get_sentence_embedding_dimension(),\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    )\n",
    "    lcqdrant = Qdrant(\n",
    "        client=client, collection_name=qdrant_collection, embeddings=embedding\n",
    "    )\n",
    "    uuids = lcqdrant.add_documents(documents=documents)\n",
    "else:\n",
    "    lcqdrant = Qdrant(\n",
    "        client=client, collection_name=qdrant_collection, embeddings=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a6fd7",
   "metadata": {},
   "source": [
    "### Test out the Qdrant collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a6de6",
   "metadata": {},
   "source": [
    "At this step, you should have a Qdrant object (`langchain_qdrant.vectorstores.Qdrant`) that has your document loaded into it in a collection. You can test out the collection by querying for a documents and checking if the results are as expected.\n",
    "\n",
    "To do this, you'll need to create a [`VectorStoreRetriever`](https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82691eec",
   "metadata": {},
   "source": [
    "```{note}\n",
    "A sample question example to ask from the document can be `\"What is the most familiar method for multiplying large numbers?\"`.\n",
    "An answer to this question can be found on page 3, section 0.2 Multiplication, Lattice Multiplication.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934d230",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "You'll probably need to tweak the arguments for creating a `VectorStoreRetriever` object for the best search type and limiting the number of documents. This part is a bit of trial and error, so don't be afraid to experiment. It is a critical part of RAG system to get the right documents for the question as that is what the LLM would use to generate the answer.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to try out the vector database retrieval with a question query\n",
    "retriever = lcqdrant.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c8fdd-f11a-44ad-8974-bd2e0b67383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What is the most familiar method for multiplying large numbers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Setup OLMo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb84bc",
   "metadata": {},
   "source": [
    "At this stage now we have the Retrieval-Augmented (RA) in RAG system. Let's now setup the Generation (G) part with the OLMo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b202679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials import download_olmo_model\n",
    "\n",
    "# This will download the OLMO model to the cache directory\n",
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this line to understand your available options for LlamaCpp Class\n",
    "# LlamaCpp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Here we've setup the LlamaCpp model,\n",
    "# but you'll need to add additional arguments to `LlamaCpp`\n",
    "# to make it work for your specific use case\n",
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    verbose=False,\n",
    "    n_ctx=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3181c",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Try asking some questions to OLMo about any content of the document you've loaded in the Qdrant collection.\n",
    "You will find that the OLMo model is not trained on your specific domain, so it might not give you the best results.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dccb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = olmo.invoke(input=\"What is the most familiar method for multiplying large numbers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8e5a9",
   "metadata": {},
   "source": [
    "Rather than a just a simple question, we'll need to refine the prompt to include instruction and context for the model to generate the answer. To do this, we'll need to setup the proper string [PromptTemplate](https://python.langchain.com/v0.2/docs/concepts/#string-prompttemplates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create the initial prompt template using OLMo's tokenizer chat template we saw in module 1.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd751a28",
   "metadata": {},
   "source": [
    "Set the question for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the most familiar method for multiplying large numbers?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef522c06",
   "metadata": {},
   "source": [
    "Set the context for the prompt.\n",
    "This is where you'll need to use the `VectorStoreRetriever` and format the document object with `format_docs`\n",
    "or simply add your own text to the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757265b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment variable below to set the context\n",
    "# context = \"Enter code or string here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9569a7",
   "metadata": {},
   "source": [
    "Set the instruction for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06338896",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3c7b",
   "metadata": {},
   "source": [
    "The original OLMo chat template takes in multiple messages with a `role` and `content` key. You can use this template to ask questions to the model. For simplicity, we'll just use a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set the input text template\n",
    "# input_text_template = f\"\"\"\\\n",
    "# {instruction}\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c540731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set the message dictionary\n",
    "# message = {\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": input_text_template,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to try out the prompt template\n",
    "# print(prompt_template.format(\n",
    "#     messages=[message]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4fcab",
   "metadata": {},
   "source": [
    "You can see above what the final prompt looks like. There are tags like `<|user|>` that signify the model that this is a user input and so on. This final string is sent to the model for generating the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4584fdb",
   "metadata": {},
   "source": [
    "At this point you have all the parts for RAG system setup. Now let's chain the prompt engineering, OLMo model and the Qdrant collection to get a more accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee065902-46d2-4262-90bb-331fef284dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set the question\n",
    "question = \"What is the most familiar method for multiplying large numbers?\"\n",
    "\n",
    "# 2. Set the context\n",
    "context = format_docs(retriever.invoke(question))\n",
    "\n",
    "# 3. Set the instruction\n",
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\"\n",
    "\n",
    "# 4. Set the input text template\n",
    "input_text_template = f\"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Set the message dictionary\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text_template,\n",
    "}\n",
    "\n",
    "# 6. Chain the prompt template and olmo model\n",
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Invoke the chain\n",
    "llm_chain.invoke(input={\"messages\": [message]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f3978",
   "metadata": {},
   "source": [
    "```{admonition} Answer Example Code\n",
    ":class: hint dropdown\n",
    "\n",
    "```{code-block} python\n",
    "# 1. Set the question\n",
    "question = \"What is the most familiar method for multiplying large numbers?\"\n",
    "\n",
    "# 2. Set the context\n",
    "context = format_docs(retriever.invoke(question))\n",
    "\n",
    "# 3. Set the instruction\n",
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\"\n",
    "\n",
    "# 4. Set the input text template\n",
    "input_text_template = f\"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Set the message dictionary\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text_template,\n",
    "}\n",
    "\n",
    "# 6. Chain the prompt template and olmo model\n",
    "llm_chain = prompt_template | olmo\n",
    "\n",
    "# 7. Invoke the chain\n",
    "llm_chain.invoke(input={\"messages\": [message]})\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b5e93-1d15-4854-9b3a-e4e61798829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f1d3f-800b-47d9-b71c-34642778b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManager, BaseCallbackHandler\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74602264-db0a-4fb4-9df0-fd97225b2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(callback_handlers: list[BaseCallbackHandler], input_prompt_template: str):\n",
    "    # 1. Set up the vector database retriever.\n",
    "    # This line of code will create a retriever object that\n",
    "    # will be used to retrieve documents from the vector database.\n",
    "    retriever = lcqdrant.as_retriever(\n",
    "        callbacks=callback_handlers,  # pass the result of the retrieval to the callback handler\n",
    "        search_type=\"mmr\",  # the mmr (maximal marginal relevance, a typical information retrieval tactic) search\n",
    "        search_kwargs={\"k\": 2},  # return top 2 results\n",
    "    )\n",
    "\n",
    "    # 2. Setup the Langchain callback manager to handle callbacks from Langchain LLM object.\n",
    "    # At which results are passed to the callback handler.\n",
    "    callback_manager = CallbackManager(callback_handlers)\n",
    "\n",
    "    # 3. Setup the Langchain llama.cpp model object.\n",
    "    # In our case, we are using the `OLMo-7B-Instruct` model.\n",
    "    # llama-cpp-python is a Python binding for llama.cpp C++ library as mentioned in previous modules.\n",
    "    olmo = LlamaCpp(\n",
    "        model_path=str(OLMO_MODEL),  # the path to the OLMo model in GGUF file format\n",
    "        callback_manager=callback_manager,  # set the callback manager to handle callbacks\n",
    "        temperature=0.8,  # set the randomness of the model's output\n",
    "        n_ctx=4096,  # set limit for the length of the input context\n",
    "        max_tokens=512,  # set limit for the length of the generated text\n",
    "        verbose=False,  # determines whether the model should print out debug information\n",
    "        echo=False,  # determines whether the input prompt should be included in the output\n",
    "    )\n",
    "\n",
    "    # 4. Set up the initial Langchain Prompt Template using text based jinja2 format\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        template=olmo.client.metadata[\n",
    "            \"tokenizer.chat_template\"\n",
    "        ],  # get the chat template from the model metadata\n",
    "        template_format=\"jinja2\",  # set the template format to jinja2\n",
    "        partial_variables={\n",
    "            \"add_generation_prompt\": True,  # add generation prompt to the template, this option is from the model metadata\n",
    "            \"eos_token\": \"<|endoftext|>\",  # set the end of sentence token\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 5. Transform the Prompt Template to include the user role and the context\n",
    "    # This will allow the model to generate text based on the context provided.\n",
    "    # However, after setting this new template, the model will be limited to\n",
    "    # generating text based on the created prompt template with input of\n",
    "    # `context` and `question` keys.\n",
    "    transformed_prompt_template = PromptTemplate.from_template(\n",
    "        prompt_template.partial(\n",
    "            # The default chat template takes a list of messages with a role and content\n",
    "            # to setup this particular app, we will only pass a single message with the user role\n",
    "            # and the input prompt content\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",  # set the role to user, this allows for user input to be passed to the model\n",
    "                    \"content\": input_prompt_template,  # the input prompt template, must have `context` and `question` keys to work\n",
    "                }\n",
    "            ]\n",
    "        ).format()\n",
    "    )\n",
    "\n",
    "    # 6. Define the `format_docs` function to format the retrieved Langchain documents object to simple string\n",
    "    def format_docs(docs):\n",
    "        text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        return text\n",
    "\n",
    "    # 7. Define the `show_docs` function to display the retrieved documents to app panel\n",
    "    # this is currently a small hack to display the retrieved documents to the app panel\n",
    "    # as mentioned in https://github.com/langchain-ai/langchain/issues/7290\n",
    "    def show_docs(docs):\n",
    "        for callback_handler in callback_handlers:\n",
    "            callback_handler.on_retriever_end(\n",
    "                docs,  # pass the retrieved documents to the callback handler\n",
    "                run_id=uuid4(),  # generate a random run id\n",
    "            )\n",
    "        return docs\n",
    "\n",
    "    # 8. Return the Langchain chain object\n",
    "    # The way the chain reads is as follows:\n",
    "    return (\n",
    "        {\n",
    "            # The Vector Database retriever documents,\n",
    "            # which is then passed to the `show_docs` function,\n",
    "            # which is then passed to the `format_docs` function for formatting\n",
    "            \"context\": retriever | show_docs | format_docs,\n",
    "            # The Question asked by the user from the Chat Text Input Interface is passed in as well\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        # The dictionary above that contains text values for `context` and `question` is now passed\n",
    "        # to the transformed prompt template so that the final prompt text can be generated\n",
    "        | transformed_prompt_template\n",
    "        # The full final prompt text with both context and question is passed to the OLMo model\n",
    "        # for generation of the final output. Note that this final prompt text cannot exceed the maximum\n",
    "        # `n_ctx` input context value set in the OLMo model above.\n",
    "        | olmo\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d880ba-f4e1-4a47-a3ab-70468a0ece91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt_template = textwrap.dedent(\n",
    "    \"\"\"\\\n",
    "You are an astrophysics expert. Please answer the question on astrophysics based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b9673-d7a5-45df-a5f0-71eb98528c2d",
   "metadata": {},
   "source": [
    "Now we will use that chain workflow to create a simple chat application using Panel. In the diagram above, this would be our \"Enterprise App\", but obviously much simpler and not ready for production at this stage.\n",
    "\n",
    "To begin, we will setup the asynchronous `callback` function for the [`pn.chat.ChatInterface`](https://panel.holoviz.org/reference/chat/ChatInterface.html) layout component. This will allow us to interact with the chat interface and ask questions.\n",
    "\n",
    "The `ChatInterface` is a high-level layout, it provides front-end interface for inputting different kinds of messages: text, images, PDFs, etc.\n",
    "\n",
    "This layout provides front-end methods to:\n",
    "\n",
    "- Input (append) messages to the chat log.\n",
    "- Re-run (resend) the most recent user input ChatMessage.\n",
    "- Remove messages until the previous user input ChatMessage.\n",
    "- Clear the chat log, erasing all ChatMessage objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e397a21-09c5-455c-852a-6a8effc051c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(contents, user, instance):\n",
    "    # 1. Create a panel callback handler\n",
    "    # The Langchain PanelCallbackHandler is useful for rendering and streaming the chain of thought\n",
    "    # from Langchain objects like Tools, Agents, and Chains.\n",
    "    # It inherits from Langchainâ€™s BaseCallbackHandler.\n",
    "    # Here we set the user to be the model name \"OLMo\" with an avatar of a tree emoji \"ðŸŒ³\"\n",
    "    # for the tree of knowledge.\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(\n",
    "        instance, user=\"OLMo\", avatar=\"ðŸŒ³\"\n",
    "    )\n",
    "\n",
    "    # 2. Set to not return the full generated result at the end of the generation;\n",
    "    # this prevents the model from repeating the result in the interface\n",
    "    callback_handler.on_llm_end = lambda response, *args, **kwargs: None\n",
    "\n",
    "    # 3. Create and setup the Langchain chain object with the callback handler and input prompt template\n",
    "    chain = get_chain(\n",
    "        callback_handlers=[callback_handler],\n",
    "        input_prompt_template=input_prompt_template,\n",
    "    )\n",
    "\n",
    "    # 4. Run the chain with the input contents\n",
    "    _ = await chain.ainvoke(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad5db3-7cab-4c2c-b121-a3abf37a2da5",
   "metadata": {},
   "source": [
    "Once we have a callback function, now we're ready to pass that to the ChatInterface layout component.\n",
    "\n",
    "The code below takes in the asynchronous callback function from above and serves the chat interface to the user. This callback function will run every time the user sends a message in the chat interface. The callback function callback will receive the input text as part of the contents. The contents will be passed to the Langchain chain where:\n",
    "\n",
    "the retriever will fetch document based on the input text\n",
    "generate prompt with instruction, document results, and question input text\n",
    "generate answer based on the prompt\n",
    "return the generated answer and retrieved document text to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fee081-db1f-4ccf-ab91-fc979d3c7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.chat.ChatInterface(callback=callback).servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc4f79-52f9-465a-8ffb-13ccb625dda5",
   "metadata": {},
   "source": [
    "If you're running this notebook on JupyterLab, there should be a Panel logo in the menu bar of your notebook. You can clear output and restart the kernel, then enable the Preview for this app by clicking on Panelâ€™s logo in the menu bar of your notebook. Once clicked, you should see a new tab being opened next to your notebook tab, and after some moment your app will be rendered in this tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f6b25f-aae4-4507-9d6b-f84bd2d8e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7df7cd",
   "metadata": {},
   "source": [
    "**Bonus: Try to create a simple chat app, by modifying the [1-olmo-chat-rag.ipynb](./1-olmo-chat-rag.ipynb) notebook with your use case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a5e52",
   "metadata": {},
   "source": [
    "Please fill out the [survey feedback form](https://tinyurl.com/ssecfeedback) to help us improve the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssec-scipy2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
