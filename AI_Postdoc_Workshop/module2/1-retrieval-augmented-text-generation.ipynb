{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c560a25",
   "metadata": {},
   "source": [
    "The moment that we've all been waiting for has finally arrived! The Retrieval-Augmented Text Generation (RAG) Framework is here! ðŸŽ‰\n",
    "\n",
    "Throughout this notebook we will be exploring RAG, what it is, how it works, and why it's so exciting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1041c",
   "metadata": {},
   "source": [
    "## Why RAG?\n",
    "\n",
    "Although trained on large datasets, stale data can severely limit LLMs. It faces several challenges:\n",
    "\n",
    "1. The models are trained on internet content, so they might not generate relevant output when prompted for information that is not publicly available on the internet.\n",
    "\n",
    "2. The models are trained up to a certain date, they might not generate relevant output when prompted for content and information that has happened after the training completion date of the model.\n",
    "\n",
    "3. The models are trained to be more generalized. This means that they can only produce generic outputs and might not perform as expected when prompted for specific deep-dive concepts related to a particular topic.\n",
    "\n",
    "One way to dynamically integrate relevant external information is retrieval-augmented generation (RAG), which can help improve the reliability of LLM outputs.\n",
    "\n",
    "Going back to our original question of how this can be utilized in our own work or organization on [section 1](1-domain-specific-question-answering.ipynb) of this module. RAG Framework can really be useful in the scenario where there may be a set of documents, GitHub repositories, research papers, and domain-specific knowledge bases that you might want to search through quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## RAG Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "RAG proposes a solution to this issue by supplementing the prompt sent to the LLM with information from external sources through a retrieval model via vector embeddings (more on this later), thereby providing the LLM with more relevant input to generation responses. It allows you to use pre-trained LLMs without fine-tuning them or training your own LLM on your training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "![RAG Workflow](../../images/rag-workflow.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "\n",
    "Image Source: [Medium Blog](https://medium.com/@henryhengluo/intro-of-retrieval-augmented-generation-rag-and-application-demos-c1d9239ababf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Multiple concepts influence RAG pipeline:\n",
    "\n",
    "1. Retrieval\n",
    "2. Augmentation\n",
    "3. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "The retrieval phase can also be considered the data and query/prompt preparation phase, focusing on efficient information retrieval or data access. To improve your RAG pipeline, the pre-retrieval phase contains tasks such as: `(1): Indexing, (2) Query Manipulation, (3) Data Modification, (4) Search, and (5) Ranking.` In this tutorial, we primarily focus on indexing and search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "`Indexing` enables fast and accurate information retrieval that sets up the context for any LLM to improve its response to a given user prompt or query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We will be indexing abstracts for all astrophysics papers and Astropy's documentation, a common core package for Astronomy in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Embeddings, also called \"Vector Embedding,\" help LLMs develop a semantic understanding of the textual data they are trained on. In simpler terms, these embedding models lay the groundwork for LLMs to perform tasks like sentence completion, similarity search, questions and answers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50d479",
   "metadata": {},
   "source": [
    "#### Embedding vs Fine-tuning\n",
    "\n",
    "|| Embedding | Fine-tuning|\n",
    "|---|---|---|\n",
    "|**Definition**| Use pre-trained LLM as feature extractorâ€‹ | Update parameters of pre-trained LLM during task-specific training|\n",
    "|**Process**| Input Encoding > tokenizedâ€‹ > Embedding Extraction â€‹ > Downstream Task | Initialization â€‹> Task-specific Trainingâ€‹ > Fine-tuning Layers (optional)|\n",
    "|**Advantages**| Efficient use of pre-trained knowledge, Faster inference | Adaptability to task-specific nuances, May require less labeled data than from scratchâ€‹|\n",
    "|**Considerations**| N/A |Risk of overfitting, Computational cost can be high|\n",
    "|**When to use**| Limited computational resourcesâ€‹, Limited labeled data | Significant computational resources, Large corpus of labeled data|\n",
    "|**Performance**| Performs well, especially with limited dataâ€‹ | Can achieve state-of-the-art results on a wide range of tasks|\n",
    "\n",
    "### In a nutshell\n",
    "\n",
    "- Embeddings models are typically small in size and less computationally intensiveâ€‹\n",
    "\n",
    "- Regular updates of embedding vectors are faster, cheaper, and simpler compared to fine-tuning a model.â€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "At the lowest level, machines only understand numeric values. For LLMs to work, natural language is converted into an array of numeric values before they are fed into the models. These arrays of numeric values are called \"Vector.\"\n",
    "\n",
    "An example of a vector: [2.5, 1.0, 3.3, 7.8]\n",
    "\n",
    "The above is an example of a vector of size 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector = np.array([2.5, 1.7, 3.3, 7.8])\n",
    "print(f\"Vector: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We stated above that **\"texts are converted into an array of numeric values called vectors\"**.\n",
    "\n",
    "But depending on your use case, each word, sentence, paragraph, or entire document can be represented as a vector. \n",
    "\n",
    "Tokens are the smallest natural language units converted into a vector. It could be at the character level, sub-word level, word level, sentence level, paragraph level, or document level.\n",
    "\n",
    "Example: Consider the text below.\n",
    "\n",
    "`Earth is a planet of the solar system. There are 9 planets in the solar system. \n",
    "All planets revolve around the sun. Sun is a star.`\n",
    "\n",
    "\n",
    "Case 1.) **Tokenizing the entire paragraph into vector.**  \n",
    "Tokenization: The entire paragraph is a single token.   \n",
    "Vectorization: A single vector.  \n",
    "Sample Vector Representation: [3.1, 6.8, 5.4, 8.0, 7.1]\n",
    "\n",
    "Case 2.) **Tokenizing each sentence into vectors.**  \n",
    "Tokenization: One token for each sentence (total 4 tokens)  \n",
    "Vectorization: One vector for each sentence (total 4 vectors).   \n",
    "Sample Vector Representation: [[1.2, 2.3, 3.8, 7.9, 0.8], [2.5, 3.0, 8.2, 6.6, 4.1], [3.2, 6.5, 8.1, 9.3, 1.4], [1.1, 0.7, 7.2, 3.5, 8.5]]\n",
    "\n",
    "Case 3.) **Tokenizing each word in the paragraph into a vector. There are 26 words in the paragraph, ignoring punctuation. Each word gets converted into a vector.**  \n",
    "Tokenization: One token for each word in the paragraph (26 tokens)  \n",
    "Vectorization: One vector for each token (total 26 vectors).    \n",
    "Sample Vector Representation: [[2.1, 3.2, 4.1, 9.8, 7.0], [8.2, 4.2, 7.1, 3.8, 2.0].....total 26 such representations]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Tokenizers are components responsible for converting large texts into tokens (tokenization). Different types of pre-trained tokenizers are available. You can even train your own tokenizers. But for the scope of this tutorial, we will use a pre-trained one. \n",
    "\n",
    "Generally, each tokenizer follows the following steps:\n",
    "\n",
    "1. Break down the original text into tokens. These tokens could again be at the character, sub-word, word, sentence, paragraph, or document levels.\n",
    "2. Assign a unique identifier to each of the tokens created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, here is how you can split a short sentence into chunks of text\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "text_splitter.split_text(text=\"Earth is a planet in the solar system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "[Learn more about how to split text into tokens in LangChain here.](https://python.langchain.com/v0.2/docs/how_to/split_by_token/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Embedding Models\n",
    "\n",
    "A language model needs to understand how tokens are related to each other in the context of human language. To understand this semantic relationship, these tokens are converted into numerical vectors.\n",
    "\n",
    "Embedding Models are trained upon these tokens to develop an \"embedding space.\"\n",
    "\n",
    "- Before the training, the embedding model initializes an N-dimensional 'vector' corresponding to each 'token' with random values. (Value of N depends on the embedding model)\n",
    "  \n",
    "- During the embedding model training, the values for these vectors are updated across iterations. In this process, similar or related tokens are updated to have similarly valued vectors.\n",
    "  \n",
    "- After the training, the collection of all the 'vectors' corresponding to all the tokens is called the \"embedding space.\"\n",
    "\n",
    "- \"Embedding Space\" is an encoded representation of meanings of tokens and inter-token relationships.\n",
    "\n",
    "See [Word Embeddings Resource](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/11-text-as-vectors-embeddings/) for more conceptual details on embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "To understand this further, let's take a look at how it all works using a pre-trained embedding model.\n",
    "\n",
    "For the tutorial and simplicity, we are using the Langchain Hugging Face integrations, which is available in the [`langchain-huggingface`](https://pypi.org/project/langchain-huggingface/) package.\n",
    "To use an embedding model available in Hugging Face,\n",
    "we will simply use the `HuggingFaceEmbedding` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbbb3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451a4ff",
   "metadata": {},
   "source": [
    "We are using the [all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) sentence-transformers embedding model for this tutorial.\n",
    "After [some evaluation](https://github.com/uw-ssec/tutorials/issues/6) that we did, we found that this model works well for our use case as it is lightweight and provides good performance.\n",
    "\n",
    "This model \"maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search\".\n",
    "\n",
    "However, you can use any other embedding model available in Hugging Face, and we recommend going to [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) to find embedding models and see how they compare to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L12-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings_model.embed_query(\"Earth is a planet in the solar system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of vector\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "In an embedding space, you can find how similar two vectors are using `dot product` or  using `cosine similarity.`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5b4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Similarity:\",\n",
    "    1\n",
    "    - spatial.distance.cosine(\n",
    "        query_result,\n",
    "        embeddings_model.embed_query(\"Mars is a planet in the solar system.\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Similarity:\",\n",
    "    1\n",
    "    - spatial.distance.cosine(\n",
    "        query_result, embeddings_model.embed_query(\"Hello Tacoma.\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1513a64",
   "metadata": {},
   "source": [
    "What we have demonstrated above in finding similarity between vectors is essentially what's happening in the retrieval phase of the RAG pipeline within a *Vector Database*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb6cf2",
   "metadata": {},
   "source": [
    "#### Algorithms Textbook\n",
    "\n",
    "We will now bring in the textbook in question, as we alluded to above. First importing it and then converting into a `langchain` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbc2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for your retrieval step,\n",
    "# see the documentation on PyMuPDF for more information:\n",
    "# https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/#using-pymupdf\n",
    "\n",
    "# Uncomment below for code to download the textbook\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf\"\n",
    "filename = os.path.basename(url)\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    # Download if file doesn't exist\n",
    "    pdf_path, headers = urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_folder_path = \".\"  # update path to point to the relevant directory\n",
    "documents = []\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder_path, file)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "for each in documents:\n",
    "    #     # print(each.page_content) # Uncomment this line to see the individual page_content\n",
    "    print(each.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Vector Stores\n",
    "\n",
    "Once the embeddings are created for our relevant documents or knowledge base, we need to store these embeddings in the database for fast retrieval. \n",
    "\n",
    "The type of databases that store these vector embeddings are called \"Vector Stores.\" We will use a vector store called \"Qdrant,\" as shown below. \n",
    "\n",
    "In the below code, \n",
    "- Vector store works along with the embedding model to create vector embeddings.\n",
    "- Vector embeddings are stored in the Qdrant Vector database collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "We will now create QDrant Collection/Database using the above Algorithms Textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f49ed",
   "metadata": {},
   "source": [
    "We start by defining the Qdrant path and collection name information. We can now use the Langchain Qdrant integrations package called `langchain-qdrant` to interact with the Qdrant database by using the `Qdrant` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8270fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from ssec_tutorials import TUTORIAL_CACHE\n",
    "import shutil\n",
    "\n",
    "qdrant_collection = \"algorithms_book\"\n",
    "qdrant_path = TUTORIAL_CACHE / \"algorithms_book\"\n",
    "\n",
    "client = QdrantClient(path=str(qdrant_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if qdrant_path.exists():\n",
    "    print(\"Removing cached data\")\n",
    "    shutil.rmtree(qdrant_path)\n",
    "\n",
    "print(\n",
    "    f\"Creating new Qdrant collection '{qdrant_collection}' from {len(documents)} documents\"\n",
    ")\n",
    "\n",
    "# Load the documents into a Qdrant Vector Database Collection\n",
    "# this will save locally in the qdrant_path as sqlite\n",
    "qdrant = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    path=str(qdrant_path),\n",
    "    collection_name=qdrant_collection,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73d977",
   "metadata": {},
   "source": [
    "Now that we have the Qdrant database instance, we are ready to search for the relevant documents based on the user query.\n",
    "However, before we can simply search, we will need a [`VectorStoreRetriever`](https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/) object.\n",
    "\n",
    "To get the `VectorStoreRetriever` object, we can simply call the `.as_retriever()` method on the Qdrant object.\n",
    "\n",
    "In this example, we will be setting the `search_type` to `\"mmr\"` and `search_kwargs` to `{\"k\": 2}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7aa006",
   "metadata": {},
   "source": [
    "\"mmr\" stands for  Maximum Marginal Relevance\n",
    "\n",
    "MMR selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\n",
    "\n",
    "The `k` parameter in `search_kwargs` specifies the number of documents to retrieve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the retriever for later step\n",
    "retriever = qdrant.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf703bba",
   "metadata": {},
   "source": [
    "Let's invoke this retriever object with some of the questions from previous section and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = retriever.invoke(\"What is the best method for multiplying large numbers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9512a",
   "metadata": {},
   "source": [
    "We got the relevant documents from the Qdrant database for the given questions. Let's see what these documents look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04949b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac457806",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66587e",
   "metadata": {},
   "source": [
    "We see that this is a core Langchain Document object that contains the document's metadata and content.\n",
    "\n",
    "Later we will see how we can use this document to generate the response, for now let's create a utility formatting function to retrieve just the content of the document so that we can put this as part of our prompt template input, also known as \"Augmentation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_docs(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Augmentation & Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Now that we can retrieve the most relevant document based on a question, we can use the retrieved document and send it along with the prompt to increase the context for the LLM.\n",
    "\n",
    "This can also be referred to as the `retrieval-augmented prompt.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2198c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from ssec_tutorials import download_olmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    temperature=0.8,\n",
    "    verbose=False,\n",
    "    n_ctx=2048,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using OLMo's tokenizer chat template we saw in module 1.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prompt you want to send to OLMo.\n",
    "question = \"What is the best method for multiplying large numbers?\"\n",
    "context = format_docs(retriever.invoke(question))\n",
    "\n",
    "final_prompt_content = prompt_template.format(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\\n",
    "                You are an algorithms expert. Please answer the question on algorithms based on the following context:\n",
    "\n",
    "                Context: {context}\n",
    "\n",
    "                Question: {question}\n",
    "            \"\"\",\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_prompt_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073742e1",
   "metadata": {},
   "source": [
    "You can see above that we now have a `context` input within the prompt.\n",
    "This context is the content of the document(s) that we retrieved from the Qdrant database.\n",
    "With this context, the LLM can generate more relevant responses.\n",
    "So let's see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c23cfea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26449257",
   "metadata": {},
   "source": [
    "### OLMo with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo.invoke(\n",
    "    final_prompt_content, config={\"callbacks\": [StreamingStdOutCallbackHandler()]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb26355",
   "metadata": {},
   "source": [
    "### OLMo without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422daa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo.invoke(question, config={\"callbacks\": [StreamingStdOutCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a35bf4",
   "metadata": {},
   "source": [
    "From the responses above, we can see that the response with context is more relevant and informative compared to the response without context, an this shows the power of the RAG framework, with just a few documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "One way to generate the response with OLMo is to build `context` using the `question` beforehand, as shown above, create an llm_chain then `invoke` it with `messages`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "However, We can further use [LangChain's convenience functions](https://python.langchain.com/v0.2/docs/tutorials/rag/#built-in-chains) to streamline our pipeline using [create_stuff_documents_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) and [create_retrieval_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html) from the main `langchain` package.\n",
    "\n",
    "The main `langchain` package contains chains, agents, and retrieval strategies that make up an application's cognitive architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "`create_stuff_documents_chain` specifies how retrieved context is fed into a prompt and LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "On looking its signature, notice that it accepts `prompt` argument of type `BasePromptTemplate` but it needs input keys as `context` and `input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "205799a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e663f",
   "metadata": {},
   "source": [
    "To use the helper functions,\n",
    "we'll need to setup our template string to use the `context` and `input` keys as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new prompt_template\n",
    "# so that it accepts `context` and `input` as input_variables\n",
    "input_string_template = \"\"\"\\\n",
    "You are an astrophysics expert. Please answer the question on astrophysics based on the following context.\n",
    "Context: {context}\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "transformed_prompt_template = PromptTemplate.from_template(\n",
    "    prompt_template.partial(\n",
    "        messages=[{\"role\": \"user\", \"content\": input_string_template}]\n",
    "    ).format()\n",
    ")\n",
    "transformed_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=olmo, prompt=transformed_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "We can run this by passing in the context directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which data structures have the most efficient lookup time?\"\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"input\": question,\n",
    "        \"context\": retriever.invoke(question),\n",
    "    },\n",
    "    config={\"callbacks\": [StreamingStdOutCallbackHandler()]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "However, we want the context to be dynamically generated using the passed input or question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "From LangChain's documentation: `create_retrieval_chain` adds the retrieval step and propagates the retrieved context through the chain, providing it alongside the final answer. It has input key `input`, and includes input, context, and answer in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke(\n",
    "    {\"input\": \"Which data structures have the most efficient lookup time\"},\n",
    "    config={\"callbacks\": [StreamingStdOutCallbackHandler()]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b91fe4",
   "metadata": {},
   "source": [
    "One of the nice things about the LangChain helper function is that the result is a dictionary containing the `input`, `context`, and `answer` keys, so you can easily see what you asked and the context that was used to generate the answer.\n",
    "\n",
    "This way of creating the RAG pipeline is quick, but not as customizable. If you need more control over the input variables, we'll need to create our own chain.\n",
    "\n",
    "In the next module, we'll explore how to do this to create a simple Panel application that uses the RAG pipeline to generate responses to user questions.\n",
    "\n",
    "For now let's clean up the qdrant client by closing it before the next module, otherwise we'll run into errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a9d42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant.client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssec-scipy2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
