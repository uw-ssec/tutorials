{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Prompting with OLMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At University of Washington, SSEC, we are very fortunate to have an in-house group of software engineers who have came up with a very interesting set of questions for us to try out with OLMo:\n",
    "\n",
    "* What is best method for multiplying large numbers?\n",
    "* What are the steps for solving dynamic programming problems?\n",
    "* What data structures can be used to represent graphs?\n",
    "* What's a minimum spanning tree? How can one find it for a given graph?\n",
    "* How can you prove that a problem is NP-hard?\n",
    "* What is the runtime of MergeSort?\n",
    "* Which search algorithms are better choices for sorted data vs for unsorted data, and why?\n",
    "* Explain why binary search's time complexity is O(logn) on a sorted set.\n",
    "* Does recursion make space complexity a more or less important consideration, and why?\n",
    "* Which data structures have the most efficient lookup time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze how OLMo does without any additional context with these questions or your own specific domain questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the OLMo Model and Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin with a recap of the previous module, setting up the OLMo model and prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from ssec_tutorials import download_olmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time during the model setup, we'll try to increase the `n_ctx`, input context length, to `2048` tokens and the `max_tokens`, maximum tokens generated by the model, to `512` tokens.\n",
    "This is so later we can really expand on the questions that we ask the model and get a more expansive answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.8,\n",
    "    verbose=False,\n",
    "    n_ctx=2048,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model ready, let's setup the prompt template like before,\n",
    "using the internal chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using OLMo's tokenizer chat template we saw in module 1.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again use the partial variables\n",
    "here to fill out the `add_generation_prompt` and `eos_token` fields.\n",
    "So that we're left with just the `messages` input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the prompt template ready, let's move on to the next step, and create a prompt for the model.\n",
    "For simplicity of this tutorial, we'll only use one message, `user` input to the model.\n",
    "This means we'll only ask the model a single question at a time,\n",
    "rather than a series of questions that can feed of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap  # a module to wrap text to make it more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like before, we'll start by checking out what our full prompt text is going to look like.\n",
    "In this example, we've also used a handy built-in python module called textwrap to wrap the text to a certain width. We are using this to dedent the extra spaces to make it look cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prompt you want to send to OLMo.\n",
    "question = \"What is the best method for multiplying large numbers?\"\n",
    "input_content = textwrap.dedent(\n",
    "    f\"\"\"\\\n",
    "    You are an algorithms expert. Please answer the following question on algorithms.\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "input_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_content,\n",
    "    }\n",
    "]\n",
    "\n",
    "full_prompt_text = prompt_template.format(messages=input_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompt looks good. Let's now make a chain and invoke it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the prompt template and olmo\n",
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the chain with a question and other parameters.\n",
    "captured_answer = llm_chain.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! At this point we have reviewed essentially the first 3 notebooks of this module.\n",
    "But to ask different questions, we'll need a way to pass in different questions to the chain.\n",
    "We know that we can just create new values for `question`, `input_content`, and `input_messages` variables,\n",
    "but that's a lot of work and formatting to do every time we want to ask a new question.\n",
    "So what can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now introduce a new concept called [partial formatting](https://python.langchain.com/v0.2/docs/how_to/prompts_partial/).\n",
    "By using this feature, we can expand the input variables to be ones that we can easily change and pass in new values to.\n",
    "Essentially, we are creating a new prompt template from the underlying model template.\n",
    "\n",
    "We've seen this feature in module 1 and above with the use of `partial_variables` in the model setup.\n",
    "This time, since we know that we're only using one message,\n",
    "we can simplify the prompt template to take variables `question` and `instruction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a simple prompt template string that takes in the variables we want to pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt_template = textwrap.dedent(\n",
    "    \"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above prompt template string is NOT an f-string, but rather the simple string, like the ones you've created in module 1.\n",
    "\n",
    "Now that we have the prompt template string ready, let's create a partial formatting from it.\n",
    "Remember that `prompt_template` is a String PromptTemplate object that contains the original jinja-2 template string with the variables `add_generation_prompt` and `eos_token` filled in. The only variable left is `messages`, which we will create a partial formatting with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prompt_template = prompt_template.partial(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_prompt_template,\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the partial formatting is simply filling in the variables `messages` and now we're left with no `input_variables`. So at this point, how can we create a new prompt template from this?\n",
    "\n",
    "The answer is pretty straightforward. Let's just call the `.format` and get the \"final\" prompt template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_string = partial_prompt_template.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a simple prompt string that we can create a String PromptTemplate from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template = PromptTemplate.from_template(new_prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see now that the new prompt template takes in `instruction` and `question`. Let's create a new chain and invoke it with this new prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A Session with OLMo\n",
    "\n",
    "We'll first create a single domain instruction, since we know that we're asking questions about algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_instruction = (\n",
    "    \"You are an algorithms expert. Please answer the following question on algorithms.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the runtime of mergeSort?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = new_prompt_template.partial(instruction=domain_instruction) | olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn ðŸ˜Ž\n",
    "\n",
    "You have two options:\n",
    "\n",
    "1. Use the questions provided at the beginning of this notebook and reuse the llm chain to ask questions about algorithms.\n",
    "2. With the new prompt template `new_prompt_template` and the `olmo` model. Create a new chain with a different domain instruction, and ask questions about that domain.\n",
    "\n",
    "Feel free to ask any questions you like, and see how OLMo responds to them! If you're open to sharing, we'd love to hear about the questions you asked and the responses you received in the etherpad at https://etherpad.wikimedia.org/p/ipvVZZVxeP2JhpPxE4j6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssec-scipy2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
