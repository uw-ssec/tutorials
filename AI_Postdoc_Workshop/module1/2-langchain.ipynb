{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LangChain: The LLM Application Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "LangChain, an open-source library, empowers developers by providing a standardized and structured interface for building and integrating various components of an LLM Application. Its model-agnostic nature allows for compatibility with models from multiple LLM providers, including OpenAI, HuggingFace, and others. \n",
    "\n",
    "Using Langchain allows us to build (\"like a chain\") reusable components as part of complex multi-step LLM-based applications clearly and succinctly. \n",
    "\n",
    "You can learn about different [LangChain components here.](https://python.langchain.com/v0.2/docs/concepts/#components)\n",
    "\n",
    "This tutorial will focus on a few LangChain components and learn about `chaining`, one of its powerful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "[Prompt Templates](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates) provides templates for designing prompts fed as inputs to the LLM models.\n",
    "It helps us design templates with multiple inputs that are parameterized and reusable.\n",
    "\n",
    "```{note}\n",
    "For this tutorial, we will only cover the use of `String PromptTemplates` as this gives us a finer control over the template string structure, unlike the `ChatPromptTemplate`.\n",
    "```\n",
    "\n",
    "Below is an example of how to use a prompt template. We can import this class from the `langchain-core` package.\n",
    "\n",
    "The `langchain-core` package contains base abstractions of different components and ways to compose them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcf656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### String PromptTemplates\n",
    "\n",
    "The [String PromptTemplates](https://python.langchain.com/v0.2/docs/concepts/#string-prompttemplates) is used to format a string input. By default, the template takes Python `f-string` format. There are currently 2 choices of `template_format` available: `f-string` and `jinja2`. Later we will see the use of `jinja2` format. In the example below, we will use the `f-string` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{planet_name} in the solar system is the \"\n",
    ")\n",
    "\n",
    "prompt_template.format(planet_name=\"Mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ac98b",
   "metadata": {},
   "source": [
    "Let's instantiate our OLMo model like in the [previous section](./2-llms-and-prompt-engineering-with-olmo.ipynb#introduction) of the tutorial with `llama-cpp-python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cf3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from ssec_tutorials import download_olmo_model\n",
    "from ssec_tutorials.scipy_conf import parse_text_generation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLMO_MODEL = (\n",
    "    download_olmo_model()\n",
    ")  # It won't actually download again if it's already there\n",
    "olmo = Llama(model_path=str(OLMO_MODEL), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf3d2e",
   "metadata": {},
   "source": [
    "Now that we have our model ready to go, let's try different prompt templating starting from the previous prompt template with an input of `planet_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=prompt_template.format(planet_name=\"Mars\"),\n",
    "    temperature=0.2,\n",
    "    max_tokens=8,\n",
    "    echo=True,\n",
    ")  # Generate a completion, can also call olmo.create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{entity_1} of the planet {entity_2} is \"\n",
    ")\n",
    "prompt_template.format(entity_1=\"Size\", entity_2=\"Earth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=prompt_template.format(entity_1=\"Size\", entity_2=\"Earth\"),\n",
    "    temperature=0.2,\n",
    "    echo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10abf9",
   "metadata": {},
   "source": [
    "#### Your turn ðŸ˜Ž\n",
    "\n",
    "Create a `StringPromptTemplate` that outputs some text generation prompt, for example, \"Sun is part of galaxy ...\".\n",
    "\n",
    "Feel free to experiment with the built in [Python `f-string` ](https://docs.python.org/3.11/tutorial/inputoutput.html#formatted-string-literals) for the `prompt` input argument to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cdc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your prompt_template and model_response code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## LLM Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67e77a",
   "metadata": {},
   "source": [
    "LangChain have implemented a [`Runnable`](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) protocol that allows us to create custom \"chains\".\n",
    "This protocol has a standard interface for defining and invoking various LLMs, PromptTemplates, and other components, enabling reusability.\n",
    "For more details, go to LangChain's [Runnable documentation](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface).\n",
    "\n",
    "```{note}\n",
    "In this tutorial, you will see the use of `.invoke` method on various LangChain's object.\n",
    "This is essentially using that standard interface for the `Runnable` protocol.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99921350",
   "metadata": {},
   "source": [
    "Loading the model via [LangChain's LlamaCpp](https://python.langchain.com/v0.2/docs/integrations/llms/llamacpp/) abstraction enables us to use the `chaining` feature. This class is part of the `langchain-community` package, which contains third party integrations that are maintained by the LangChain community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    temperature=0.8,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5710a",
   "metadata": {},
   "source": [
    "As you can see below, we now have a `LlamaCpp` Langchain object rather than the `Llama` llama-cpp-python object from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(olmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a900fc",
   "metadata": {},
   "source": [
    "We learned above about the `Runnable` protocol. Let's see how we can invoke the model using the standard interface compared to how we originally invoked the model with `llama-cpp-python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864d5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = olmo.invoke(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ce37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b730d9e",
   "metadata": {},
   "source": [
    "If you'd like to access the base object `Llama` object from the `llama-cpp-python` package, you can access it via the `.client` attribute of the `LlamaCpp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ddd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(olmo.client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a1da2",
   "metadata": {},
   "source": [
    "With access to the underlying `Llama` object, you can directly retrieve any metadata information. In this example, we are retrieving OLMo's tokenizer chat template we saw in the [previous notebook](./2-llms-and-prompt-engineering-with-olmo.ipynb#chat-completion) to setup a String PromptTemplate.\n",
    "\n",
    "The built in model's chat template is using [`jinja2`](https://jinja.palletsprojects.com/en/3.1.x/) templating syntax, which is a popular templating engine for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"], template_format=\"jinja2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31784193",
   "metadata": {},
   "source": [
    "The `PromptTemplate` object has 2 main attributes that are very useful to explore the built-in prompt template of the model:\n",
    "- `input_variables`: This is a list of all the input variables that the prompt template expects.\n",
    "- `template`: This is the actual template string that the model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3af2bc",
   "metadata": {},
   "source": [
    "For this particular template, we can see that it expects `add_generation_prompt`, `eos_token` and `messages`. But what are the variable types for these inputs? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ebacd",
   "metadata": {},
   "source": [
    "We can answer the questions above by looking at the template string itself. The template string is using the jinja2 templating engine syntax, so it may look confusing at first, but at the end of the day it's essentially just some python code in a template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4822b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79759be0",
   "metadata": {},
   "source": [
    "As we can see above, the template reads as follows:\n",
    "- `eos_token` is a string that is added at the top of the resulting string after prompt is formatted.\n",
    "You can also see that `eos_token` is used to append `content` string values from an `assistant` `role`.\n",
    "You can find this value by going to the Model's [`tokenizer_config.json`](https://huggingface.co/allenai/OLMo-7B-Instruct-hf/blob/main/tokenizer_config.json#L233) file and looking for the `eos_token` key. *Unfornately, this is currently the only way to get this information, you can go to https://github.com/ggerganov/llama.cpp/issues/5040 for more details.* In our case, the `eos_token` is `<|endoftext|>`.\n",
    "- `messages` is a list of dictionary that is iterated over. As you can see that this dictionary should contain a `role` and `content` key.\n",
    "- `add_generation_prompt` is a boolean that is used to determine whether to add a generation prompt or not. In this case, when it's the last message and `add_generation_prompt` is `True`, it will add `<|assistant|>` string to the end of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad5f5c",
   "metadata": {},
   "source": [
    "Now that we know what the template expects we can create the final prompt string by passing in the expected input variables, this time, instead of using the `.format` method, let's see what happens if we use the `.invoke` method on the `PromptTemplate` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.invoke(\n",
    "    messages=(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "        }\n",
    "    ),\n",
    "    add_generation_prompt=True,\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb45212",
   "metadata": {},
   "source": [
    "As you can see, this results to an error if we pass in the input variables directly. This is because the `.invoke` method expects an input argument called `input` that is a **dictionary** of the input variables, which will be passed into the runnable.\n",
    "Also, there's a `config` input argument that is a `RunnableConfig` object, however, this is optional and can be omitted,\n",
    "and you will see that we will use this later when invoking the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf42949",
   "metadata": {},
   "outputs": [],
   "source": [
    "?prompt_template.invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9cf84",
   "metadata": {},
   "source": [
    "Let's try again, this time with the correct input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4caf24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = prompt_template.invoke(\n",
    "    input=dict(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "            }\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        eos_token=\"<|endoftext|>\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33e0d4",
   "metadata": {},
   "source": [
    "You can see below that we get [`StringPromptValue`](https://api.python.langchain.com/en/latest/prompt_values/langchain_core.prompt_values.StringPromptValue.html) object this time as the output rather than pure string. But we can still get the string value by calling the `.to_string` method on the `StringPromptValue` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46faf746",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9525fc6",
   "metadata": {},
   "source": [
    "The output string above contains the necessary signifier tokens for the OLMo Model to understand what the user input is and where the model should put generated responses. This whole string output will then become the full prompt for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b20ffc",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For the rest of the tutorial, we won't be using `.invoke` method on the `PromptTemplate` object, but rather we will use the `.format` method to get the final prompt string. This is more straightforward and easier to understand. The walkthrough above is just to show you how to use the `.invoke` method.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Chain in LangChain\n",
    "\n",
    "Chaining allows us to combine multiple components, as described above, in series or parallel to develop a multi-step LLM pipeline.\n",
    "As shown in the image below, any number of components can be linked together to form a chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "![LancChain Chain](../../images/langchain-chain.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "\n",
    "Image Source: [www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2023/10/a-comprehensive-guide-to-using-chains-in-langchain/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Internally, the chain works like below:\n",
    "\n",
    "STEP 1: Dictionary is processed as an input to the prompt template.  \n",
    "STEP 2: Prompt Template reads the variables to form the prompt text as output - \"What are stars and moon?\"  \n",
    "STEP 3: The prompt is given as input to the LLM model.  \n",
    "STEP 4: LLM Model produces output.  \n",
    "STEP 5: The output goes through StrOutputParser that parses it into string and gives the result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We can use the pipe operator (\"|\"), which is part of the [LCEL(Lang Chain Expression Language)](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel). The pipe operator sequentially arranges each component, similar to the above image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2337d",
   "metadata": {},
   "source": [
    "When we check the type of the resulting chain, it's just a `RunnableSequence`! So, essentially, it's a series of runnables that are executed in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3364f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e296cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b5e1c",
   "metadata": {},
   "source": [
    "Like other `Runnable` type, it has an `invoke` method that expects the same `input` and `config` arguments as we've seen before with the `LLM` and `PromptTemplate` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "?llm_chain.invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d4c13",
   "metadata": {},
   "source": [
    "Just like the example above, we'll need to pass in the input variables as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the prompt as expected by OLMo\n",
    "llm_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "            }\n",
    "        ],\n",
    "        \"add_generation_prompt\": True,\n",
    "        \"eos_token\": \"<|endoftext|>\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Instead of having to invoke `llm_chain` repeatedly with `add_generation_prompt` and `eos_token`, we can update our `prompt_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using OLMo's tokenizer chat template we saw earlier, but this time use partial variables.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt_template | olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Let's stream the output instead of waiting for OLMo to generate and display the text.\n",
    "We can use [Callbacks](https://python.langchain.com/v0.2/docs/concepts/#callbacks) to subscribe to various events in your LLM application pipeline. \n",
    "Check [this out](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) for a list of events.\n",
    "\n",
    "Below, we will use the [`StreamingStdOutCallbackHander`](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html#langchain-core-callbacks-streaming-stdout-streamingstdoutcallbackhandler) to stream the output to the console.\n",
    "To do this, we can pass in a dictionary to the `config` argument of the `invoke` method, with a `callbacks` key that contains a list of callback handlers, to see all the options, checkout the [`RunnableConfig`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are a helpful assistant. Tell me a joke about cats\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config={\"callbacks\": [StreamingStdOutCallbackHandler()]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "We will cover more LangChain concepts in upcoming notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90d26f",
   "metadata": {},
   "source": [
    "#### Your turn ðŸ˜Ž\n",
    "\n",
    "Try different messages value(s) and see how the output changes. But remember to follow the template structure.\n",
    "The dictionary keys must contain `role` and `content` and the allowed `role` values are only `user` and `assistant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your llm_chain.invoke code here, feel free to also, create your own template and try partial_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssec-scipy2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
