{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LLMs: Prompt Engineering, and OLMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "An introduction to large language models and how they're trained is out of scope, but they have been trained over large amounts of textual information available on the Internet, including books, articles, websites, and other digital content. Getting into the weeds of how these models are trained is out of the scope of this tutorial, but we have added links to papers and tutorials if you'd like to understand how LLMs are trained. Do note that training LLMs is expensive; the cost can easily increase to millions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Early language models could predict the probability of a single word token or n-grams; modern large language models can predict the likelihood of sentences, paragraphs, or entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "However, LLMs are notoriously unable to retrieve and manipulate the knowledge they possess, which leads to issues like hallucination (i.e., generating factually incorrect information), knowledge cutoffs, and poor performance in domain-specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "For this entire tutorial, we will be using [Open Language Model: OLMo](https://allenai.org/olmo), an open LLM framework built by [Allen Institute for AI](https://allenai.org/). With this open framework, you can access its complete pretraining data ([dolma](https://github.com/allenai/dolma)), training code, model weights, and evaluation suite. Tracking openness, transparency, accountability, and risks in LLMs is a growing research area. Checkout this [tool](https://opening-up-chatgpt.github.io/) to understand the range of openness in these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b7a4e",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Throughout this tutorial, you will encounter imports from a utility library called `ssec_tutorials`. This library is a collection of utility functions that we have created to make it easier to interact with the models and datasets we use in our tutorials. You can find the source code for this library at https://github.com/uw-ssec/ssec_tutorials.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb88029",
   "metadata": {},
   "source": [
    "We will first download the model, if you haven't already, using the download script mentioned during the local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc9000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74deb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials import download_olmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at /Users/anshultambay/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/anshultambay/.cache/ssec_tutorials/OLMo-7B-Instruct-Q4_K_M.gguf')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLMO_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OLMo-7B-Instruct-Q4_K_M.gguf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the name of the model\n",
    "OLMO_MODEL.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a293705",
   "metadata": {},
   "source": [
    "There are multiple things to note in the model name that gives us a lot of information about the model such as: `7B`, `Instruct`, `Q4_K_M`, and `.gguf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fef6d0",
   "metadata": {},
   "source": [
    "### `.gguf`\n",
    "\n",
    "We will cover each of these in the following sections. For now, let's focus on the file format, which is `.gguf`.\n",
    "\n",
    "We have chosen the [GGUF format](https://huggingface.co/ssec-uw/OLMo-7B-Instruct-GGUF) of the [`OLMo 7B-Instruct`](https://huggingface.co/allenai/OLMo-7B-Instruct-hf) model for this tutorial.\n",
    "\n",
    "[`GGUF`](https://huggingface.co/docs/hub/en/gguf) is a file format for storing models for inference with [GGML](https://github.com/ggerganov/ggml) and executors based on GGML, a tensor library for machine learning.\n",
    "\n",
    "This file format is optimized for fast inference on CPUs, which is why we have chosen it for this tutorial. To use the model in this format, we are utilizing [`llama.cpp`](https://github.com/ggerganov/llama.cpp), a popular C/C++ LLM inference framework. Instead of directly calling the C/C++ code, we will use the Python bindings of it called [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python).\n",
    "\n",
    "Let's start by loading the model to memory and interacting with it using the `llama-cpp-python` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c585cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90db94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = Llama(model_path=str(OLMO_MODEL), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15fed1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_cpp.llama.Llama at 0x110c64550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13a8f5",
   "metadata": {},
   "source": [
    "With just a few lines of code, now you have access to a local LLM at your fingertips!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7edba6",
   "metadata": {},
   "source": [
    "### `Q4_K_M`\n",
    "\n",
    "Before moving further, let's take a look at the `Q4_K_M` part of the model name.\n",
    "This signifies the model's *quantization* type. In other words, *compression* for a model.\n",
    "\n",
    "**Quantization** reduces a high-precision representation (usually the regular 32-bit floating-point) for weights and activations to a lower-precision data type,\n",
    "the `GGUF` format has many [quantization types](https://huggingface.co/docs/hub/en/gguf#quantization-types),\n",
    "in `Q4_K_M` each weight is reduced to a 4-bit representation.\n",
    "\n",
    "If you are curious about the details of Quantization, please refer to an excellent concept guide on [Quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization) by HuggingFace.\n",
    "\n",
    "For the sake of this tutorial, we have quantized the original OLMo model to the `Q4_K_M` type.\n",
    "You can explore the other types of quantization that we've done at [https://huggingface.co/ssec-uw/OLMo-7B-Instruct-GGUF/tree/main](https://huggingface.co/ssec-uw/OLMo-7B-Instruct-GGUF/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53b730",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "If you'd like to play around with the other quantization type,\n",
    "you can use the `download_olmo_model` function with a specific `model_file` input argument value.\n",
    "\n",
    "For example, to download the `Q5_K_M` model, you can use the following code:\n",
    "\n",
    "`OLMO_MODEL_Q5_K_M = download_olmo_model(model_file=\"OLMo-7B-Instruct-Q5_K_M.gguf\")`\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29aa88d",
   "metadata": {},
   "source": [
    "### `7B`\n",
    "\n",
    "**B** stands for billion, and 7B suggests that this specific model has 7 billion parameters.\n",
    "\n",
    "**Base models**, for example [AllenAi's OLMo-7B](https://huggingface.co/allenai/OLMo-7B), [AllenAi's OLMo-1B](https://huggingface.co/allenai/OLMo-1B), and [Meta's Llama-3-8B](meta-llama/Meta-Llama-3-8B) processes billions of words and texts. The training process is semi-supervised, meaning data is supplied without much annotation or labeling, but much effort is poured into improving the data quality. We have found that training the model with tremendous amount of text allows it to learn language patterns and general knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "When prompted, the model predicts the next tokens (words) statistically likely to follow.\n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66aeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials.scipy_conf import parse_text_generation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=\"Jupiter is the largest\", echo=True, max_tokens=1, temperature=0.8\n",
    ")  # Generate a completion, can also call olmo.create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is the largest planet\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "But when prompted with, `What is the capital of Washington state in the USA?`, a base model **could** generate logical text that may or may not contain the right answer. \n",
    "\n",
    "This is when `Instruction` fine-tuning comes into play, which enhances the base model's ability to execute specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd051d5c",
   "metadata": {},
   "source": [
    "### `Instruct`\n",
    "\n",
    "For `Instruction` fine-tuning, we can take the base models and further train them on much smaller and more specific datasets. For this tutorial, we the [OLMo-7B-Instruct](https://huggingface.co/allenai/OLMo-7B-Instruct-hf), which has been fine-tuned on [Tulu 2 SFT Mix](https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture) and [Ultrafeedback Cleaned](https://huggingface.co/datasets/allenai/ultrafeedback_binarized_cleaned) datasets. That is where the keyword **Instruct** comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=\"What is the capital of Washington state in the USA?\",\n",
    "    echo=True,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Washington state in the USA?\n",
      "Olympia is the capital of the U.S. state of Washington.\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## LLM Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We typically interact with the LLM via an API through which we can send prompts, and we can configure different parameters to get different results from LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a34282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'prompt': <Parameter \"prompt: 'str'\">,\n",
       "              'suffix': <Parameter \"suffix: 'Optional[str]' = None\">,\n",
       "              'max_tokens': <Parameter \"max_tokens: 'Optional[int]' = 16\">,\n",
       "              'temperature': <Parameter \"temperature: 'float' = 0.8\">,\n",
       "              'top_p': <Parameter \"top_p: 'float' = 0.95\">,\n",
       "              'min_p': <Parameter \"min_p: 'float' = 0.05\">,\n",
       "              'typical_p': <Parameter \"typical_p: 'float' = 1.0\">,\n",
       "              'logprobs': <Parameter \"logprobs: 'Optional[int]' = None\">,\n",
       "              'echo': <Parameter \"echo: 'bool' = False\">,\n",
       "              'stop': <Parameter \"stop: 'Optional[Union[str, List[str]]]' = []\">,\n",
       "              'frequency_penalty': <Parameter \"frequency_penalty: 'float' = 0.0\">,\n",
       "              'presence_penalty': <Parameter \"presence_penalty: 'float' = 0.0\">,\n",
       "              'repeat_penalty': <Parameter \"repeat_penalty: 'float' = 1.1\">,\n",
       "              'top_k': <Parameter \"top_k: 'int' = 40\">,\n",
       "              'stream': <Parameter \"stream: 'bool' = False\">,\n",
       "              'seed': <Parameter \"seed: 'Optional[int]' = None\">,\n",
       "              'tfs_z': <Parameter \"tfs_z: 'float' = 1.0\">,\n",
       "              'mirostat_mode': <Parameter \"mirostat_mode: 'int' = 0\">,\n",
       "              'mirostat_tau': <Parameter \"mirostat_tau: 'float' = 5.0\">,\n",
       "              'mirostat_eta': <Parameter \"mirostat_eta: 'float' = 0.1\">,\n",
       "              'model': <Parameter \"model: 'Optional[str]' = None\">,\n",
       "              'stopping_criteria': <Parameter \"stopping_criteria: 'Optional[StoppingCriteriaList]' = None\">,\n",
       "              'logits_processor': <Parameter \"logits_processor: 'Optional[LogitsProcessorList]' = None\">,\n",
       "              'grammar': <Parameter \"grammar: 'Optional[LlamaGrammar]' = None\">,\n",
       "              'logit_bias': <Parameter \"logit_bias: 'Optional[Dict[str, float]]' = None\">})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(olmo).parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Some standard parameters are:\n",
    "\n",
    "**prompt:** The prompt to generate text from.\n",
    "\n",
    "**max_tokens:** The maximum number of tokens to generate.\n",
    "\n",
    "**temperature:** A higher temperature produces more creative and diverse output, while a lower temperature produces more deterministic output. In practical terms, you should use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For creative tasks, it might be beneficial to increase the temperature value.\n",
    "\n",
    "**top_p:** This parameter, in conjunction with temperature, offers a powerful tool for controlling the model's output. Known as nucleus sampling, it allows you to determine the level of determinism in the responses. By using `top_p`, you can specify that only the tokens comprising the top_p probability mass are considered for responses. A low top_p value selects the most confident responses, while a higher value prompts the model to consider more possible words, leading to more diverse outputs. The general recommendation is to alter `temperature` or `top_p` but not both.\n",
    "\n",
    "**stop:** A list of strings to stop generation when encountered. This is another way to control the length and structure of the model's response. \n",
    "\n",
    "**frequency_penalty:** The frequency penalty applies a penalty on the next token based on how many times that token has already appeared in the generated response and prompt. The higher the frequency penalty, the less likely a word will reappear. This setting reduces the repetition of words in the generated response by giving tokens that appear more a higher penalty.\n",
    "\n",
    "**presence_penalty:** The presence penalty applies the same penalty for all repeated tokens. A token that appears twice and a token that appears n times are penalized the same. You may choose a higher presence penalty if you want the model to generate diverse or creative text. \n",
    "\n",
    "To learn more about other parameters, refer to [create_completion API reference.](https://llama-cpp-python.readthedocs.io/en/stable/api-reference/#llama_cpp.Llama.create_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(\n",
    "    prompt=\"Write a sarcastic but nice poem about the city of Seattle\",\n",
    "    echo=True,\n",
    "    temperature=1,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a sarcastic but nice poem about the city of Seattle. It can be funny, satirical or just sarcastic.\n",
      "Seattle is a city that's both chilly and wet\n",
      "With coffee that tastes like mud, and people who are out of sorts\n",
      "They claim it's because of the rain\n",
      "But I think they're just plain old grumpy\n",
      "\n",
      "The roads are more like rivers, with puddles so deep\n",
      "That even Mr. T would be left standing in defeat\n",
      "The traffic is slow, and the drivers are clueless\n",
      "It's a city that's difficult to navigate at best\n",
      "\n",
      "But wait! There's hope. The coffee shops do offer some solace\n",
      "With their artisan brews and latte art to boot\n",
      "And although the food may be pricey, it's worth every cent\n",
      "Seattle may not be perfect, but it has its charms\n",
      "\n",
      "So if you're ever in the area, don't be a stranger\n",
      "Just embrace the rain, the chilly weather, and all\n",
      "For it's what makes Seattle unique and unparalleled.\n",
      "Go ahead, laugh at my poetic attempt,\n",
      "I know I did when I heard myself humming \"Seattle, oh Seattle!\"\n",
      "It's a city that's full of contrasts,\n",
      "And although it may be quirky,\n",
      "It's one of the most charming cities around.\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Another critical concept to understand is Context length. It is the number of tokens an LLM can process at once, the maximum length of the input sequence. You can interpret it as the model's memory or attention span.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Prompt engineering or prompting is a discipline for developing and optimizing prompts to use LLMs for various applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Prompt Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "In general, prompt could contain any of the following:\n",
    "\n",
    "**Instruction:** Text to explain a specific task or instructions for the model to perform.\n",
    "\n",
    "**Context:** Additional context that can help the model generate better responses.\n",
    "\n",
    "**Input Data:** The input or question a user is interested in finding a response for.\n",
    "\n",
    "**Output Indicator:** The type or format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "A use case for LLMs is chat. In a chat context, rather than prompting LLM with a single string of text, you prompt the model with a conversation that consists of one or more messages, each of which includes a role, like `user` or `assistant`, as well as text as `content`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "`llama-cpp-python` provides a [high-level API for chat completion.](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#chat-completion) \n",
    "\n",
    "The model typically formats the messages in the conversation into a single prompt using a chat template from the `gguf` model's metadata. Chat templates are part of the tokenizers (more on that in `Module 2`.) They specify how to convert a chat conversation, represented as lists of messages, into a single tokenizable string in the format that the model expects, i.e., a prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "For OLMo you can see its chat template using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{ eos_token }}{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo.metadata[\"tokenizer.chat_template\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Prompts can help you get results on different tasks with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "The zero-shot prompt directly instructs the model to perform a task without any additional knowledge, but entirely based on its pre-existing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Classify the following text into neutral, negative, or positive. Today's Seattle weather is beautiful.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70b30f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials.scipy_conf import parse_chat_completion_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Positive.\\nThe text \"Today\\'s Seattle weather is beautiful.\" is positive because it describes the current weather conditions in a favorable way by using the word \"beautiful\".'}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Note that in the prompt above, we didn't provide OLMo with any additional context; OLMo already understands the `sentiment`‚Äîthat's zero-shot at work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "##### Prompting with Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "OLMo or other LLMs can demonstrate remarkable zero-shot capabilities, they can fail in more complex or specific tasks. In this case, we can introduce examples (shots) or additional context within the prompt to improve the OLMo's response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Let's try zero-shot to learn more about SciPy 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Did you hear about the 2025 Schmidt Sciences AI in Science Postdoctoral Fellowship Generative AI / RAG Copilot for Scientific Software?\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"I'm not personally aware of the specific Schmidt Sciences AI in Science Postdoctoral Fellowships mentioned in the Science Postdoctoral Research Advisory Group (RAG) workshop, but I can provide some general information on postdoctoral fellowships and AI in science.\\n\\nPostdoctoral fellowships are designed to help recent PhD graduates gain research experience and further develop their skills before pursuing an academic or research career. These fellowships often come with a stipend and typically last one to three years.\\n\\nAI has the potential to revolutionize many aspects of scientific research, from data analysis and modeling to simulation and prediction. Many postdoctoral fellows are exploring how AI can be applied to various fields in science, including biology, physics, chemistry, and environmental studies. Some examples of AI-focused postdoctoral fellowships include:\\n\\n* National Science Foundation (NSF) Postdoctoral Fellowships in Research on Gender, Science, Technology, and Society (GES&T): This fellowship program supports research that explores the intersection of gender, science, technology, and society.\\n* American Physical Society (APS) Postdoctoral Fellowship Program: This program offers fellowships to early-career physicists who are interested in using AI techniques to study fundamental physical phenomena.\\n* National Institutes of Health (NIH) Postdoctoral Fellowships for Minority Students: This fellowship program supports postdocs from underrepresented backgrounds who are pursuing research careers in the biomedical sciences.\\n\\nIt's possible that the Schmidt Sciences AI in Science Postdoctoral Fellowship RAG Workshop you mentioned is one of these programs or another similar initiative. However, without more context or information about the specific workshop, I can't confirm this for certain.\"}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Interpret the response before moving on. \n",
    "\n",
    "What if we provide relevant information to answer the prompt? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The first annual Schmidt Sciences AI in Science Postdoctoral Fellowship Generative AI / RAG Copilot for Scientific Software will be held virtually on Friday, March 7th from 9AM to 11AM Pacific Time. The initial online ‚ÄúGenerative AI / RAG Copilot for Scientific Software‚Äù tutorial will focus on how to utilize the underlying methods in Generative AI to advance scientific research, including the basics of LLMs followed by a demo of using LLMs and RAG for creating a question answering tool based on private data. The audience for this tutorial is researchers, with some experience programming in Python, who want to use LLMs for their research.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Did you hear about the 2025 Schmidt Sciences AI in Science Postdoctoral Fellowship Generative AI / RAG Copilot for Scientific Software?\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"I'm not aware of any specific information about a 2025 Schmidt Sciences AI in Science Postdoctoral Fellowship RAG Workshop, as I don't have real-time access to all available event data. However, it's possible that such an event could be planned or announced in the future. If you come across any details about this workshop, please let me know, and I'd be happy to update my response accordingly.\\n\\nIn general, workshops focused on AI in Science (RAG) are becoming increasingly popular as researchers explore the potential of artificial intelligence (AI) techniques for enhancing scientific discovery and innovation. These workshops typically bring together postdocs with a background in science and experience programming in Python or similar languages to learn about and apply AI methods to their research areas.\\n\\nIf you have any more information about this specific 2025 Schmidt Sciences AI in Science Postdoctoral Fellowship RAG Workshop, please share it with me, and I'll do my best to help you understand the event better.\"}\n"
     ]
    }
   ],
   "source": [
    "print(parse_chat_completion_response(chat_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "OLMo is able to generate a response that's much more helpful to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Many other prompting techniques (e.g., chain-of-thought, ReAct, etc.) exist. For this tutorial, we will focus on **Retrieval-Augmented Generation**, which can enhance OLMo's responses by integrating information retrieved from external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1328e3",
   "metadata": {},
   "source": [
    "#### Your turn üòé\n",
    "\n",
    "Try different messages value(s) and see how the output changes. But remember to follow the template structure.\n",
    "The dictionary keys must contain `role` and `content` and the allowed `role` values are only `user` and `assistant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your olmo.create_chat_completion code here. You can use the above example as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "**References**\n",
    "1. https://news.ycombinator.com/item?id=35712334\n",
    "2. https://benjaminwarner.dev/2023/07/01/attention-mechanism\n",
    "3. [Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators](https://dl.acm.org/doi/10.1145/3571884.3604316)\n",
    "4. https://www.promptingguide.ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssec-scipy2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
